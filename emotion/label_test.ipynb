{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13495b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\skn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f72f27ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í•„í„°ë§ ì „: 3360ê°œ\n",
      "re_category1ì—ì„œ ì¤‘ë¦½: 3ê°œ\n",
      "re_category2ì—ì„œ ì¤‘ë¦½: 1ê°œ\n",
      "ë°ì´í„° í•„í„°ë§ í›„: 3359ê°œ\n",
      "ì œê±°ëœ ë°ì´í„°: 1ê°œ\n",
      "ë‚¨ì€ re_category1 í´ë˜ìŠ¤ (10ê°œ): ['ê¸°ì¨', 'ë‘ë ¤ì›€', 'ë¯¸ì›€(ìƒëŒ€ë°©)', 'ë¶„ë…¸', 'ì‚¬ë‘', 'ìˆ˜ì¹˜ì‹¬', 'ìŠ¬í””', 'ì‹«ì–´í•¨(ìƒíƒœ)', 'ìš•ë§', 'ì¤‘ë¦½']\n",
      "ë‚¨ì€ re_category2 í´ë˜ìŠ¤ (64ê°œ): ['ê°ˆë“±', 'ê°ë™', 'ê±±ì •', 'ê²½ë©¸', 'ê³ ë§ˆì›€', 'ê³ í†µ', 'ê³µê°', 'ê³µí¬', 'ê¶ê¸ˆí•¨', 'ê·€ì¤‘í•¨', 'ê·¸ë¦¬ì›€', 'ê¸°ëŒ€ê°', 'ë‚œì²˜í•¨', 'ë‚ ì¹´ë¡œì›€', 'ëƒ‰ë‹´', 'ë„ˆê·¸ëŸ¬ì›€', 'ë†€ëŒ', 'ë‹¤ì •í•¨', 'ë‹µë‹µí•¨', 'ë™ì •(ìŠ¬í””)', 'ë‘ê·¼ê±°ë¦¼', 'ë§Œì¡±ê°', 'ë§¤ë ¥ì ', 'ë¬´ê¸°ë ¥', 'ë¯¸ì•ˆí•¨', 'ë°˜ê°€ì›€', 'ë°˜ê°', 'ë°œì—´', 'ë¶€ë„ëŸ¬ì›€', 'ë¶ˆë§Œ', 'ë¶ˆì‹ ê°', 'ë¶ˆì¾Œ', 'ë¶ˆí¸í•¨', 'ë¹„ìœ„ìƒí•¨', 'ì‚¬ë‚˜ì›€', 'ìˆ˜ì¹˜ì‹¬', 'ì‹œê¸°ì‹¬', 'ì‹ ë¢°ê°', 'ì‹ ëª…ë‚¨', 'ì‹¤ë§', 'ì‹«ì¦', 'ì‹¬ì‹¬í•¨', 'ì•„ì‰¬ì›€', 'ì•„í””', 'ì•ˆì •ê°', 'ì–µìš¸í•¨', 'ì™¸ë¡œì›€', 'ì™¸ë©´', 'ìš•ì‹¬', 'ì›ë§', 'ìœ„ì¶•ê°', 'ìë‘ìŠ¤ëŸ¬ì›€', 'ìì‹ ê°', 'ì ˆë§', 'ì£„ì±…ê°', 'ì¦ê±°ì›€', 'ì´ˆì¡°í•¨', 'ì¹˜ì‚¬í•¨', 'íƒ€ì˜¤ë¦„', 'í†µì¾Œí•¨', 'í¸ì•ˆí•¨', 'í—ˆë§', 'í˜¸ê°', 'í›„íšŒ']\n",
      "í•„í„°ë§ í›„ re_category1 ì¤‘ë¦½: 2ê°œ\n",
      "í•„í„°ë§ í›„ re_category2 ì¤‘ë¦½: 0ê°œ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>generator_context</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>input_context</th>\n",
       "      <th>original_index</th>\n",
       "      <th>augmentation_index</th>\n",
       "      <th>re_category1</th>\n",
       "      <th>re_category2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ê°‘ìê¸° ë‚´ ì±…ìƒ ìœ„ì— ë†“ì¸ ë”°ëœ»í•œ ì†í¸ì§€ì— ë§ˆìŒì´ ë­‰í´í•´ì¡Œë‹¤.</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "      <td>ì„¤íƒ• ìŠ¤í‹± ê»´ì¤€ê±° ì„¼ìŠ¤ ë°±ì  ë§Œì ì— ì²œì </td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ë¹„ê°€ ì˜¤ëŠ”ë°ë„ ì¹œêµ¬ê°€ ë‚´ ì¢‹ì•„í•˜ëŠ” ì¹´í˜ê¹Œì§€ ìš°ì‚° ë“¤ê³  ë”°ë¼ì™€ì¤˜ì„œ ë§ˆìŒì´ ë”°ëœ»í•´ì¡Œì–´.</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "      <td>ì•„ì“° ì‚°ì°¨ì´ ê¸°ë¶„ ì•ˆ ì¢‹ì€ ê±° ì•Œì•„ì±„ê³  ì‚°ì°¨ì´ê°€ ê°€ê³  ì‹¶ë‹¤ë˜ í† ë¼ì§‘ ë°ë ¤ì˜¨ ê±° ê°ë™</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>í–‡ì‚´ ì•„ë˜ ë°˜ì§ì´ëŠ” ì•„ì´ì˜ ëˆˆë™ìê°€ ë§ˆì¹˜ ì‘ì€ ë³´ì„ì²˜ëŸ¼ ë¹›ë‚¬ë‹¤. ê·¸ ìˆœê°„, ì„¸ìƒ ëª¨...</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "      <td>ì‹ ë°ë ë¼ ë“œë ˆìŠ¤ëŠ” ë‹¤ì‹œ ë´ë„ ë„ˆë¬´ ì•„ë¦„ë‹¤ì›Œ. ì‚¬ëŒì—ê²Œ ê¿ˆì˜ ë¬¼ê²°ì„ ì…íˆë‹¤ë‹ˆìš”.</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ì´ë²ˆ ì „ì‹œíšŒ ì¤€ë¹„í•˜ë©´ì„œ ì² ì €í•˜ê²Œ ì„¸ë¶€ê¹Œì§€ ì±™ê²¨ì¤€ ë•ë¶„ì— ëª¨ë“  ê²Œ ì™„ë²½í•˜ê²Œ ë§ˆë¬´ë¦¬ë¼ì„œ...</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "      <td>ì™€ ë¯¼í¬ì§„ ì”¨ ì• ë“¤ ìˆ™ì†Œ ìŠ¤íƒ€ì¼ë§ê¹Œì§€ ë§¡ê¸°ë©´ì„œ ì‹ ê²½ì¨ ì¤€ ê±° ì§„ì§œ ì¢€ ëŒ€ë‹¨í•˜ë„¤</td>\n",
       "      <td>107.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ë¹„ ì˜¤ëŠ” ë‚  ë‚¯ì„  ì‚¬ëŒì´ ë‚´ê²Œ ë‹´ìš”ë¥¼ ê±´ë„¤ë©° ì¶”ìœ„ ê±±ì •í•´ ì¤¬ë‹¤. ë§ˆìŒì´ ë”°ëœ»í•´ì ¸ì„œ ...</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "      <td>ê°œê°ë™ì¸ ê±° ìê¸°ê°€ ì“°ê³  ìˆë˜ ìš°ì‚° ë‚˜ ì£¼ê³ \\nìê¸°ê°€ ë¹„ ë§ì•„ê°€ë©´ì„œ ë’¤ì§‘ì–´ì¤€ ê±°ì•¼\\...</td>\n",
       "      <td>137.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                  generator_context category1  \\\n",
       "0           0                 ê°‘ìê¸° ë‚´ ì±…ìƒ ìœ„ì— ë†“ì¸ ë”°ëœ»í•œ ì†í¸ì§€ì— ë§ˆìŒì´ ë­‰í´í•´ì¡Œë‹¤.        ê¸°ì¨   \n",
       "1           1     ë¹„ê°€ ì˜¤ëŠ”ë°ë„ ì¹œêµ¬ê°€ ë‚´ ì¢‹ì•„í•˜ëŠ” ì¹´í˜ê¹Œì§€ ìš°ì‚° ë“¤ê³  ë”°ë¼ì™€ì¤˜ì„œ ë§ˆìŒì´ ë”°ëœ»í•´ì¡Œì–´.        ê¸°ì¨   \n",
       "2           2  í–‡ì‚´ ì•„ë˜ ë°˜ì§ì´ëŠ” ì•„ì´ì˜ ëˆˆë™ìê°€ ë§ˆì¹˜ ì‘ì€ ë³´ì„ì²˜ëŸ¼ ë¹›ë‚¬ë‹¤. ê·¸ ìˆœê°„, ì„¸ìƒ ëª¨...        ê¸°ì¨   \n",
       "3           3  ì´ë²ˆ ì „ì‹œíšŒ ì¤€ë¹„í•˜ë©´ì„œ ì² ì €í•˜ê²Œ ì„¸ë¶€ê¹Œì§€ ì±™ê²¨ì¤€ ë•ë¶„ì— ëª¨ë“  ê²Œ ì™„ë²½í•˜ê²Œ ë§ˆë¬´ë¦¬ë¼ì„œ...        ê¸°ì¨   \n",
       "4           4  ë¹„ ì˜¤ëŠ” ë‚  ë‚¯ì„  ì‚¬ëŒì´ ë‚´ê²Œ ë‹´ìš”ë¥¼ ê±´ë„¤ë©° ì¶”ìœ„ ê±±ì •í•´ ì¤¬ë‹¤. ë§ˆìŒì´ ë”°ëœ»í•´ì ¸ì„œ ...        ê¸°ì¨   \n",
       "\n",
       "  category2                                      input_context  \\\n",
       "0        ê°ë™                             ì„¤íƒ• ìŠ¤í‹± ê»´ì¤€ê±° ì„¼ìŠ¤ ë°±ì  ë§Œì ì— ì²œì    \n",
       "1        ê°ë™     ì•„ì“° ì‚°ì°¨ì´ ê¸°ë¶„ ì•ˆ ì¢‹ì€ ê±° ì•Œì•„ì±„ê³  ì‚°ì°¨ì´ê°€ ê°€ê³  ì‹¶ë‹¤ë˜ í† ë¼ì§‘ ë°ë ¤ì˜¨ ê±° ê°ë™   \n",
       "2        ê°ë™        ì‹ ë°ë ë¼ ë“œë ˆìŠ¤ëŠ” ë‹¤ì‹œ ë´ë„ ë„ˆë¬´ ì•„ë¦„ë‹¤ì›Œ. ì‚¬ëŒì—ê²Œ ê¿ˆì˜ ë¬¼ê²°ì„ ì…íˆë‹¤ë‹ˆìš”.   \n",
       "3        ê°ë™        ì™€ ë¯¼í¬ì§„ ì”¨ ì• ë“¤ ìˆ™ì†Œ ìŠ¤íƒ€ì¼ë§ê¹Œì§€ ë§¡ê¸°ë©´ì„œ ì‹ ê²½ì¨ ì¤€ ê±° ì§„ì§œ ì¢€ ëŒ€ë‹¨í•˜ë„¤   \n",
       "4        ê°ë™  ê°œê°ë™ì¸ ê±° ìê¸°ê°€ ì“°ê³  ìˆë˜ ìš°ì‚° ë‚˜ ì£¼ê³ \\nìê¸°ê°€ ë¹„ ë§ì•„ê°€ë©´ì„œ ë’¤ì§‘ì–´ì¤€ ê±°ì•¼\\...   \n",
       "\n",
       "   original_index  augmentation_index re_category1 re_category2  \n",
       "0            20.0                 NaN           ê¸°ì¨           ê°ë™  \n",
       "1            79.0                 NaN           ê¸°ì¨           ê°ë™  \n",
       "2           104.0                 NaN           ê¸°ì¨           ê°ë™  \n",
       "3           107.0                 NaN           ê¸°ì¨           ê°ë™  \n",
       "4           137.0                 NaN           ê¸°ì¨           ê°ë™  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(r'C:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\data\\retest_augmentation.xlsx')\n",
    "\n",
    "# re_category2ì—ì„œë§Œ ì¤‘ë¦½ ë°ì´í„° ì œê±° (re_category1ì€ ì¤‘ë¦½ ìœ ì§€)\n",
    "print(f\"ë°ì´í„° í•„í„°ë§ ì „: {len(data)}ê°œ\")\n",
    "print(f\"re_category1ì—ì„œ ì¤‘ë¦½: {(data['re_category1'] == 'ì¤‘ë¦½').sum()}ê°œ\")\n",
    "print(f\"re_category2ì—ì„œ ì¤‘ë¦½: {(data['re_category2'] == 'ì¤‘ë¦½').sum()}ê°œ\")\n",
    "\n",
    "# re_category2ì—ì„œë§Œ ì¤‘ë¦½ ë°ì´í„° ì œê±° (Category2ì— ì¤‘ë¦½ì´ ì—†ìœ¼ë¯€ë¡œ ë¼ë²¨ ë¶ˆì¼ì¹˜ ë°©ì§€)\n",
    "original_count = len(data)\n",
    "data = data[data['re_category2'] != 'ì¤‘ë¦½'].copy()\n",
    "\n",
    "# ì¸ë±ìŠ¤ ì¬ì„¤ì •\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print(f\"ë°ì´í„° í•„í„°ë§ í›„: {len(data)}ê°œ\")\n",
    "print(f\"ì œê±°ëœ ë°ì´í„°: {original_count - len(data)}ê°œ\")\n",
    "\n",
    "# í•„í„°ë§ëœ ë°ì´í„° í™•ì¸\n",
    "print(f\"ë‚¨ì€ re_category1 í´ë˜ìŠ¤ ({len(data['re_category1'].unique())}ê°œ): {sorted(data['re_category1'].unique())}\")\n",
    "print(f\"ë‚¨ì€ re_category2 í´ë˜ìŠ¤ ({len(data['re_category2'].unique())}ê°œ): {sorted(data['re_category2'].unique())}\")\n",
    "\n",
    "# ì¤‘ë¦½ í™•ì¸\n",
    "print(f\"í•„í„°ë§ í›„ re_category1 ì¤‘ë¦½: {(data['re_category1'] == 'ì¤‘ë¦½').sum()}ê°œ\")\n",
    "print(f\"í•„í„°ë§ í›„ re_category2 ì¤‘ë¦½: {(data['re_category2'] == 'ì¤‘ë¦½').sum()}ê°œ\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dbf27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_model():\n",
    "  \"\"\"\n",
    "  ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "  \"\"\"\n",
    "  model = SentenceTransformer(\"dragonkue/snowflake-arctic-embed-l-v2.0-ko\") \n",
    "  vec_dim = len(model.encode(\"dummy_text\"))\n",
    "  print(f\"ëª¨ë¸ ì°¨ì›: {vec_dim}\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c873e343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ì°¨ì›: 1024\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = embeddings_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4926f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë³€ìˆ˜ X ì´ˆê¸°í™”ë¨\n",
      "ë³€ìˆ˜ y ì´ˆê¸°í™”ë¨\n",
      "âœ… ê¸°ì¡´ ë³€ìˆ˜ë“¤ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3359 entries, 0 to 3358\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0          3359 non-null   int64  \n",
      " 1   generator_context   3359 non-null   object \n",
      " 2   category1           3359 non-null   object \n",
      " 3   category2           3359 non-null   object \n",
      " 4   input_context       3359 non-null   object \n",
      " 5   original_index      664 non-null    float64\n",
      " 6   augmentation_index  2695 non-null   float64\n",
      " 7   re_category1        3359 non-null   object \n",
      " 8   re_category2        3359 non-null   object \n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 236.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ì¡´ ë³€ìˆ˜ ì´ˆê¸°í™” (ì¤‘ë¦½ ë°ì´í„° ì œê±°ë¡œ ì¸í•œ í¬ê¸° ë¶ˆì¼ì¹˜ ë°©ì§€)\n",
    "vars_to_reset = ['X', 'y', 'y_encoded', 'X_combined', 'y_cat2', 'y_cat2_encoded', 'le', 'le_cat2', 'cat1_encoder']\n",
    "for var_name in vars_to_reset:\n",
    "    if var_name in locals():\n",
    "        del locals()[var_name]\n",
    "        print(f\"ë³€ìˆ˜ {var_name} ì´ˆê¸°í™”ë¨\")\n",
    "\n",
    "print(\"âœ… ê¸°ì¡´ ë³€ìˆ˜ë“¤ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ë°ì´í„° ì •ë³´ í™•ì¸\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd04d86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ í•„í„°ë§ëœ ë°ì´í„°ë¡œ ë²¡í„° ìƒì„±...\n",
      "âœ… ë²¡í„° ìƒì„± ì™„ë£Œ: 3359ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ì¤‘ë¦½ ë°ì´í„° ì œê±° í›„ ë²¡í„° ìƒì„±\n",
    "print(\"ğŸ“ í•„í„°ë§ëœ ë°ì´í„°ë¡œ ë²¡í„° ìƒì„±...\")\n",
    "data['vector'] = data['generator_context'].apply(lambda x: embeddings_model.encode(x).tolist())\n",
    "print(f\"âœ… ë²¡í„° ìƒì„± ì™„ë£Œ: {len(data)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0axv5b84ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œ Top3 ìœ ì‚¬í•œ ìƒ˜í”Œ ì°¾ê¸°\n",
      "============================================================\n",
      "ğŸ“Š ë²¡í„° ë°ì´í„° ì¤€ë¹„ ì¤‘...\n",
      "ë²¡í„° í–‰ë ¬ í¬ê¸°: (3359, 1024)\n",
      "ğŸ§® ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘... (ì´ 3359ê°œ ìƒ˜í”Œì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬)\n",
      "ì²˜ë¦¬ ì¤‘: 1-100 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 101-200 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 201-300 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 301-400 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 401-500 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 501-600 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 601-700 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 701-800 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 801-900 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 901-1000 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 1001-1100 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 1101-1200 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 1201-1300 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 1301-1400 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 1401-1500 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 1501-1600 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 1601-1700 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 1701-1800 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 1801-1900 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 1901-2000 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 2001-2100 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 2101-2200 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 2201-2300 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 2301-2400 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 2401-2500 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 2501-2600 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 2601-2700 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 2701-2800 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 2801-2900 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 2901-3000 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 3001-3100 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 3101-3200 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 3201-3300 / 3359\n",
      "ì²˜ë¦¬ ì¤‘: 3301-3359 / 3359\n",
      "âœ… Top3 ìœ ì‚¬í•œ ìƒ˜í”Œ ì°¾ê¸° ì™„ë£Œ\n",
      "ğŸ“ ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤ì„ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€ ì¤‘...\n",
      "âœ… ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ\n",
      "\n",
      "ğŸ“‹ ì—…ë°ì´íŠ¸ëœ ë°ì´í„°í”„ë ˆì„ ì •ë³´:\n",
      "ë°ì´í„° í¬ê¸°: (3359, 16)\n",
      "ìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼: top1_context, top1_category, top2_context, top2_category, top3_context, top3_category\n",
      "\n",
      "ğŸ” Top3 ìœ ì‚¬ë„ ê²°ê³¼ ìƒ˜í”Œ (ì²« 3ê°œ):\n",
      "====================================================================================================\n",
      "[ìƒ˜í”Œ 1]\n",
      "ì›ë³¸ í…ìŠ¤íŠ¸: ê°‘ìê¸° ë‚´ ì±…ìƒ ìœ„ì— ë†“ì¸ ë”°ëœ»í•œ ì†í¸ì§€ì— ë§ˆìŒì´ ë­‰í´í•´ì¡Œë‹¤....\n",
      "ì›ë³¸ ì¹´í…Œê³ ë¦¬: ê¸°ì¨_ê°ë™\n",
      "\n",
      "  Top1 ìœ ì‚¬ í…ìŠ¤íŠ¸: ì¹œêµ¬ê°€ ì˜¤ëœë§Œì— ë³´ë‚´ì˜¨ í¸ì§€ì— ë§ˆìŒì´ ë”°ëœ»í•´ì ¸ì„œ ì €ë„ ëª¨ë¥´ê²Œ ëˆˆê°€ê°€ ì´‰ì´‰í•´ì¡Œì–´ìš”....\n",
      "  Top1 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: ê¸°ì¨_ê°ë™_0.7459\n",
      "\n",
      "  Top2 ìœ ì‚¬ í…ìŠ¤íŠ¸: ë”°ëœ»í•œ í–‡ì‚´ ì•„ë˜ì„œ ì¹œêµ¬ì˜ ì§„ì‹¬ ì–´ë¦° í¸ì§€ë¥¼ ì½ëŠ”ë°, ê°€ìŠ´ì´ ë²…ì°¨ì˜¬ë¼ í•œì°¸ì„ ë©í•˜ë‹ˆ ê·¸ ìë¦¬ì— ì„œ ìˆì—ˆì–´....\n",
      "  Top2 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: ê¸°ì¨_ê°ë™_0.7265\n",
      "\n",
      "  Top3 ìœ ì‚¬ í…ìŠ¤íŠ¸: ì˜¤ëœë§Œì— ë§Œë‚œ ì¹œêµ¬ê°€ ì§„ì‹¬ ì–´ë¦° í¸ì§€ë¥¼ ê±´ë„¤ì£¼ì ë§ˆìŒì´ ë”°ëœ»í•´ì ¸ì„œ ëˆˆë¬¼ì´ ë‚¬ë‹¤....\n",
      "  Top3 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: ê¸°ì¨_ê°ë™_0.7153\n",
      "====================================================================================================\n",
      "[ìƒ˜í”Œ 2]\n",
      "ì›ë³¸ í…ìŠ¤íŠ¸: ë¹„ê°€ ì˜¤ëŠ”ë°ë„ ì¹œêµ¬ê°€ ë‚´ ì¢‹ì•„í•˜ëŠ” ì¹´í˜ê¹Œì§€ ìš°ì‚° ë“¤ê³  ë”°ë¼ì™€ì¤˜ì„œ ë§ˆìŒì´ ë”°ëœ»í•´ì¡Œì–´....\n",
      "ì›ë³¸ ì¹´í…Œê³ ë¦¬: ê¸°ì¨_ê°ë™\n",
      "\n",
      "  Top1 ìœ ì‚¬ í…ìŠ¤íŠ¸: ì˜¤í›„ì— ê°‘ìê¸° ë¹„ê°€ ìŸì•„ì¡ŒëŠ”ë°, ë™ë£Œê°€ ìš°ì‚°ì„ ê°™ì´ ì¨ì£¼ë©° \"ê±±ì • ë§ˆ, ë‚´ê°€ ìˆì–´\"ë¼ê³  ë§í•´ì¤˜ì„œ ë§ˆìŒì´ ë”°ëœ»í•´ì¡Œì–´....\n",
      "  Top1 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: ê¸°ì¨_ê³ ë§ˆì›€_0.7774\n",
      "\n",
      "  Top2 ìœ ì‚¬ í…ìŠ¤íŠ¸: ë¹„ ì˜¤ëŠ” ë‚ , ì¹œêµ¬ê°€ ìš°ì‚°ì„ ë‚˜ëˆ ì¤˜ì„œ ì“¸ì“¸í•¨ì´ ì‚¬ë¼ì§€ê³  ë§ˆìŒì†ì— í¬ê·¼í•¨ì´ ê°€ë“ ì°¨ì˜¬ëë‹¤....\n",
      "  Top2 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: ê¸°ì¨_ê³ ë§ˆì›€_0.7719\n",
      "\n",
      "  Top3 ìœ ì‚¬ í…ìŠ¤íŠ¸: ì°½ë°– ë¹„ ë‚´ë¦¬ëŠ” ì¹´í˜ì—ì„œ ì¹œêµ¬ê°€ ë‚´ ê³ ë¯¼ì— ê·€ ê¸°ìš¸ì´ë©° ë”°ëœ»í•œ ëˆˆë¹›ìœ¼ë¡œ ë§ì¥êµ¬ì¹  ë•Œ, ë§ˆìŒì´ ë“ ë“ í•´ì¡Œë‹¤....\n",
      "  Top3 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: ê¸°ì¨_ì‹ ë¢°ê°_0.7643\n",
      "====================================================================================================\n",
      "[ìƒ˜í”Œ 3]\n",
      "ì›ë³¸ í…ìŠ¤íŠ¸: í–‡ì‚´ ì•„ë˜ ë°˜ì§ì´ëŠ” ì•„ì´ì˜ ëˆˆë™ìê°€ ë§ˆì¹˜ ì‘ì€ ë³´ì„ì²˜ëŸ¼ ë¹›ë‚¬ë‹¤. ê·¸ ìˆœê°„, ì„¸ìƒ ëª¨ë“  í¬ë§ì´ ê·¸ë…€ì—ê²Œ ìŸì•„ì§€ëŠ” ë“¯í–ˆë‹¤....\n",
      "ì›ë³¸ ì¹´í…Œê³ ë¦¬: ê¸°ì¨_ê°ë™\n",
      "\n",
      "  Top1 ìœ ì‚¬ í…ìŠ¤íŠ¸: í–‡ì‚´ì´ ê·¸ë…€ì˜ ëˆˆë™ìì— ë°˜ì§ì´ë©°, ë¬´ì‹¬í•œ ë“¯ ë‹¤ê°€ì˜¤ëŠ” ê·¸ ëª¨ìŠµì— ë‚˜ëŠ” ê°€ìŠ´ì´ ë›°ì—ˆë‹¤....\n",
      "  Top1 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: ì‚¬ë‘_ë‘ê·¼ê±°ë¦¼_0.7278\n",
      "\n",
      "  Top2 ìœ ì‚¬ í…ìŠ¤íŠ¸: í–‡ì‚´ ì•„ë˜ ê·¸ë…€ì˜ ë¯¸ì†Œê°€ ë°˜ì§ì˜€ë‹¤. ëˆˆì„ ë—„ ìˆ˜ ì—†ëŠ” ê·¸ ëª¨ìŠµì— ë§ˆìŒì´ ì €ì ˆë¡œ ëŒë ¸ë‹¤....\n",
      "  Top2 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: ì‚¬ë‘_ë§¤ë ¥ì _0.6993\n",
      "\n",
      "  Top3 ìœ ì‚¬ í…ìŠ¤íŠ¸: í–‡ì‚´ ì•„ë˜ ë°˜ì§ì´ëŠ” ëˆˆë™ìì²˜ëŸ¼, ì—°ì¸ì˜ ì‘ì€ ì†ê¸¸ì´ ë‚´ í•˜ë£¨ë¥¼ íŠ¹ë³„í•˜ê²Œ ë¬¼ë“¤ì˜€ë‹¤....\n",
      "  Top3 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: ì‚¬ë‘_ê·€ì¤‘í•¨_0.6877\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“ˆ ìœ ì‚¬ë„ ë¶„í¬ í†µê³„:\n",
      "í‰ê·  ìœ ì‚¬ë„: 0.7333\n",
      "ìµœëŒ€ ìœ ì‚¬ë„: 0.9572\n",
      "ìµœì†Œ ìœ ì‚¬ë„: 0.3699\n",
      "í‘œì¤€í¸ì°¨: 0.0780\n",
      "\n",
      "ğŸ“Š Top3 ìœ ì‚¬ ìƒ˜í”Œë“¤ê³¼ ì›ë³¸ì˜ ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ ë¶„ì„:\n",
      "Top1 - Category1 ì¼ì¹˜: 2570/3359 (76.5%)\n",
      "Top1 - Category2 ì¼ì¹˜: 1595/3359 (47.5%)\n",
      "Top1 - ë‘˜ ë‹¤ ì¼ì¹˜: 1564/3359 (46.6%)\n",
      "\n",
      "Top2 - Category1 ì¼ì¹˜: 2501/3359 (74.5%)\n",
      "Top2 - Category2 ì¼ì¹˜: 1479/3359 (44.0%)\n",
      "Top2 - ë‘˜ ë‹¤ ì¼ì¹˜: 1447/3359 (43.1%)\n",
      "\n",
      "Top3 - Category1 ì¼ì¹˜: 2448/3359 (72.9%)\n",
      "Top3 - Category2 ì¼ì¹˜: 1398/3359 (41.6%)\n",
      "Top3 - ë‘˜ ë‹¤ ì¼ì¹˜: 1373/3359 (40.9%)\n",
      "\n",
      "ğŸ‰ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ Top3 ìœ ì‚¬í•œ ìƒ˜í”Œ ì¶”ê°€ ì™„ë£Œ!\n",
      "ì´ 3359ê°œ ìƒ˜í”Œì— ëŒ€í•´ ê°ê° Top3 ìœ ì‚¬í•œ ìƒ˜í”Œ ì •ë³´ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“‹ ìµœì¢… ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ ëª©ë¡:\n",
      "  - Unnamed: 0\n",
      "  - generator_context\n",
      "  - category1\n",
      "  - category2\n",
      "  - input_context\n",
      "  - original_index\n",
      "  - augmentation_index\n",
      "  - re_category1\n",
      "  - re_category2\n",
      "  - vector\n",
      "  - top1_context\n",
      "  - top1_category\n",
      "  - top2_context\n",
      "  - top2_category\n",
      "  - top3_context\n",
      "  - top3_category\n",
      "\n",
      "ë°ì´í„° ì €ì¥ ê¶Œì¥ì‚¬í•­:\n",
      "data.to_excel('enhanced_data_with_similarity.xlsx', index=False)\n",
      "# ë˜ëŠ”\n",
      "data.to_csv('enhanced_data_with_similarity.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œ Top3 ìœ ì‚¬í•œ ìƒ˜í”Œ ì°¾ê¸°\n",
    "print(\"ğŸ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œ Top3 ìœ ì‚¬í•œ ìƒ˜í”Œ ì°¾ê¸°\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# ë²¡í„° ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "print(\"ğŸ“Š ë²¡í„° ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "vectors = np.vstack(data['vector'].values)\n",
    "print(f\"ë²¡í„° í–‰ë ¬ í¬ê¸°: {vectors.shape}\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë°°ì¹˜ë¡œ ì²˜ë¦¬\n",
    "batch_size = 100\n",
    "n_samples = len(data)\n",
    "\n",
    "print(f\"ğŸ§® ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘... (ì´ {n_samples}ê°œ ìƒ˜í”Œì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬)\")\n",
    "\n",
    "# ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "top1_context = []\n",
    "top1_category = []\n",
    "top2_context = []\n",
    "top2_category = []\n",
    "top3_context = []\n",
    "top3_category = []\n",
    "\n",
    "# ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬\n",
    "for batch_start in range(0, n_samples, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, n_samples)\n",
    "    print(f\"ì²˜ë¦¬ ì¤‘: {batch_start+1}-{batch_end} / {n_samples}\")\n",
    "    \n",
    "    # í˜„ì¬ ë°°ì¹˜ì— ëŒ€í•œ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    batch_vectors = vectors[batch_start:batch_end]\n",
    "    similarities = cosine_similarity(batch_vectors, vectors)\n",
    "    \n",
    "    # ê° ìƒ˜í”Œì— ëŒ€í•´ Top3 ìœ ì‚¬í•œ ìƒ˜í”Œ ì°¾ê¸°\n",
    "    for i in range(batch_end - batch_start):\n",
    "        current_idx = batch_start + i\n",
    "        sim_row = similarities[i]\n",
    "        \n",
    "        # ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ -1ë¡œ ì„¤ì • (ì œì™¸)\n",
    "        sim_row[current_idx] = -1\n",
    "        \n",
    "        # ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœì„œë¡œ ì •ë ¬ (ì¸ë±ìŠ¤ ë°˜í™˜)\n",
    "        top_indices = np.argsort(sim_row)[::-1][:3]  # ìƒìœ„ 3ê°œ\n",
    "        top_similarities = sim_row[top_indices]\n",
    "        \n",
    "        # Top 1\n",
    "        idx1 = top_indices[0]\n",
    "        sim1 = top_similarities[0]\n",
    "        top1_context.append(data.iloc[idx1]['generator_context'])\n",
    "        top1_category.append(f\"{data.iloc[idx1]['re_category1']}_{data.iloc[idx1]['re_category2']}_{sim1:.4f}\")\n",
    "        \n",
    "        # Top 2\n",
    "        idx2 = top_indices[1]\n",
    "        sim2 = top_similarities[1]\n",
    "        top2_context.append(data.iloc[idx2]['generator_context'])\n",
    "        top2_category.append(f\"{data.iloc[idx2]['re_category1']}_{data.iloc[idx2]['re_category2']}_{sim2:.4f}\")\n",
    "        \n",
    "        # Top 3\n",
    "        idx3 = top_indices[2]\n",
    "        sim3 = top_similarities[2]\n",
    "        top3_context.append(data.iloc[idx3]['generator_context'])\n",
    "        top3_category.append(f\"{data.iloc[idx3]['re_category1']}_{data.iloc[idx3]['re_category2']}_{sim3:.4f}\")\n",
    "\n",
    "print(\"âœ… Top3 ìœ ì‚¬í•œ ìƒ˜í”Œ ì°¾ê¸° ì™„ë£Œ\")\n",
    "\n",
    "# ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤ì„ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
    "print(\"ğŸ“ ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤ì„ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€ ì¤‘...\")\n",
    "data['top1_context'] = top1_context\n",
    "data['top1_category'] = top1_category\n",
    "data['top2_context'] = top2_context\n",
    "data['top2_category'] = top2_category\n",
    "data['top3_context'] = top3_context\n",
    "data['top3_category'] = top3_category\n",
    "\n",
    "print(\"âœ… ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ\")\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(f\"\\nğŸ“‹ ì—…ë°ì´íŠ¸ëœ ë°ì´í„°í”„ë ˆì„ ì •ë³´:\")\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {data.shape}\")\n",
    "print(f\"ìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼: top1_context, top1_category, top2_context, top2_category, top3_context, top3_category\")\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "print(f\"\\nğŸ” Top3 ìœ ì‚¬ë„ ê²°ê³¼ ìƒ˜í”Œ (ì²« 3ê°œ):\")\n",
    "print(\"=\"*100)\n",
    "for i in range(3):\n",
    "    print(f\"[ìƒ˜í”Œ {i+1}]\")\n",
    "    print(f\"ì›ë³¸ í…ìŠ¤íŠ¸: {data.iloc[i]['generator_context'][:80]}...\")\n",
    "    print(f\"ì›ë³¸ ì¹´í…Œê³ ë¦¬: {data.iloc[i]['re_category1']}_{data.iloc[i]['re_category2']}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"  Top1 ìœ ì‚¬ í…ìŠ¤íŠ¸: {data.iloc[i]['top1_context'][:80]}...\")\n",
    "    print(f\"  Top1 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: {data.iloc[i]['top1_category']}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"  Top2 ìœ ì‚¬ í…ìŠ¤íŠ¸: {data.iloc[i]['top2_context'][:80]}...\")\n",
    "    print(f\"  Top2 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: {data.iloc[i]['top2_category']}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"  Top3 ìœ ì‚¬ í…ìŠ¤íŠ¸: {data.iloc[i]['top3_context'][:80]}...\")\n",
    "    print(f\"  Top3 ì¹´í…Œê³ ë¦¬&ìœ ì‚¬ë„: {data.iloc[i]['top3_category']}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "# ìœ ì‚¬ë„ ë¶„í¬ í†µê³„\n",
    "print(f\"\\nğŸ“ˆ ìœ ì‚¬ë„ ë¶„í¬ í†µê³„:\")\n",
    "all_similarities = []\n",
    "for i in range(len(data)):\n",
    "    for j in [1, 2, 3]:\n",
    "        category_col = f'top{j}_category'\n",
    "        similarity_str = data.iloc[i][category_col].split('_')[-1]\n",
    "        all_similarities.append(float(similarity_str))\n",
    "\n",
    "similarities_array = np.array(all_similarities)\n",
    "print(f\"í‰ê·  ìœ ì‚¬ë„: {similarities_array.mean():.4f}\")\n",
    "print(f\"ìµœëŒ€ ìœ ì‚¬ë„: {similarities_array.max():.4f}\")\n",
    "print(f\"ìµœì†Œ ìœ ì‚¬ë„: {similarities_array.min():.4f}\")\n",
    "print(f\"í‘œì¤€í¸ì°¨: {similarities_array.std():.4f}\")\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ ë¶„ì„\n",
    "print(f\"\\nğŸ“Š Top3 ìœ ì‚¬ ìƒ˜í”Œë“¤ê³¼ ì›ë³¸ì˜ ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ ë¶„ì„:\")\n",
    "category1_matches = [0, 0, 0]  # top1, top2, top3\n",
    "category2_matches = [0, 0, 0]\n",
    "both_matches = [0, 0, 0]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    orig_cat1 = data.iloc[i]['re_category1']\n",
    "    orig_cat2 = data.iloc[i]['re_category2']\n",
    "    \n",
    "    for j in range(3):\n",
    "        top_category = data.iloc[i][f'top{j+1}_category']\n",
    "        top_cat1, top_cat2, _ = top_category.split('_')\n",
    "        \n",
    "        if orig_cat1 == top_cat1:\n",
    "            category1_matches[j] += 1\n",
    "        if orig_cat2 == top_cat2:\n",
    "            category2_matches[j] += 1\n",
    "        if orig_cat1 == top_cat1 and orig_cat2 == top_cat2:\n",
    "            both_matches[j] += 1\n",
    "\n",
    "for j in range(3):\n",
    "    print(f\"Top{j+1} - Category1 ì¼ì¹˜: {category1_matches[j]}/{len(data)} ({category1_matches[j]/len(data)*100:.1f}%)\")\n",
    "    print(f\"Top{j+1} - Category2 ì¼ì¹˜: {category2_matches[j]}/{len(data)} ({category2_matches[j]/len(data)*100:.1f}%)\")\n",
    "    print(f\"Top{j+1} - ë‘˜ ë‹¤ ì¼ì¹˜: {both_matches[j]}/{len(data)} ({both_matches[j]/len(data)*100:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "print(f\"ğŸ‰ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ Top3 ìœ ì‚¬í•œ ìƒ˜í”Œ ì¶”ê°€ ì™„ë£Œ!\")\n",
    "print(f\"ì´ {len(data)}ê°œ ìƒ˜í”Œì— ëŒ€í•´ ê°ê° Top3 ìœ ì‚¬í•œ ìƒ˜í”Œ ì •ë³´ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì»¬ëŸ¼ ì •ë³´ ìµœì¢… í™•ì¸\n",
    "print(f\"\\nğŸ“‹ ìµœì¢… ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ ëª©ë¡:\")\n",
    "for col in data.columns:\n",
    "    print(f\"  - {col}\")\n",
    "    \n",
    "print(f\"\\në°ì´í„° ì €ì¥ ê¶Œì¥ì‚¬í•­:\")\n",
    "print(f\"data.to_excel('enhanced_data_with_similarity.xlsx', index=False)\")\n",
    "print(f\"# ë˜ëŠ”\")\n",
    "print(f\"data.to_csv('enhanced_data_with_similarity.csv', index=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e9ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('enhanced_data_with_similarity.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e02917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'C:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\data\\enhanced_data_with_similarity.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b5067f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['re_category1']!=data['category1'])|(data['re_category2']!=data['category2'])].to_excel('dif.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "890f8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data = data[(data['re_category1']==data['category1'])&(data['re_category2']==data['category2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad90c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_label = pd.read_excel(r'C:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\data\\dif.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc20f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3359 entries, 0 to 3358\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0          3359 non-null   int64  \n",
      " 1   generator_context   3359 non-null   object \n",
      " 2   category1           3359 non-null   object \n",
      " 3   category2           3359 non-null   object \n",
      " 4   input_context       3359 non-null   object \n",
      " 5   original_index      664 non-null    float64\n",
      " 6   augmentation_index  2695 non-null   float64\n",
      " 7   re_category1        3359 non-null   object \n",
      " 8   re_category2        3359 non-null   object \n",
      " 9   vector              3359 non-null   object \n",
      " 10  top1_context        3359 non-null   object \n",
      " 11  top1_category       3359 non-null   object \n",
      " 12  top2_context        3359 non-null   object \n",
      " 13  top2_category       3359 non-null   object \n",
      " 14  top3_context        3359 non-null   object \n",
      " 15  top3_category       3359 non-null   object \n",
      "dtypes: float64(2), int64(1), object(13)\n",
      "memory usage: 420.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([good_data, re_label], axis=0, ignore_index=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5d1688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:, :9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b7dde",
   "metadata": {},
   "source": [
    "### category1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c27bb42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ìƒíƒœ í™•ì¸:\n",
      "í•„í„°ë§ëœ ë°ì´í„° ê°œìˆ˜: 3359\n",
      "ë²¡í„° íƒ€ì…: <class 'list'>\n",
      "ë²¡í„° ê¸¸ì´: 1024\n",
      "ë²¡í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì…ë‹ˆë‹¤. ì§ì ‘ ë³€í™˜í•©ë‹ˆë‹¤...\n",
      "X shape: (3359, 1024)\n",
      "y shape: (3359,)\n",
      "âœ… Xì™€ yì˜ í¬ê¸°ê°€ ì¼ì¹˜í•©ë‹ˆë‹¤!\n",
      "âœ… ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "# í•„í„°ë§ëœ ë°ì´í„°ë¡œ ë²¡í„°ì™€ ë¼ë²¨ ìƒì„±\n",
    "print(\"ë°ì´í„° ìƒíƒœ í™•ì¸:\")\n",
    "print(f\"í•„í„°ë§ëœ ë°ì´í„° ê°œìˆ˜: {len(data)}\")\n",
    "print(f\"ë²¡í„° íƒ€ì…: {type(data['vector'].iloc[0])}\")\n",
    "print(f\"ë²¡í„° ê¸¸ì´: {len(data['vector'].iloc[0])}\")\n",
    "\n",
    "# ë²¡í„°ê°€ ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¼ë©´ ì§ì ‘ numpy arrayë¡œ ë³€í™˜\n",
    "if isinstance(data['vector'].iloc[0], list):\n",
    "    print(\"ë²¡í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì…ë‹ˆë‹¤. ì§ì ‘ ë³€í™˜í•©ë‹ˆë‹¤...\")\n",
    "    X = np.vstack(data['vector'].values)\n",
    "    y = data['re_category1'].values  # ë³€ê²½: category1 â†’ re_category1\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "    # í¬ê¸° ì¼ì¹˜ í™•ì¸\n",
    "    if X.shape[0] == y.shape[0]:\n",
    "        print(\"âœ… Xì™€ yì˜ í¬ê¸°ê°€ ì¼ì¹˜í•©ë‹ˆë‹¤!\")\n",
    "    else:\n",
    "        print(f\"âŒ í¬ê¸° ë¶ˆì¼ì¹˜: X {X.shape[0]} vs y {y.shape[0]}\")\n",
    "    \n",
    "    print(\"âœ… ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"ë²¡í„° í˜•íƒœì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "wqea3alqoni",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ì‹¤ì œ test_data ë¡œë“œ ë° í‰ê°€\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ë³¸ ì •ë³´:\n",
      "ë°ì´í„° í¬ê¸°: (664, 5)\n",
      "ì»¬ëŸ¼ë“¤: ['index', 'context', 'annotations_split', 'category1', 'category2']\n",
      "\n",
      "ë°ì´í„° ìƒ˜í”Œ:\n",
      "   index  \\\n",
      "0      0   \n",
      "1      1   \n",
      "2      2   \n",
      "3      3   \n",
      "4      4   \n",
      "\n",
      "                                                                                              context  \\\n",
      "0                                            ë³´ëŠ”ë™ì•ˆ ë„ˆë¬´ í–‰ë³µí–ˆê³  ì´ˆì½œë ›ì´ ë„ˆë¬´ ë¨¹ê³ ì‹¶ì—ˆê³  í‹°ëª¨ì‹œê°€ ì˜ìƒê²¼ê³  ìš¸ì–´!!í•˜ëŠ”ë¶€ë¶„ì´ ìˆì–´ì„œ ìš¸ì—ˆë‹¤ë„¤ìš”   \n",
      "1                                            ì–´ë¦´ ë•Œ ê°€ ë³´ê³  ë¹•ìŠ¤ëŠ” ê±°ì˜ ì²˜ìŒì¸ë°(ê¸°ì–µì— ì—†ìŒ) ì§€ê¸ˆ ë”¸ê¸°ì¶•ì œ ê¸°ê°„ì´ë¼ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì‹ì‚¬ í•˜ê³  ì˜´   \n",
      "2  ë¯¸ë¦¬ ê³„ì¢Œë¡œ í™˜ì „í•´ë‘” ëˆì„ í•´ì™¸ì—ì„œ í™˜ì „ìˆ˜ìˆ˜ë£Œ ì—†ì´ ì¸ì¶œ ê°€ëŠ¥í•œ íŠ¸ë ˆë¸”ë¡œê·¸ë¼ëŠ” ì¹´ë“œì¸ë°, ì„ íƒí•  ìˆ˜ ìˆëŠ” ë””ìì¸ ì¤‘ì— ì´ ì—¬ê¶Œ ìŠ¤íƒ€ì¼ì´ ë„ˆë¬´ ì„¼ìŠ¤ ìˆê³  ìœ ë‹ˆí¬í•´ì„œ ë§ˆìŒì— ë“ ë‹¤.   \n",
      "3                                                   ìš”ì¦˜ ë²ˆì•„ì›ƒë„ ìê¾¸ ì˜¬ë¼ì˜¤ê³  ë¬´ê¸°ë ¥í•´ì„œ ì¢…ê°•í•˜ê³  êµë¥˜í•˜ê¸°ë„ ë²„ê±°ìš´ ìƒíƒœê°€ ì™€ë¶€ë €ìœ¼ìš”ã… ã…     \n",
      "4                                    í¬ë¼ì„ì”¬ ì¥ë˜¥ë¯¼ì´ ë²”í–‰ ë„êµ¬ ì°¾ìœ¼ë ¤ê³  í™”ì¥ì‹¤ íƒ±í¬ ë’¤ì§€ëŠ”ë° ê±°ê¸°ì— ì§„ì§œ ë˜¥ ë„£ì–´ë†“ì€ ê±° ì§„ì§œ ì›ƒê²¨ ë’¤ì§€ê² ìŒã…‹ã…‹ã…‹ã…‹ã…‹   \n",
      "\n",
      "                                                                  annotations_split  \\\n",
      "0         [['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ê°ë™'], ['ê¸°ì¨', 'ê°ë™'], ['ê¸°ì¨', 'ë§Œì¡±ê°']]   \n",
      "1       [['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°']]   \n",
      "2       [['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°']]   \n",
      "3  [['ìŠ¬í””', 'ë¬´ê¸°ë ¥'], ['ì‹«ì–´í•¨(ìƒíƒœ)', 'ë¬´ê¸°ë ¥'], ['ìŠ¬í””', 'ë¬´ê¸°ë ¥'], ['ìŠ¬í””', 'ë¬´ê¸°ë ¥'], ['ìŠ¬í””', 'ë¬´ê¸°ë ¥']]   \n",
      "4       [['ê¸°ì¨', 'ì¦ê±°ì›€'], ['ê¸°ì¨', 'í†µì¾Œí•¨'], ['ê¸°ì¨', 'í†µì¾Œí•¨'], ['ê¸°ì¨', 'ì¦ê±°ì›€'], ['ê¸°ì¨', 'ì¦ê±°ì›€']]   \n",
      "\n",
      "  category1 category2  \n",
      "0        ê¸°ì¨       ë§Œì¡±ê°  \n",
      "1        ê¸°ì¨       ë§Œì¡±ê°  \n",
      "2        ê¸°ì¨       ë§Œì¡±ê°  \n",
      "3        ìŠ¬í””       ë¬´ê¸°ë ¥  \n",
      "4        ê¸°ì¨       ì¦ê±°ì›€  \n",
      "\n",
      "test_data ì»¬ëŸ¼ í™•ì¸:\n",
      "- index: int64\n",
      "- context: object\n",
      "- annotations_split: object\n",
      "- category1: object\n",
      "- category2: object\n",
      "\n",
      "ì‹ë³„ëœ ì»¬ëŸ¼:\n",
      "í…ìŠ¤íŠ¸ ì»¬ëŸ¼: context\n",
      "Category1 ì»¬ëŸ¼: category1\n",
      "\n",
      "âœ… í•„ìš”í•œ ì»¬ëŸ¼ë“¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: 664\n",
      "Category1 í´ë˜ìŠ¤ë“¤: ['ê¸°ì¨' 'ìŠ¬í””' 'ì‹«ì–´í•¨(ìƒíƒœ)' 'ë¯¸ì›€(ìƒëŒ€ë°©)' 'ë‘ë ¤ì›€' 'ìˆ˜ì¹˜ì‹¬' 'ìš•ë§' 'ë¶„ë…¸' 'ì‚¬ë‘' 'ì¤‘ë¦½']\n"
     ]
    }
   ],
   "source": [
    "# 9. ì‹¤ì œ test_dataë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
    "\n",
    "print(\"ğŸ“ ì‹¤ì œ test_data ë¡œë“œ ë° í‰ê°€\\n\")\n",
    "\n",
    "# test_data ë¡œë“œ\n",
    "test_data = pd.read_excel(r'C:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\data\\ì¦ê°•í• ë°ì´í„°33.xlsx')\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ë³¸ ì •ë³´:\")\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {test_data.shape}\")\n",
    "print(f\"ì»¬ëŸ¼ë“¤: {list(test_data.columns)}\")\n",
    "print(\"\\në°ì´í„° ìƒ˜í”Œ:\")\n",
    "print(test_data.head())\n",
    "\n",
    "# test_dataì—ì„œ í…ìŠ¤íŠ¸ì™€ category1 ì»¬ëŸ¼ í™•ì¸\n",
    "print(f\"\\ntest_data ì»¬ëŸ¼ í™•ì¸:\")\n",
    "for col in test_data.columns:\n",
    "    print(f\"- {col}: {test_data[col].dtype}\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì»¬ëŸ¼ê³¼ category1 ì»¬ëŸ¼ ì‹ë³„ (ì»¬ëŸ¼ëª…ì— ë”°ë¼ ì¡°ì • í•„ìš”)\n",
    "text_column = None\n",
    "category1_column = None\n",
    "\n",
    "# ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…ë“¤\n",
    "possible_text_columns = ['context', 'text', 'content', 'sentence', 'ë‚´ìš©', 'ë¬¸ì¥']\n",
    "for col in test_data.columns:\n",
    "    if any(keyword in col.lower() for keyword in possible_text_columns):\n",
    "        text_column = col\n",
    "        break\n",
    "\n",
    "# ê°€ëŠ¥í•œ category1 ì»¬ëŸ¼ëª…ë“¤\n",
    "possible_cat1_columns = ['category1', 'cat1', 'label', 'ê°ì •', 'ì¹´í…Œê³ ë¦¬1']\n",
    "for col in test_data.columns:\n",
    "    if any(keyword in col.lower() for keyword in possible_cat1_columns):\n",
    "        category1_column = col\n",
    "        break\n",
    "\n",
    "print(f\"\\nì‹ë³„ëœ ì»¬ëŸ¼:\")\n",
    "print(f\"í…ìŠ¤íŠ¸ ì»¬ëŸ¼: {text_column}\")\n",
    "print(f\"Category1 ì»¬ëŸ¼: {category1_column}\")\n",
    "\n",
    "if text_column and category1_column:\n",
    "    print(f\"\\nâœ… í•„ìš”í•œ ì»¬ëŸ¼ë“¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: {len(test_data)}\")\n",
    "    print(f\"Category1 í´ë˜ìŠ¤ë“¤: {test_data[category1_column].unique()}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ í•„ìš”í•œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì§€ì •í•´ì£¼ì„¸ìš”.\")\n",
    "    print(\"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤:\")\n",
    "    for i, col in enumerate(test_data.columns):\n",
    "        print(f\"{i}: {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "mqo1whwl3l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”§ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²¡í„° ìƒì„± ë° ë¼ë²¨ ì¤€ë¹„...\n",
      "í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:\n",
      "1. ë³´ëŠ”ë™ì•ˆ ë„ˆë¬´ í–‰ë³µí–ˆê³  ì´ˆì½œë ›ì´ ë„ˆë¬´ ë¨¹ê³ ì‹¶ì—ˆê³  í‹°ëª¨ì‹œê°€ ì˜ìƒê²¼ê³  ìš¸ì–´!!í•˜ëŠ”ë¶€ë¶„ì´ ìˆì–´ì„œ ìš¸ì—ˆë‹¤ë„¤ìš”...\n",
      "2. ì–´ë¦´ ë•Œ ê°€ ë³´ê³  ë¹•ìŠ¤ëŠ” ê±°ì˜ ì²˜ìŒì¸ë°(ê¸°ì–µì— ì—†ìŒ) ì§€ê¸ˆ ë”¸ê¸°ì¶•ì œ ê¸°ê°„ì´ë¼ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì‹ì‚¬ í•˜ê³  ì˜´...\n",
      "3. ë¯¸ë¦¬ ê³„ì¢Œë¡œ í™˜ì „í•´ë‘” ëˆì„ í•´ì™¸ì—ì„œ í™˜ì „ìˆ˜ìˆ˜ë£Œ ì—†ì´ ì¸ì¶œ ê°€ëŠ¥í•œ íŠ¸ë ˆë¸”ë¡œê·¸ë¼ëŠ” ì¹´ë“œì¸ë°, ì„ íƒí•  ìˆ˜ ìˆëŠ” ë””ìì¸ ì¤‘ì— ì´ ì—¬ê¶Œ ìŠ¤íƒ€ì¼ì´ ë„ˆë¬´ ì„¼ìŠ¤ ìˆê³  ìœ ë‹ˆí¬í•´ì„œ ë§ˆìŒì— ë“ ë‹¤....\n",
      "\n",
      "ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²¡í„°í™” ì¤‘...\n",
      "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:\n",
      "test_X shape: (664, 1024)\n",
      "test_y_actual shape: (664,)\n",
      "test_y_actual_cat2 shape: (664,)\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° category1 í´ë˜ìŠ¤: 10ê°œ\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° category2 í´ë˜ìŠ¤: 64ê°œ\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìœ„í•œ ë²¡í„° ìƒì„± ë° ë¼ë²¨ ì¤€ë¹„\n",
    "print(f\"\\nğŸ”§ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²¡í„° ìƒì„± ë° ë¼ë²¨ ì¤€ë¹„...\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë²¡í„° ìƒì„±\n",
    "test_texts = test_data['context'].values\n",
    "print(f\"í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. {test_texts[i][:100]}...\")\n",
    "\n",
    "print(f\"\\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²¡í„°í™” ì¤‘...\")\n",
    "test_vectors = [embeddings_model.encode(text).tolist() for text in test_texts]\n",
    "test_X = np.array(test_vectors)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¼ë²¨ ì¤€ë¹„\n",
    "test_y_actual = test_data['category1'].values\n",
    "test_y_actual_cat2 = test_data['category2'].values\n",
    "\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:\")\n",
    "print(f\"test_X shape: {test_X.shape}\")\n",
    "print(f\"test_y_actual shape: {test_y_actual.shape}\")\n",
    "print(f\"test_y_actual_cat2 shape: {test_y_actual_cat2.shape}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° category1 í´ë˜ìŠ¤: {len(np.unique(test_y_actual))}ê°œ\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° category2 í´ë˜ìŠ¤: {len(np.unique(test_y_actual_cat2))}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89mic30g7jg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤– ë³µí•© ë¼ë²¨ ë°©ì‹ AutoGluon ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
      "âœ… AutoGluon ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ\n",
      "ë³µí•© ë¼ë²¨ ì˜ˆì‹œ: ['ê¸°ì¨_ê°ë™' 'ê¸°ì¨_ê°ë™' 'ê¸°ì¨_ê°ë™' 'ê¸°ì¨_ê°ë™' 'ê¸°ì¨_ê°ë™']\n",
      "ì´ ë³µí•© ë¼ë²¨ ì¢…ë¥˜: 76ê°œ\n",
      "\n",
      "ğŸ“ˆ ë³µí•© ë¼ë²¨ ìƒìœ„ 10ê°œ ë¶„í¬:\n",
      "ìŠ¬í””_ë¬´ê¸°ë ¥      98\n",
      "ìˆ˜ì¹˜ì‹¬_ë¶€ë„ëŸ¬ì›€    96\n",
      "ìŠ¬í””_ì™¸ë¡œì›€      90\n",
      "ê¸°ì¨_ê¸°ëŒ€ê°      84\n",
      "ë‘ë ¤ì›€_ë†€ëŒ      83\n",
      "ê¸°ì¨_ê³µê°       83\n",
      "ê¸°ì¨_í¸ì•ˆí•¨      80\n",
      "ìŠ¬í””_í—ˆë§       78\n",
      "ê¸°ì¨_ì•ˆì •ê°      73\n",
      "ë¶„ë…¸_ë¶ˆì¾Œ       72\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âš ï¸ í¬ì†Œí•œ ë¼ë²¨ (7ê°œ): ['ì¤‘ë¦½_ê³µê°', 'ìˆ˜ì¹˜ì‹¬_ìˆ˜ì¹˜ì‹¬', 'ë¯¸ì›€(ìƒëŒ€ë°©)_ë¶ˆì¾Œ', 'ìŠ¬í””_ë‹µë‹µí•¨', 'ìŠ¬í””_ì£„ì±…ê°', 'ì‚¬ë‘_ê³µê°', 'ìŠ¬í””_ê³µê°']\n",
      "ì´ëŸ¬í•œ ë¼ë²¨ë“¤ì€ í›ˆë ¨ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Š AutoGluon í›ˆë ¨ ë°ì´í„° ì¤€ë¹„...\n",
      "í›ˆë ¨ ë°ì´í„° í¬ê¸°: (3359, 1025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          16\n",
      "Memory Avail:       14.78 GB / 31.91 GB (46.3%)\n",
      "Disk Space Avail:   73.07 GB / 465.12 GB (15.7%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=2, num_bag_folds=5, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"c:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\autogluon_combined_f1_model\\ds_sub_fit\\sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ì¡´ ëª¨ë¸ í´ë” autogluon_combined_f1_model ì‚­ì œë¨\n",
      "\n",
      "ğŸš€ AutoGluon F1-Score ìµœì í™” ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
      "============================================================\n",
      "ë³µí•© ë¼ë²¨ ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘... (ì˜ˆìƒ ì†Œìš” ì‹œê°„: 5-10ë¶„)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     WeightedEnsemble_L2       0.611152   0.626684    f1_macro        1.280977       0.348022   54.318815                 0.004001                0.003001           0.272061            2       True          4\n",
      "1  NeuralNetFastAI_BAG_L1       0.608330   0.622798    f1_macro        1.072931       0.185041   14.538218                 1.072931                0.185041          14.538218            1       True          1\n",
      "2     WeightedEnsemble_L4       0.605447   0.637328    f1_macro        1.531044       0.692788   95.697352                 0.007002                0.004000           0.352452            4       True         10\n",
      "3  NeuralNetFastAI_BAG_L3       0.587420   0.610210    f1_macro        1.692081       0.909314  109.469433                 0.168039                0.220526          14.124533            3       True          8\n",
      "4  NeuralNetFastAI_BAG_L2       0.581237   0.619527    f1_macro        1.430011       0.582268   68.185716                 0.153035                0.237247          14.138962            2       True          5\n",
      "5     WeightedEnsemble_L3       0.580401   0.620483    f1_macro        1.527043       0.692788   95.549434                 0.003001                0.004000           0.204534            3       True          7\n",
      "6       LightGBMXT_BAG_L2       0.496096   0.430220    f1_macro        1.371008       0.451540   81.205938                 0.094031                0.106520          27.159184            2       True          6\n",
      "7       LightGBMXT_BAG_L3       0.458464   0.358741    f1_macro        1.594057       0.763795  103.127346                 0.070014                0.075007           7.782446            3       True          9\n",
      "8       LightGBMXT_BAG_L1       0.446330   0.374656    f1_macro        0.162036       0.109023   35.847080                 0.162036                0.109023          35.847080            1       True          2\n",
      "9         LightGBM_BAG_L1       0.209314   0.197870    f1_macro        0.042010       0.050956    3.661456                 0.042010                0.050956           3.661456            1       True          3\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t163s\t = DyStack   runtime |\t437s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 437s\n",
      "AutoGluon will save models to \"c:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\autogluon_combined_f1_model\"\n",
      "Train Data Rows:    3359\n",
      "Train Data Columns: 1024\n",
      "Label Column:       combined_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 67 out of 76 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9937481393271808\n",
      "Train Data Class Count: 67\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    15057.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 26.08 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1024 | ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1024 | ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', ...]\n",
      "\t5.0s = Fit runtime\n",
      "\t1024 features in original data used to generate 1024 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.08 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 432.16s of the 432.15s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=1.87%)\n",
      "\t0.6115\t = Validation score   (f1_macro)\n",
      "\t14.38s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 414.04s of the 414.04s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=7.02%)\n",
      "\t0.5382\t = Validation score   (f1_macro)\n",
      "\t334.31s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 76.16s of the 76.16s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.47%)\n",
      "\t0.3453\t = Validation score   (f1_macro)\n",
      "\t63.54s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 9.53s of the 9.52s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 106 due to low time. Expected time usage reduced from 24.9s -> 9.1s...\n",
      "\t0.3719\t = Validation score   (f1_macro)\n",
      "\t3.57s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 5.26s of the 5.25s of remaining time.\n",
      "\tWarning: Model is expected to require 56.2s to train, which exceeds the maximum time limit of 4.9s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 3.35s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.455, 'RandomForestGini_BAG_L1': 0.364, 'LightGBMXT_BAG_L1': 0.182}\n",
      "\t0.6213\t = Validation score   (f1_macro)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 434.34s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 672.9 rows/s (668 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\autogluon_combined_f1_model\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“‹ AutoGluon ëª¨ë¸ ë¦¬ë”ë³´ë“œ (F1-macro ê¸°ì¤€):\n",
      "                     model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0      WeightedEnsemble_L2   0.621278    f1_macro       1.250298  352.629694   \n",
      "1   NeuralNetFastAI_BAG_L1   0.611499    f1_macro       0.189046   14.384632   \n",
      "2        LightGBMXT_BAG_L1   0.538246    f1_macro       0.739181  334.311806   \n",
      "3  RandomForestGini_BAG_L1   0.371932    f1_macro       0.319072    3.574869   \n",
      "4          LightGBM_BAG_L1   0.345299    f1_macro       0.142033   63.536590   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.003000           0.358388            2       True   \n",
      "1                0.189046          14.384632            1       True   \n",
      "2                0.739181         334.311806            1       True   \n",
      "3                0.319072           3.574869            1       True   \n",
      "4                0.142033          63.536590            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          5  \n",
      "1          1  \n",
      "2          2  \n",
      "3          4  \n",
      "4          3  \n",
      "\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: WeightedEnsemble_L2\n",
      "ê²€ì¦ F1-macro Score: 0.6213\n",
      "\n",
      "ğŸ¯ AutoGluon ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡...\n",
      "âœ… AutoGluon ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“Š AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ ì„±ëŠ¥:\n",
      "==================================================\n",
      "ì •í™•ë„:\n",
      "  Category1: 0.5331 (53.31%)\n",
      "  Category2: 0.3434 (34.34%)\n",
      "  ë³µí•© ë¼ë²¨: 0.3208 (32.08%)\n",
      "\n",
      "F1-Score (macro):\n",
      "  Category1: 0.3936\n",
      "  Category2: 0.2410\n",
      "  ë³µí•© ë¼ë²¨: 0.2134\n",
      "\n",
      "ğŸ¯ ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: 0.3208 (32.08%)\n",
      "ì •ë‹µ ê°œìˆ˜: 213/664\n",
      "\n",
      "âœ… AutoGluon F1-ìµœì í™” ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ!\n",
      "\n",
      "ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: autogluon_combined_f1_model\n",
      "ğŸ“¥ ëª¨ë¸ ì¬ë¡œë“œ ë°©ë²•:\n",
      "combined_predictor = TabularPredictor.load('autogluon_combined_f1_model')\n",
      "\n",
      "ğŸ‰ ìµœì¢… ì„±ê³¼ ìš”ì•½:\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: WeightedEnsemble_L2\n",
      "ğŸ“ˆ ê²€ì¦ F1-macro: 0.6213\n",
      "ğŸ“Š í…ŒìŠ¤íŠ¸ F1-macro: 0.2134\n",
      "ğŸ¯ ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ì •ë‹µë¥ : 32.08%\n"
     ]
    }
   ],
   "source": [
    "# ë³µí•© ë¼ë²¨ ë°©ì‹ AutoGluon ëª¨ë¸ í›ˆë ¨\n",
    "print(f\"\\nğŸ¤– ë³µí•© ë¼ë²¨ ë°©ì‹ AutoGluon ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "try:\n",
    "    from autogluon.tabular import TabularPredictor\n",
    "    print(\"âœ… AutoGluon ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ\")\n",
    "    autogluon_available = True\n",
    "except ImportError:\n",
    "    print(\"âŒ AutoGluonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:\")\n",
    "    print(\"pip install autogluon\")\n",
    "    autogluon_available = False\n",
    "    raise ImportError(\"AutoGluonì´ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ë³µí•© ë¼ë²¨ ìƒì„±\n",
    "combined_labels = y + \"_\" + data['re_category2'].values\n",
    "print(f\"ë³µí•© ë¼ë²¨ ì˜ˆì‹œ: {combined_labels[:5]}\")\n",
    "print(f\"ì´ ë³µí•© ë¼ë²¨ ì¢…ë¥˜: {len(np.unique(combined_labels))}ê°œ\")\n",
    "\n",
    "# ë³µí•© ë¼ë²¨ ë¶„í¬ í™•ì¸\n",
    "label_counts = pd.Series(combined_labels).value_counts()\n",
    "print(f\"\\nğŸ“ˆ ë³µí•© ë¼ë²¨ ìƒìœ„ 10ê°œ ë¶„í¬:\")\n",
    "print(label_counts.head(10))\n",
    "\n",
    "# í¬ì†Œí•œ ë¼ë²¨ í™•ì¸ (3ê°œ ë¯¸ë§Œ)\n",
    "rare_labels = label_counts[label_counts < 3]\n",
    "if len(rare_labels) > 0:\n",
    "    print(f\"\\nâš ï¸ í¬ì†Œí•œ ë¼ë²¨ ({len(rare_labels)}ê°œ): {list(rare_labels.index[:10])}\")\n",
    "    print(\"ì´ëŸ¬í•œ ë¼ë²¨ë“¤ì€ í›ˆë ¨ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# AutoGluonìš© í›ˆë ¨ ë°ì´í„° ì¤€ë¹„\n",
    "print(f\"\\nğŸ“Š AutoGluon í›ˆë ¨ ë°ì´í„° ì¤€ë¹„...\")\n",
    "train_combined_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "train_combined_df['combined_label'] = combined_labels\n",
    "\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {train_combined_df.shape}\")\n",
    "\n",
    "# AutoGluon ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "combined_save_path = \"autogluon_combined_f1_model\"\n",
    "if os.path.exists(combined_save_path):\n",
    "    import shutil\n",
    "    shutil.rmtree(combined_save_path)\n",
    "    print(f\"ê¸°ì¡´ ëª¨ë¸ í´ë” {combined_save_path} ì‚­ì œë¨\")\n",
    "\n",
    "# AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ í›ˆë ¨ (F1 ìŠ¤ì½”ì–´ ìµœì í™”)\n",
    "print(f\"\\nğŸš€ AutoGluon F1-Score ìµœì í™” ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "combined_predictor = TabularPredictor(\n",
    "    label='combined_label',\n",
    "    path=combined_save_path,\n",
    "    eval_metric='f1_macro',  # F1-macro ì ìˆ˜ë¡œ ìµœì í™”\n",
    "    problem_type='multiclass'\n",
    ")\n",
    "\n",
    "print(\"ë³µí•© ë¼ë²¨ ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘... (ì˜ˆìƒ ì†Œìš” ì‹œê°„: 5-10ë¶„)\")\n",
    "combined_predictor = combined_predictor.fit(\n",
    "    train_data=train_combined_df,\n",
    "    time_limit=600,  # 10ë¶„ ì œí•œ (ë” ì¢‹ì€ ì„±ëŠ¥ì„ ìœ„í•´)\n",
    "    presets='best_quality',  # ìµœê³  í’ˆì§ˆ í”„ë¦¬ì…‹\n",
    "    num_bag_folds=5,  # 5-fold êµì°¨ ê²€ì¦\n",
    "    num_bag_sets=1,\n",
    "    num_stack_levels=2,  # ìŠ¤íƒœí‚¹ ë ˆë²¨ ì¦ê°€\n",
    "    hyperparameters='default'  # ë‹¤ì¤‘ í´ë˜ìŠ¤ ìµœì í™”\n",
    ")\n",
    "\n",
    "print(\"âœ… AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "\n",
    "# ëª¨ë¸ ë¦¬ë”ë³´ë“œ ì¶œë ¥\n",
    "print(f\"\\nğŸ“‹ AutoGluon ëª¨ë¸ ë¦¬ë”ë³´ë“œ (F1-macro ê¸°ì¤€):\")\n",
    "leaderboard = combined_predictor.leaderboard()\n",
    "print(leaderboard.head(10))\n",
    "\n",
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´\n",
    "best_model = leaderboard.iloc[0]['model']\n",
    "best_f1_score = leaderboard.iloc[0]['score_val']\n",
    "print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model}\")\n",
    "print(f\"ê²€ì¦ F1-macro Score: {best_f1_score:.4f}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "print(f\"\\nğŸ¯ AutoGluon ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡...\")\n",
    "test_combined_df = pd.DataFrame(test_X, columns=[f'feature_{i}' for i in range(test_X.shape[1])])\n",
    "\n",
    "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "test_pred_combined_labels = combined_predictor.predict(test_combined_df)\n",
    "test_pred_proba = combined_predictor.predict_proba(test_combined_df)\n",
    "\n",
    "print(f\"âœ… AutoGluon ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "\n",
    "# ì˜ˆì¸¡ëœ ë³µí•© ë¼ë²¨ì„ ê°œë³„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬\n",
    "test_pred_split = test_pred_combined_labels.str.split('_', expand=True)\n",
    "test_pred_cat1 = test_pred_split[0].values\n",
    "test_pred_cat2 = test_pred_split[1].values\n",
    "\n",
    "# ì‹¤ì œ ë¼ë²¨ë„ ë³µí•© ë¼ë²¨ í˜•íƒœë¡œ ìƒì„± (ë¹„êµìš©)\n",
    "test_actual_combined_labels = test_y_actual + \"_\" + test_y_actual_cat2\n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "cat1_accuracy = accuracy_score(test_y_actual, test_pred_cat1)\n",
    "cat2_accuracy = accuracy_score(test_y_actual_cat2, test_pred_cat2)\n",
    "combined_accuracy = accuracy_score(test_actual_combined_labels, test_pred_combined_labels)\n",
    "\n",
    "# F1 ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "f1_cat1 = f1_score(test_y_actual, test_pred_cat1, average='macro')\n",
    "f1_cat2 = f1_score(test_y_actual_cat2, test_pred_cat2, average='macro') \n",
    "f1_combined = f1_score(test_actual_combined_labels, test_pred_combined_labels, average='macro')\n",
    "\n",
    "print(f\"\\nğŸ“Š AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ ì„±ëŠ¥:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ì •í™•ë„:\")\n",
    "print(f\"  Category1: {cat1_accuracy:.4f} ({cat1_accuracy*100:.2f}%)\")\n",
    "print(f\"  Category2: {cat2_accuracy:.4f} ({cat2_accuracy*100:.2f}%)\")\n",
    "print(f\"  ë³µí•© ë¼ë²¨: {combined_accuracy:.4f} ({combined_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nF1-Score (macro):\")\n",
    "print(f\"  Category1: {f1_cat1:.4f}\")\n",
    "print(f\"  Category2: {f1_cat2:.4f}\")\n",
    "print(f\"  ë³µí•© ë¼ë²¨: {f1_combined:.4f}\")\n",
    "\n",
    "# ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µì¸ ê²½ìš°\n",
    "both_correct = (test_pred_cat1 == test_y_actual) & (test_pred_cat2 == test_y_actual_cat2)\n",
    "both_accuracy = both_correct.mean()\n",
    "print(f\"\\nğŸ¯ ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: {both_accuracy:.4f} ({both_accuracy*100:.2f}%)\")\n",
    "print(f\"ì •ë‹µ ê°œìˆ˜: {both_correct.sum()}/{len(test_y_actual)}\")\n",
    "\n",
    "print(f\"\\nâœ… AutoGluon F1-ìµœì í™” ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ!\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ ì •ë³´\n",
    "print(f\"\\nğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {combined_save_path}\")\n",
    "print(f\"ğŸ“¥ ëª¨ë¸ ì¬ë¡œë“œ ë°©ë²•:\")\n",
    "print(f\"combined_predictor = TabularPredictor.load('{combined_save_path}')\")\n",
    "\n",
    "# ìµœì¢… ì„±ê³¼ ìš”ì•½\n",
    "print(f\"\\nğŸ‰ ìµœì¢… ì„±ê³¼ ìš”ì•½:\")\n",
    "print(f\"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model}\")\n",
    "print(f\"ğŸ“ˆ ê²€ì¦ F1-macro: {best_f1_score:.4f}\")\n",
    "print(f\"ğŸ“Š í…ŒìŠ¤íŠ¸ F1-macro: {f1_combined:.4f}\")\n",
    "print(f\"ğŸ¯ ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ì •ë‹µë¥ : {both_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tbujoi3bkj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ ë°ëª¨ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹œìŠ¤í…œ\n",
      "============================================================\n",
      "ğŸŒŸ AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ì„ ì‚¬ìš©í•œ ê°ì • í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ë°ëª¨:\n",
      "ì´ 7ê°œì˜ ì˜ˆì‹œ í…ìŠ¤íŠ¸ë¡œ ê°ì • ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "ğŸ† ì‚¬ìš© ëª¨ë¸: WeightedEnsemble_L2 (F1-macro: 0.6213)\n",
      "================================================================================\n",
      "[ì˜ˆì‹œ 1]\n",
      "ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: ì†ìˆ˜ ë§Œë“  ì‘ì€ í¬ì¥ì„ ëëƒˆë‹¤. ê³„íšëŒ€ë¡œ ë§ˆë¬´ë¦¬ë¼ ë§ˆìŒì†ìœ¼ë¡œ ì€ê·¼íˆ ê¸°ë»¤ë‹¤.\n",
      "ğŸ¯ AutoGluon ì˜ˆì¸¡ ê²°ê³¼:\n",
      "   ë³µí•© ë¼ë²¨: ê¸°ì¨_ë§Œì¡±ê° (ì‹ ë¢°ë„: 0.609)\n",
      "   Category1 (ëŒ€ë¶„ë¥˜): ê¸°ì¨\n",
      "   Category2 (ì„¸ë¶„ë¥˜): ë§Œì¡±ê°\n",
      "ğŸ“Š ìƒìœ„ 5ê°œ ë³µí•© ë¼ë²¨ í›„ë³´:\n",
      "   1. ê¸°ì¨_ë§Œì¡±ê° (ê¸°ì¨|ë§Œì¡±ê°): 0.609\n",
      "   2. ê¸°ì¨_ê¸°ëŒ€ê° (ê¸°ì¨|ê¸°ëŒ€ê°): 0.084\n",
      "   3. ê¸°ì¨_ìë‘ìŠ¤ëŸ¬ì›€ (ê¸°ì¨|ìë‘ìŠ¤ëŸ¬ì›€): 0.033\n",
      "   4. ê¸°ì¨_ê°ë™ (ê¸°ì¨|ê°ë™): 0.018\n",
      "   5. ìŠ¬í””_ì™¸ë¡œì›€ (ìŠ¬í””|ì™¸ë¡œì›€): 0.014\n",
      "\n",
      "[ì˜ˆì‹œ 2]\n",
      "ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: í•™ìŠµ ë…¸íŠ¸ë¥¼ ë§ˆì¹˜ê³  ì •ë¦¬í–ˆë‹¤. ì°¨ê³¡ì°¨ê³¡ ìŒ“ì¸ í”ì ì´ ë§ˆìŒì†ìœ¼ë¡œ ë§Œì¡±ìŠ¤ëŸ¬ì› ë‹¤.\n",
      "ğŸ¯ AutoGluon ì˜ˆì¸¡ ê²°ê³¼:\n",
      "   ë³µí•© ë¼ë²¨: ê¸°ì¨_ë§Œì¡±ê° (ì‹ ë¢°ë„: 0.541)\n",
      "   Category1 (ëŒ€ë¶„ë¥˜): ê¸°ì¨\n",
      "   Category2 (ì„¸ë¶„ë¥˜): ë§Œì¡±ê°\n",
      "ğŸ“Š ìƒìœ„ 5ê°œ ë³µí•© ë¼ë²¨ í›„ë³´:\n",
      "   1. ê¸°ì¨_ë§Œì¡±ê° (ê¸°ì¨|ë§Œì¡±ê°): 0.541\n",
      "   2. ê¸°ì¨_ìë‘ìŠ¤ëŸ¬ì›€ (ê¸°ì¨|ìë‘ìŠ¤ëŸ¬ì›€): 0.049\n",
      "   3. ê¸°ì¨_ìì‹ ê° (ê¸°ì¨|ìì‹ ê°): 0.028\n",
      "   4. ê¸°ì¨_ê³ ë§ˆì›€ (ê¸°ì¨|ê³ ë§ˆì›€): 0.028\n",
      "   5. ê¸°ì¨_ê¸°ëŒ€ê° (ê¸°ì¨|ê¸°ëŒ€ê°): 0.023\n",
      "\n",
      "[ì˜ˆì‹œ 3]\n",
      "ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: ì‘ì€ ëª¨í˜• ë°°ë¥¼ ì¡°ë¦½í•´ ì™„ì„±í–ˆë‹¤. ê³„íšëŒ€ë¡œ ë§ì•„ ë§ˆìŒì´ ì€ì€íˆ ê¸°ë»¤ë‹¤.\n",
      "ğŸ¯ AutoGluon ì˜ˆì¸¡ ê²°ê³¼:\n",
      "   ë³µí•© ë¼ë²¨: ê¸°ì¨_ë§Œì¡±ê° (ì‹ ë¢°ë„: 0.641)\n",
      "   Category1 (ëŒ€ë¶„ë¥˜): ê¸°ì¨\n",
      "   Category2 (ì„¸ë¶„ë¥˜): ë§Œì¡±ê°\n",
      "ğŸ“Š ìƒìœ„ 5ê°œ ë³µí•© ë¼ë²¨ í›„ë³´:\n",
      "   1. ê¸°ì¨_ë§Œì¡±ê° (ê¸°ì¨|ë§Œì¡±ê°): 0.641\n",
      "   2. ê¸°ì¨_ê¸°ëŒ€ê° (ê¸°ì¨|ê¸°ëŒ€ê°): 0.071\n",
      "   3. ê¸°ì¨_ìë‘ìŠ¤ëŸ¬ì›€ (ê¸°ì¨|ìë‘ìŠ¤ëŸ¬ì›€): 0.025\n",
      "   4. ê¸°ì¨_ê°ë™ (ê¸°ì¨|ê°ë™): 0.022\n",
      "   5. ê¸°ì¨_ê³µê° (ê¸°ì¨|ê³µê°): 0.018\n",
      "\n",
      "[ì˜ˆì‹œ 4]\n",
      "ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: ì¸íŒŒê°€ ë¶ì ì´ëŠ” ì§€í•˜ì² ì—­ ì…êµ¬ì—ì„œ í° ì†Œë¦¬ë¡œ ì¹¨ì„ ë±‰ìœ¼ë©° ì§€ë‚˜ê°€ëŠ” ì‚¬ëŒë“¤ì´ ì§œì¦ì´ ë‚¬ë‹¤.\n",
      "ğŸ¯ AutoGluon ì˜ˆì¸¡ ê²°ê³¼:\n",
      "   ë³µí•© ë¼ë²¨: ë¯¸ì›€(ìƒëŒ€ë°©)_ë¹„ìœ„ìƒí•¨ (ì‹ ë¢°ë„: 0.750)\n",
      "   Category1 (ëŒ€ë¶„ë¥˜): ë¯¸ì›€(ìƒëŒ€ë°©)\n",
      "   Category2 (ì„¸ë¶„ë¥˜): ë¹„ìœ„ìƒí•¨\n",
      "ğŸ“Š ìƒìœ„ 5ê°œ ë³µí•© ë¼ë²¨ í›„ë³´:\n",
      "   1. ë¯¸ì›€(ìƒëŒ€ë°©)_ë¹„ìœ„ìƒí•¨ (ë¯¸ì›€(ìƒëŒ€ë°©)|ë¹„ìœ„ìƒí•¨): 0.750\n",
      "   2. ë¶„ë…¸_ë¶ˆì¾Œ (ë¶„ë…¸|ë¶ˆì¾Œ): 0.140\n",
      "   3. ì‹«ì–´í•¨(ìƒíƒœ)_ë¶ˆí¸í•¨ (ì‹«ì–´í•¨(ìƒíƒœ)|ë¶ˆí¸í•¨): 0.025\n",
      "   4. ìˆ˜ì¹˜ì‹¬_ë¶€ë„ëŸ¬ì›€ (ìˆ˜ì¹˜ì‹¬|ë¶€ë„ëŸ¬ì›€): 0.007\n",
      "   5. ì‚¬ë‘_ë‘ê·¼ê±°ë¦¼ (ì‚¬ë‘|ë‘ê·¼ê±°ë¦¼): 0.007\n",
      "\n",
      "[ì˜ˆì‹œ 5]\n",
      "ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: ì •ë§ ë¯¿ìŒì´ ê¹¨ì ¸ë²„ë ¸ì–´. ì•„ë¬´ë¦¬ ì•½ì†í•´ë„ ë’¤í†µìˆ˜ë¥¼ ë§ëŠ” ê¸°ë¶„, ì´ì   ëˆ„êµ´ ë¯¿ì–´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´.\n",
      "ğŸ¯ AutoGluon ì˜ˆì¸¡ ê²°ê³¼:\n",
      "   ë³µí•© ë¼ë²¨: ë¯¸ì›€(ìƒëŒ€ë°©)_ë¶ˆì‹ ê° (ì‹ ë¢°ë„: 0.860)\n",
      "   Category1 (ëŒ€ë¶„ë¥˜): ë¯¸ì›€(ìƒëŒ€ë°©)\n",
      "   Category2 (ì„¸ë¶„ë¥˜): ë¶ˆì‹ ê°\n",
      "ğŸ“Š ìƒìœ„ 5ê°œ ë³µí•© ë¼ë²¨ í›„ë³´:\n",
      "   1. ë¯¸ì›€(ìƒëŒ€ë°©)_ë¶ˆì‹ ê° (ë¯¸ì›€(ìƒëŒ€ë°©)|ë¶ˆì‹ ê°): 0.860\n",
      "   2. ë¶„ë…¸_ì›ë§ (ë¶„ë…¸|ì›ë§): 0.014\n",
      "   3. ìŠ¬í””_ì‹¤ë§ (ìŠ¬í””|ì‹¤ë§): 0.014\n",
      "   4. ìŠ¬í””_ì–µìš¸í•¨ (ìŠ¬í””|ì–µìš¸í•¨): 0.010\n",
      "   5. ìŠ¬í””_ì ˆë§ (ìŠ¬í””|ì ˆë§): 0.007\n",
      "\n",
      "[ì˜ˆì‹œ 6]\n",
      "ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: ì €ë… ëª¨ì„ì—ì„œ ë‚´ ê¸°ì—¬ë¥¼ ë¬´ì‹œí•œ ì±„ ë‹¤ë¥¸ ì‚¬ëŒì´ ì¹­ì°¬ë°›ëŠ” ê±¸ ë³´ë©° ë§ˆìŒì†ì— ì°¨ì˜¤ë¥´ëŠ” ì‹¸ëŠ˜í•œ ë¶ˆì¾Œê°ì´ ë©ˆì¶”ì§€ ì•Šì•˜ë‹¤.\n",
      "ğŸ¯ AutoGluon ì˜ˆì¸¡ ê²°ê³¼:\n",
      "   ë³µí•© ë¼ë²¨: ë¶„ë…¸_ë¶ˆì¾Œ (ì‹ ë¢°ë„: 0.730)\n",
      "   Category1 (ëŒ€ë¶„ë¥˜): ë¶„ë…¸\n",
      "   Category2 (ì„¸ë¶„ë¥˜): ë¶ˆì¾Œ\n",
      "ğŸ“Š ìƒìœ„ 5ê°œ ë³µí•© ë¼ë²¨ í›„ë³´:\n",
      "   1. ë¶„ë…¸_ë¶ˆì¾Œ (ë¶„ë…¸|ë¶ˆì¾Œ): 0.730\n",
      "   2. ë¯¸ì›€(ìƒëŒ€ë°©)_ì‹œê¸°ì‹¬ (ë¯¸ì›€(ìƒëŒ€ë°©)|ì‹œê¸°ì‹¬): 0.055\n",
      "   3. ìŠ¬í””_ì–µìš¸í•¨ (ìŠ¬í””|ì–µìš¸í•¨): 0.026\n",
      "   4. ë¶„ë…¸_ì›ë§ (ë¶„ë…¸|ì›ë§): 0.016\n",
      "   5. ìŠ¬í””_ì™¸ë¡œì›€ (ìŠ¬í””|ì™¸ë¡œì›€): 0.012\n",
      "\n",
      "[ì˜ˆì‹œ 7]\n",
      "ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: ì‹ë‹¹ì—ì„œ ê·¸ê°€ ì£¼ë¬¸ì„ ë°›ê³ ë„ ë¬´í‘œì •í•˜ê²Œ ëŒì•„ì„œì, ë‚˜ëŠ” ê¹Šì€ ì“¸ì“¸í•¨ê³¼ í•¨ê»˜ ì°¨ê°€ìš´ ë²½ì— ë§‰íŒ ê¸°ë¶„ì´ ë“¤ì—ˆë‹¤.\n",
      "ğŸ¯ AutoGluon ì˜ˆì¸¡ ê²°ê³¼:\n",
      "   ë³µí•© ë¼ë²¨: ìŠ¬í””_ì™¸ë¡œì›€ (ì‹ ë¢°ë„: 0.777)\n",
      "   Category1 (ëŒ€ë¶„ë¥˜): ìŠ¬í””\n",
      "   Category2 (ì„¸ë¶„ë¥˜): ì™¸ë¡œì›€\n",
      "ğŸ“Š ìƒìœ„ 5ê°œ ë³µí•© ë¼ë²¨ í›„ë³´:\n",
      "   1. ìŠ¬í””_ì™¸ë¡œì›€ (ìŠ¬í””|ì™¸ë¡œì›€): 0.777\n",
      "   2. ë¯¸ì›€(ìƒëŒ€ë°©)_ì™¸ë©´ (ë¯¸ì›€(ìƒëŒ€ë°©)|ì™¸ë©´): 0.058\n",
      "   3. ë¯¸ì›€(ìƒëŒ€ë°©)_ëƒ‰ë‹´ (ë¯¸ì›€(ìƒëŒ€ë°©)|ëƒ‰ë‹´): 0.020\n",
      "   4. ìŠ¬í””_ì‹¤ë§ (ìŠ¬í””|ì‹¤ë§): 0.018\n",
      "   5. ìŠ¬í””_ì•„í”” (ìŠ¬í””|ì•„í””): 0.008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ ë°ëª¨ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹œìŠ¤í…œ\n",
    "print(\"ğŸ¯ AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ ë°ëª¨ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹œìŠ¤í…œ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def predict_emotion_autogluon(text, predictor=combined_predictor, embedder=embeddings_model):\n",
    "    \"\"\"\n",
    "    AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        text (str): ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸\n",
    "        predictor: AutoGluon ì˜ˆì¸¡ê¸°\n",
    "        embedder: í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸\n",
    "    \n",
    "    Returns:\n",
    "        dict: ì˜ˆì¸¡ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
    "    text_vector = embedder.encode(text)\n",
    "    \n",
    "    # AutoGluon ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜\n",
    "    text_df = pd.DataFrame([text_vector], columns=[f'feature_{i}' for i in range(len(text_vector))])\n",
    "    \n",
    "    # ë³µí•© ë¼ë²¨ ì˜ˆì¸¡\n",
    "    combined_pred = predictor.predict(text_df).iloc[0]\n",
    "    \n",
    "    # ì˜ˆì¸¡ í™•ë¥ \n",
    "    pred_proba = predictor.predict_proba(text_df)\n",
    "    combined_confidence = pred_proba.iloc[0].max()\n",
    "    \n",
    "    # ë³µí•© ë¼ë²¨ì„ ê°œë³„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬\n",
    "    cat1_pred, cat2_pred = combined_pred.split('_')\n",
    "    \n",
    "    # ìƒìœ„ 5ê°œ ë³µí•© ë¼ë²¨ í›„ë³´\n",
    "    top5_proba = pred_proba.iloc[0].nlargest(5)\n",
    "    top5_predictions = [(label, score) for label, score in top5_proba.items()]\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'combined_label': combined_pred,\n",
    "        'category1': cat1_pred,\n",
    "        'category2': cat2_pred,\n",
    "        'confidence': combined_confidence,\n",
    "        'top5_predictions': top5_predictions\n",
    "    }\n",
    "\n",
    "def print_autogluon_prediction(result):\n",
    "    \"\"\"AutoGluon ë³µí•© ë¼ë²¨ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    print(f\"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: {result['text']}\")\n",
    "    print(f\"ğŸ¯ AutoGluon ì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "    print(f\"   ë³µí•© ë¼ë²¨: {result['combined_label']} (ì‹ ë¢°ë„: {result['confidence']:.3f})\")\n",
    "    print(f\"   Category1 (ëŒ€ë¶„ë¥˜): {result['category1']}\")\n",
    "    print(f\"   Category2 (ì„¸ë¶„ë¥˜): {result['category2']}\")\n",
    "    \n",
    "    print(f\"ğŸ“Š ìƒìœ„ 5ê°œ ë³µí•© ë¼ë²¨ í›„ë³´:\")\n",
    "    for i, (label, score) in enumerate(result['top5_predictions']):\n",
    "        cat1, cat2 = label.split('_')\n",
    "        print(f\"   {i+1}. {label} ({cat1}|{cat2}): {score:.3f}\")\n",
    "    print()\n",
    "\n",
    "# ë‹¤ì–‘í•œ ê°ì • í‘œí˜„ ë°ëª¨ í…ìŠ¤íŠ¸ë“¤\n",
    "demo_texts = [\n",
    "    # \"ì˜¤ëŠ˜ ì¹œêµ¬ê°€ ìƒì¼ì„ ë¬¼ì„ ê¹œì§ ì¤€ë¹„í•´ì¤˜ì„œ ì •ë§ ê°ë™ë°›ì•˜ì–´ìš”. ë„ˆë¬´ ê³ ë§ˆì›Œì„œ ëˆˆë¬¼ì´ ë‚¬ì–´ìš”.\",\n",
    "    # \"ì‹œí—˜ì—ì„œ ë–¨ì–´ì ¸ì„œ ë„ˆë¬´ ì‹¤ë§ìŠ¤ëŸ½ê³  ìš°ìš¸í•´ìš”. ì•ìœ¼ë¡œ ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´ìš”.\",\n",
    "    # \"ìƒˆë¡œìš´ ì§ì¥ì—ì„œ ì²«ë‚ ì¸ë° ë„ˆë¬´ ë–¨ë ¤ìš”. ì˜í•  ìˆ˜ ìˆì„ê¹Œ ê±±ì •ì´ ë§ì´ ë¼ìš”.\",\n",
    "    # \"ì—°ì¸ê³¼ í—¤ì–´ì ¸ì„œ ë„ˆë¬´ í™”ë‚˜ê³  ì›ë§ìŠ¤ëŸ¬ì›Œìš”. ë°°ì‹ ê°ì´ ë“¤ì–´ì„œ ì ë„ ëª» ìê² ì–´ìš”.\",\n",
    "    # \"ë¡œë˜ì— ë‹¹ì²¨ë˜ì–´ì„œ ë„ˆë¬´ ê¸°ë»ìš”! ê¿ˆë§Œ ê°™ì•„ì„œ ë¯¿ê¸°ì§€ê°€ ì•Šì•„ìš”.\",\n",
    "    # \"ìƒˆë¡œìš´ ì·¨ë¯¸ë¥¼ ì‹œì‘í•˜ê²Œ ë˜ì–´ì„œ ì„¤ë ˆê³  ê¸°ëŒ€ë¼ìš”. ë­”ê°€ ìƒˆë¡œìš´ ë„ì „ì´ë¼ í¥ë¯¸ë¡œì›Œìš”.\",\n",
    "    # \"í˜¼ì ì§‘ì— ìˆëŠ”ë° ê°‘ìê¸° ì´ìƒí•œ ì†Œë¦¬ê°€ ë“¤ë ¤ì„œ ë¬´ì„œì›Œìš”. ì‹¬ì¥ì´ ë¹¨ë¦¬ ë›°ì–´ìš”.\",\n",
    "    # \"ë¶€ëª¨ë‹˜ê»˜ì„œ ì œ ê¿ˆì„ ì‘ì›í•´ì£¼ì…”ì„œ ë„ˆë¬´ ë“ ë“ í•˜ê³  ì‚¬ë‘ë°›ëŠ” ê¸°ë¶„ì´ì—ìš”.\",\n",
    "    \"ì†ìˆ˜ ë§Œë“  ì‘ì€ í¬ì¥ì„ ëëƒˆë‹¤. ê³„íšëŒ€ë¡œ ë§ˆë¬´ë¦¬ë¼ ë§ˆìŒì†ìœ¼ë¡œ ì€ê·¼íˆ ê¸°ë»¤ë‹¤.\",\n",
    "    \"ì‘ì€ ëª¨í˜• ë°°ë¥¼ ì¡°ë¦½í•´ ì™„ì„±í–ˆë‹¤. ê³„íšëŒ€ë¡œ ë§ì•„ ë§ˆìŒì´ ì€ì€íˆ ê¸°ë»¤ë‹¤.\",\n",
    "    \"ì¸íŒŒê°€ ë¶ì ì´ëŠ” ì§€í•˜ì² ì—­ ì…êµ¬ì—ì„œ í° ì†Œë¦¬ë¡œ ì¹¨ì„ ë±‰ìœ¼ë©° ì§€ë‚˜ê°€ëŠ” ì‚¬ëŒë“¤ì´ ì§œì¦ì´ ë‚¬ë‹¤.\",\n",
    "    \"ì •ë§ ë¯¿ìŒì´ ê¹¨ì ¸ë²„ë ¸ì–´. ì•„ë¬´ë¦¬ ì•½ì†í•´ë„ ë’¤í†µìˆ˜ë¥¼ ë§ëŠ” ê¸°ë¶„, ì´ì   ëˆ„êµ´ ë¯¿ì–´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´.\",\n",
    "    \"ì €ë… ëª¨ì„ì—ì„œ ë‚´ ê¸°ì—¬ë¥¼ ë¬´ì‹œí•œ ì±„ ë‹¤ë¥¸ ì‚¬ëŒì´ ì¹­ì°¬ë°›ëŠ” ê±¸ ë³´ë©° ë§ˆìŒì†ì— ì°¨ì˜¤ë¥´ëŠ” ì‹¸ëŠ˜í•œ ë¶ˆì¾Œê°ì´ ë©ˆì¶”ì§€ ì•Šì•˜ë‹¤.\",\n",
    "    \"ì‹ë‹¹ì—ì„œ ê·¸ê°€ ì£¼ë¬¸ì„ ë°›ê³ ë„ ë¬´í‘œì •í•˜ê²Œ ëŒì•„ì„œì, ë‚˜ëŠ” ê¹Šì€ ì“¸ì“¸í•¨ê³¼ í•¨ê»˜ ì°¨ê°€ìš´ ë²½ì— ë§‰íŒ ê¸°ë¶„ì´ ë“¤ì—ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "print(f\"ğŸŒŸ AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ì„ ì‚¬ìš©í•œ ê°ì • í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ë°ëª¨:\")\n",
    "print(f\"ì´ {len(demo_texts)}ê°œì˜ ì˜ˆì‹œ í…ìŠ¤íŠ¸ë¡œ ê°ì • ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\")\n",
    "print(f\"ğŸ† ì‚¬ìš© ëª¨ë¸: {best_model} (F1-macro: {best_f1_score:.4f})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ê° ë°ëª¨ í…ìŠ¤íŠ¸ì— ëŒ€í•´ AutoGluon ë³µí•© ë¼ë²¨ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "for i, demo_text in enumerate(demo_texts, 1):\n",
    "    print(f\"[ì˜ˆì‹œ {i}]\")\n",
    "    result = predict_emotion_autogluon(demo_text)\n",
    "    print_autogluon_prediction(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ivgsrxka90q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ ê°ì • ì˜ˆì¸¡\n",
      "============================================================\n",
      "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜ì •í•´ì„œ ì›í•˜ëŠ” í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ì˜ˆì¸¡í•´ë³´ì„¸ìš”!\n",
      "\n",
      "ğŸ¨ ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ ì˜ˆì¸¡:\n",
      "--------------------------------------------------\n",
      "ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜ ì—¬í–‰ì„ ê°€ì„œ ì •ë§ ì¦ê±°ì› ì–´ìš”. ì˜¤ëœë§Œì— ìŠ¤íŠ¸ë ˆìŠ¤ê°€ í’€ë ¸ì–´ìš”.\n",
      "ğŸ¯ AutoGluon ì˜ˆì¸¡ ê²°ê³¼:\n",
      "   ë³µí•© ë¼ë²¨: ê¸°ì¨_ì¦ê±°ì›€ (ì‹ ë¢°ë„: 0.331)\n",
      "   Category1 (ëŒ€ë¶„ë¥˜): ê¸°ì¨\n",
      "   Category2 (ì„¸ë¶„ë¥˜): ì¦ê±°ì›€\n",
      "ğŸ“Š ìƒìœ„ 5ê°œ ë³µí•© ë¼ë²¨ í›„ë³´:\n",
      "   1. ê¸°ì¨_ì¦ê±°ì›€ (ê¸°ì¨|ì¦ê±°ì›€): 0.331\n",
      "   2. ê¸°ì¨_ë§Œì¡±ê° (ê¸°ì¨|ë§Œì¡±ê°): 0.131\n",
      "   3. ê¸°ì¨_í¸ì•ˆí•¨ (ê¸°ì¨|í¸ì•ˆí•¨): 0.113\n",
      "   4. ê¸°ì¨_ì‹ ëª…ë‚¨ (ê¸°ì¨|ì‹ ëª…ë‚¨): 0.038\n",
      "   5. ê¸°ì¨_ê³µê° (ê¸°ì¨|ê³µê°): 0.032\n",
      "\n",
      "ğŸ’¡ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:\n",
      "1. ìœ„ì˜ 'custom_text' ë³€ìˆ˜ì— ì›í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”\n",
      "2. ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”\n",
      "\n",
      "ğŸ“Œ ì˜ˆì¸¡ í•¨ìˆ˜ ì‚¬ìš©ë²•:\n",
      "result = predict_emotion_autogluon('ì—¬ê¸°ì— í…ìŠ¤íŠ¸ ì…ë ¥')\n",
      "print_autogluon_prediction(result)\n",
      "\n",
      "ğŸ“‹ AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ ì •ë³´ ìš”ì•½:\n",
      "============================================================\n",
      "ğŸ“Š í›ˆë ¨ ë°ì´í„°: 3359ê°œ ìƒ˜í”Œ\n",
      "ğŸ·ï¸ ë³µí•© ë¼ë²¨ í´ë˜ìŠ¤: 76ê°œ\n",
      "   - ì˜ˆì‹œ: ê¸°ì¨_ê°ë™, ê¸°ì¨_ê³ ë§ˆì›€, ê¸°ì¨_ê³µê°, ê¸°ì¨_ê¸°ëŒ€ê°, ê¸°ì¨_ë†€ëŒ, ê¸°ì¨_ë§Œì¡±ê°, ê¸°ì¨_ë°˜ê°€ì›€, ê¸°ì¨_ì‹ ë¢°ê°, ê¸°ì¨_ì‹ ëª…ë‚¨, ê¸°ì¨_ì•ˆì •ê°...\n",
      "ğŸ¤– ì„ë² ë”© ëª¨ë¸: 1024ì°¨ì›\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: WeightedEnsemble_L2\n",
      "\n",
      "ğŸ¯ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:\n",
      "   - Category1 ì •í™•ë„: 53.31%\n",
      "   - Category2 ì •í™•ë„: 34.34%\n",
      "   - ë³µí•© ë¼ë²¨ ì •í™•ë„: 32.08%\n",
      "   - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: 32.08%\n",
      "\n",
      "ğŸ“ˆ F1-Score (macro):\n",
      "   - Category1: 0.3936\n",
      "   - Category2: 0.2410\n",
      "   - ë³µí•© ë¼ë²¨: 0.2134\n",
      "   - ê²€ì¦ F1-macro: 0.6213\n",
      "\n",
      "ğŸ¯ AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ì˜ ì¥ì :\n",
      "âœ… F1-macro ì ìˆ˜ ìµœì í™”ë¡œ ë¶ˆê· í˜• í´ë˜ìŠ¤ ì„±ëŠ¥ í–¥ìƒ\n",
      "âœ… ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ì•™ìƒë¸”\n",
      "âœ… ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ì¡°í•©ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±\n",
      "âœ… ì¹´í…Œê³ ë¦¬ ê°„ ì˜ì¡´ì„±ì„ ì§ì ‘ í•™ìŠµ\n",
      "âœ… 5-fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ ë³´ì¥\n",
      "âœ… ìŠ¤íƒœí‚¹ ì•™ìƒë¸”ë¡œ ì˜ˆì¸¡ ì•ˆì •ì„± í–¥ìƒ\n",
      "\n",
      "ğŸ’¾ ëª¨ë¸ ì¬ì‚¬ìš©:\n",
      "ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: autogluon_combined_f1_model\n",
      "loaded_predictor = TabularPredictor.load('autogluon_combined_f1_model')\n",
      "prediction = loaded_predictor.predict(new_data)\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ (AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸)\n",
    "print(\"ğŸ” AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ ê°ì • ì˜ˆì¸¡\")\n",
    "print(\"=\"*60)\n",
    "print(\"ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜ì •í•´ì„œ ì›í•˜ëŠ” í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ì˜ˆì¸¡í•´ë³´ì„¸ìš”!\")\n",
    "print()\n",
    "\n",
    "# ì—¬ê¸°ì— ì›í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”\n",
    "custom_text = \"ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜ ì—¬í–‰ì„ ê°€ì„œ ì •ë§ ì¦ê±°ì› ì–´ìš”. ì˜¤ëœë§Œì— ìŠ¤íŠ¸ë ˆìŠ¤ê°€ í’€ë ¸ì–´ìš”.\"\n",
    "\n",
    "print(f\"ğŸ¨ ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ ì˜ˆì¸¡:\")\n",
    "print(\"-\"*50)\n",
    "custom_result = predict_emotion_autogluon(custom_text)\n",
    "print_autogluon_prediction(custom_result)\n",
    "\n",
    "print(\"ğŸ’¡ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:\")\n",
    "print(\"1. ìœ„ì˜ 'custom_text' ë³€ìˆ˜ì— ì›í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”\")\n",
    "print(\"2. ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”\")\n",
    "print()\n",
    "print(\"ğŸ“Œ ì˜ˆì¸¡ í•¨ìˆ˜ ì‚¬ìš©ë²•:\")\n",
    "print(\"result = predict_emotion_autogluon('ì—¬ê¸°ì— í…ìŠ¤íŠ¸ ì…ë ¥')\")\n",
    "print(\"print_autogluon_prediction(result)\")\n",
    "print()\n",
    "\n",
    "# AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ ì •ë³´ ìš”ì•½\n",
    "print(\"ğŸ“‹ AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ ì •ë³´ ìš”ì•½:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(data)}ê°œ ìƒ˜í”Œ\")\n",
    "print(f\"ğŸ·ï¸ ë³µí•© ë¼ë²¨ í´ë˜ìŠ¤: {len(combined_predictor.class_labels)}ê°œ\")\n",
    "\n",
    "# ë³µí•© ë¼ë²¨ í´ë˜ìŠ¤ ì˜ˆì‹œ í‘œì‹œ\n",
    "sample_labels = list(combined_predictor.class_labels)[:10]\n",
    "print(f\"   - ì˜ˆì‹œ: {', '.join(sample_labels)}{'...' if len(combined_predictor.class_labels) > 10 else ''}\")\n",
    "\n",
    "print(f\"ğŸ¤– ì„ë² ë”© ëª¨ë¸: {embeddings_model.get_sentence_embedding_dimension()}ì°¨ì›\")\n",
    "print(f\"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:\")\n",
    "print(f\"   - Category1 ì •í™•ë„: {cat1_accuracy*100:.2f}%\")\n",
    "print(f\"   - Category2 ì •í™•ë„: {cat2_accuracy*100:.2f}%\") \n",
    "print(f\"   - ë³µí•© ë¼ë²¨ ì •í™•ë„: {combined_accuracy*100:.2f}%\")\n",
    "print(f\"   - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: {both_accuracy*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ F1-Score (macro):\")\n",
    "print(f\"   - Category1: {f1_cat1:.4f}\")\n",
    "print(f\"   - Category2: {f1_cat2:.4f}\")\n",
    "print(f\"   - ë³µí•© ë¼ë²¨: {f1_combined:.4f}\")\n",
    "print(f\"   - ê²€ì¦ F1-macro: {best_f1_score:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ì˜ ì¥ì :\")\n",
    "print(\"âœ… F1-macro ì ìˆ˜ ìµœì í™”ë¡œ ë¶ˆê· í˜• í´ë˜ìŠ¤ ì„±ëŠ¥ í–¥ìƒ\")\n",
    "print(\"âœ… ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ì•™ìƒë¸”\")\n",
    "print(\"âœ… ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ì¡°í•©ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±\")\n",
    "print(\"âœ… ì¹´í…Œê³ ë¦¬ ê°„ ì˜ì¡´ì„±ì„ ì§ì ‘ í•™ìŠµ\")\n",
    "print(\"âœ… 5-fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ ë³´ì¥\")\n",
    "print(\"âœ… ìŠ¤íƒœí‚¹ ì•™ìƒë¸”ë¡œ ì˜ˆì¸¡ ì•ˆì •ì„± í–¥ìƒ\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ ëª¨ë¸ ì¬ì‚¬ìš©:\")\n",
    "print(f\"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {combined_save_path}\")\n",
    "print(\"loaded_predictor = TabularPredictor.load('autogluon_combined_f1_model')\")\n",
    "print(\"prediction = loaded_predictor.predict(new_data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e6c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
