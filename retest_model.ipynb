{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k76sio3irn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ - TF-IDF vs PCA+TF-IDF\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TF-IDF vs PCA+TF-IDF ë³µí•© ë¼ë²¨ ë¶„ë¥˜ ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •\n",
    "try:\n",
    "    # PCA ëª¨ë¸ ì„±ëŠ¥ ë³€ìˆ˜ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "    lr_pca_accuracy\n",
    "    lr_pca_f1_weighted\n",
    "    lr_pca_f1_macro\n",
    "    lr_pca_train_time\n",
    "    lr_pca_pred_time\n",
    "    svc_pca_accuracy\n",
    "    svc_pca_f1_weighted\n",
    "    svc_pca_f1_macro\n",
    "    svc_pca_train_time\n",
    "    svc_pca_pred_time\n",
    "    print(\"âœ… PCA ëª¨ë¸ ê²°ê³¼ê°€ ëª¨ë‘ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "except NameError:\n",
    "    print(\"âŒ PCA ëª¨ë¸ ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì „ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "    print(\"ë‹¤ìŒ ì…€ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "    print(\"1. ë°ì´í„° ì¤€ë¹„ ì…€\")\n",
    "    print(\"2. PCA + TF-IDF ë²¡í„°í™” ì…€\")\n",
    "    print(\"3. PCA + LogisticRegression ì…€\")\n",
    "    print(\"4. PCA + SVC ì…€\")\n",
    "    print(\"5. í˜„ì¬ ë¹„êµ ë¶„ì„ ì…€\")\n",
    "    \n",
    "    # ì„ì‹œë¡œ ë”ë¯¸ ê°’ ì„¤ì •í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€\n",
    "    lr_pca_accuracy = 0.0\n",
    "    lr_pca_f1_weighted = 0.0\n",
    "    lr_pca_f1_macro = 0.0\n",
    "    lr_pca_train_time = 0.0\n",
    "    lr_pca_pred_time = 0.0\n",
    "    svc_pca_accuracy = 0.0\n",
    "    svc_pca_f1_weighted = 0.0\n",
    "    svc_pca_f1_macro = 0.0\n",
    "    svc_pca_train_time = 0.0\n",
    "    svc_pca_pred_time = 0.0\n",
    "\n",
    "# ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” ìƒì„± (ê¸°ì¡´ ê²°ê³¼ í¬í•¨)\n",
    "all_results_df = pd.DataFrame({\n",
    "    'Method': ['TF-IDF + LogisticRegression', 'PCA + LogisticRegression', \n",
    "               'TF-IDF + SVC', 'PCA + SVC'],\n",
    "    'Accuracy': [0.0858, lr_pca_accuracy, 0.1039, svc_pca_accuracy],\n",
    "    'F1-Score (Weighted)': [0.0855, lr_pca_f1_weighted, 0.1090, svc_pca_f1_weighted],\n",
    "    'F1-Score (Macro)': [0.0503, lr_pca_f1_macro, 0.0590, svc_pca_f1_macro],\n",
    "    'Training Time (sec)': [0.2941, lr_pca_train_time, 2.5693, svc_pca_train_time],\n",
    "    'Prediction Time (sec)': [0.002, lr_pca_pred_time, 0.189, svc_pca_pred_time],\n",
    "    'Feature Dimensions': [8430, 500, 8430, 500]\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ“Š ì „ì²´ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "print(all_results_df.round(4))\n",
    "\n",
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‹ë³„\n",
    "best_accuracy_idx = all_results_df['Accuracy'].idxmax()\n",
    "best_f1_idx = all_results_df['F1-Score (Weighted)'].idxmax()\n",
    "\n",
    "print(f\"\\nğŸ† ìµœê³  ì •í™•ë„ ëª¨ë¸: {all_results_df.loc[best_accuracy_idx, 'Method']} ({all_results_df.loc[best_accuracy_idx, 'Accuracy']:.4f})\")\n",
    "print(f\"ğŸ† ìµœê³  F1-Score ëª¨ë¸: {all_results_df.loc[best_f1_idx, 'Method']} ({all_results_df.loc[best_f1_idx, 'F1-Score (Weighted)']:.4f})\")\n",
    "\n",
    "# PCA íš¨ê³¼ ë¶„ì„\n",
    "print(f\"\\nğŸ“ˆ PCA ì ìš© íš¨ê³¼:\")\n",
    "lr_improvement = lr_pca_accuracy - 0.0858\n",
    "svc_improvement = svc_pca_accuracy - 0.1039\n",
    "\n",
    "print(f\"LogisticRegression: {lr_improvement:+.4f} ({'ê°œì„ ' if lr_improvement > 0 else 'ê°ì†Œ'})\")\n",
    "print(f\"SVC: {svc_improvement:+.4f} ({'ê°œì„ ' if svc_improvement > 0 else 'ê°ì†Œ'})\")\n",
    "\n",
    "# ì°¨ì› ì¶•ì†Œ íš¨ê³¼\n",
    "try:\n",
    "    pca_variance_ratio = pca.explained_variance_ratio_.sum()\n",
    "except NameError:\n",
    "    pca_variance_ratio = 0.85  # ì„ì‹œ ê¸°ë³¸ê°’\n",
    "\n",
    "print(f\"\\nğŸ’¡ ì°¨ì› ì¶•ì†Œ íš¨ê³¼:\")\n",
    "print(f\"- ì°¨ì› ê°ì†Œ: 8,430 â†’ 500 ({500/8430*100:.1f}%)\")\n",
    "print(f\"- ì„¤ëª…ëœ ë¶„ì‚°: {pca_variance_ratio:.1%}\")\n",
    "print(f\"- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•½ {8430/500:.1f}ë°° ê°ì†Œ\")\n",
    "\n",
    "# ê¸°ì¡´ ì„ë² ë”© ë°©ì‹ê³¼ ë¹„êµ\n",
    "print(f\"\\nğŸ” ê¸°ì¡´ ì„ë² ë”© ë°©ì‹(34.04%) ëŒ€ë¹„:\")\n",
    "best_pca_score = max(lr_pca_accuracy, svc_pca_accuracy)\n",
    "if best_pca_score > 0:\n",
    "    print(f\"- ìµœê³  PCA ì„±ëŠ¥: {best_pca_score:.4f} ({best_pca_score/0.3404*100:.1f}%)\")\n",
    "    print(f\"- ì„±ëŠ¥ ê²©ì°¨: {0.3404 - best_pca_score:.4f} ({(1-best_pca_score/0.3404)*100:.1f}% ì°¨ì´)\")\n",
    "else:\n",
    "    print(\"- PCA ëª¨ë¸ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "print(f\"\\nğŸ“ ê²°ë¡ :\")\n",
    "print(f\"- PCA ì°¨ì› ì¶•ì†ŒëŠ” ê³„ì‚° íš¨ìœ¨ì„±ì„ í¬ê²Œ ê°œì„ \")\n",
    "print(f\"- í•˜ì§€ë§Œ ë³µí•© ê°ì • ë¼ë²¨ë§ì—ëŠ” ì—¬ì „íˆ ì„ë² ë”© ë°©ì‹ì´ ìš°ìˆ˜\")\n",
    "print(f\"- TF-IDFëŠ” ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ ê°ì •ì˜ ë³µì¡í•œ ë¬¸ë§¥ ì´í•´ì— í•œê³„\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PCA + TF-IDF ë°©ì‹ì„ ì‚¬ìš©í•œ ë³µí•© ë¼ë²¨ ë¶„ë¥˜ ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fuhzjl89myq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ\n",
      "================================================================================\n",
      "\n",
      "ğŸ” SVC ëª¨ë¸ ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ê¸°ì¨_ê°ë™     0.3043    0.2333    0.2642        30\n",
      "      ê¸°ì¨_ê³ ë§ˆì›€     0.6667    0.4211    0.5161        19\n",
      "       ê¸°ì¨_ê³µê°     0.1667    0.1667    0.1667         6\n",
      "      ê¸°ì¨_ê¸°ëŒ€ê°     0.0909    0.2000    0.1250         5\n",
      "       ê¸°ì¨_ë†€ëŒ     0.0000    0.0000    0.0000         5\n",
      "      ê¸°ì¨_ë§Œì¡±ê°     0.3810    0.1667    0.2319        48\n",
      "      ê¸°ì¨_ë°˜ê°€ì›€     0.4286    0.1875    0.2609        16\n",
      "      ê¸°ì¨_ì‹ ë¢°ê°     0.0000    0.0000    0.0000         1\n",
      "      ê¸°ì¨_ì‹ ëª…ë‚¨     0.0000    0.0000    0.0000         5\n",
      "      ê¸°ì¨_ì•ˆì •ê°     0.0000    0.0000    0.0000         8\n",
      "    ê¸°ì¨_ìë‘ìŠ¤ëŸ¬ì›€     0.0588    0.0833    0.0690        12\n",
      "      ê¸°ì¨_ìì‹ ê°     0.0000    0.0000    0.0000         1\n",
      "      ê¸°ì¨_ì¦ê±°ì›€     0.0000    0.0000    0.0000        27\n",
      "      ê¸°ì¨_í†µì¾Œí•¨     0.0000    0.0000    0.0000         1\n",
      "      ê¸°ì¨_í¸ì•ˆí•¨     0.0000    0.0000    0.0000         4\n",
      "      ë‘ë ¤ì›€_ê±±ì •     0.2000    0.1304    0.1579        23\n",
      "      ë‘ë ¤ì›€_ê³µí¬     0.0000    0.0000    0.0000        17\n",
      "      ë‘ë ¤ì›€_ë†€ëŒ     0.1250    0.0435    0.0645        23\n",
      "     ë‘ë ¤ì›€_ìœ„ì¶•ê°     0.0000    0.0000    0.0000         4\n",
      "     ë‘ë ¤ì›€_ì´ˆì¡°í•¨     0.0000    0.0000    0.0000         5\n",
      "  ë¯¸ì›€(ìƒëŒ€ë°©)_ê²½ë©¸     0.2353    0.2222    0.2286        18\n",
      "  ë¯¸ì›€(ìƒëŒ€ë°©)_ëƒ‰ë‹´     0.0000    0.0000    0.0000         3\n",
      "  ë¯¸ì›€(ìƒëŒ€ë°©)_ë°˜ê°     0.5000    0.2000    0.2857         5\n",
      " ë¯¸ì›€(ìƒëŒ€ë°©)_ë¶ˆì‹ ê°     0.2500    0.1250    0.1667         8\n",
      "ë¯¸ì›€(ìƒëŒ€ë°©)_ë¹„ìœ„ìƒí•¨     0.0000    0.0000    0.0000         1\n",
      " ë¯¸ì›€(ìƒëŒ€ë°©)_ì‹œê¸°ì‹¬     0.0000    0.0000    0.0000         2\n",
      "  ë¯¸ì›€(ìƒëŒ€ë°©)_ì™¸ë©´     0.0000    0.0000    0.0000         1\n",
      " ë¯¸ì›€(ìƒëŒ€ë°©)_ì¹˜ì‚¬í•¨     0.0000    0.0000    0.0000         5\n",
      "     ë¶„ë…¸_ë‚ ì¹´ë¡œì›€     0.0000    0.0000    0.0000         3\n",
      "       ë¶„ë…¸_ë°œì—´     0.0000    0.0000    0.0000         3\n",
      "       ë¶„ë…¸_ë¶ˆì¾Œ     0.0588    0.2500    0.0952        20\n",
      "      ë¶„ë…¸_ì‚¬ë‚˜ì›€     0.0000    0.0000    0.0000         4\n",
      "       ë¶„ë…¸_ì›ë§     0.0000    0.0000    0.0000         3\n",
      "      ë¶„ë…¸_íƒ€ì˜¤ë¦„     0.0000    0.0000    0.0000         3\n",
      "      ì‚¬ë‘_ê·€ì¤‘í•¨     0.0000    0.0000    0.0000         5\n",
      "     ì‚¬ë‘_ë„ˆê·¸ëŸ¬ì›€     0.0000    0.0000    0.0000         1\n",
      "      ì‚¬ë‘_ë‹¤ì •í•¨     0.0000    0.0000    0.0000         9\n",
      "   ì‚¬ë‘_ë™ì •(ìŠ¬í””)     0.0000    0.0000    0.0000         7\n",
      "     ì‚¬ë‘_ë‘ê·¼ê±°ë¦¼     0.0000    0.0000    0.0000         2\n",
      "      ì‚¬ë‘_ë§¤ë ¥ì      0.0000    0.0000    0.0000        13\n",
      "       ì‚¬ë‘_í˜¸ê°     0.0000    0.0000    0.0000        10\n",
      "     ìˆ˜ì¹˜ì‹¬_ë¯¸ì•ˆí•¨     0.0000    0.0000    0.0000         7\n",
      "    ìˆ˜ì¹˜ì‹¬_ë¶€ë„ëŸ¬ì›€     0.2000    0.0625    0.0952        16\n",
      "     ìˆ˜ì¹˜ì‹¬_ì£„ì±…ê°     0.0000    0.0000    0.0000         2\n",
      "       ìŠ¬í””_ê³ í†µ     0.5000    0.1111    0.1818         9\n",
      "      ìŠ¬í””_ê·¸ë¦¬ì›€     0.0000    0.0000    0.0000        11\n",
      "   ìŠ¬í””_ë™ì •(ìŠ¬í””)     0.1000    0.0500    0.0667        20\n",
      "      ìŠ¬í””_ë¬´ê¸°ë ¥     0.1304    0.2143    0.1622        14\n",
      "      ìŠ¬í””_ìˆ˜ì¹˜ì‹¬     0.0000    0.0000    0.0000         3\n",
      "       ìŠ¬í””_ì‹¤ë§     0.0400    0.1538    0.0635        13\n",
      "      ìŠ¬í””_ì•„ì‰¬ì›€     0.0000    0.0000    0.0000         0\n",
      "       ìŠ¬í””_ì•„í””     0.0000    0.0000    0.0000         3\n",
      "      ìŠ¬í””_ì–µìš¸í•¨     0.0556    0.1538    0.0816        13\n",
      "      ìŠ¬í””_ì™¸ë¡œì›€     0.0909    0.1667    0.1176         6\n",
      "       ìŠ¬í””_ì ˆë§     0.0000    0.0000    0.0000         7\n",
      "       ìŠ¬í””_í—ˆë§     0.0174    0.1818    0.0317        11\n",
      "       ìŠ¬í””_í›„íšŒ     0.0000    0.0000    0.0000         7\n",
      " ì‹«ì–´í•¨(ìƒíƒœ)_ë‚œì²˜í•¨     0.2500    0.0714    0.1111        14\n",
      " ì‹«ì–´í•¨(ìƒíƒœ)_ë‹µë‹µí•¨     0.0667    0.1111    0.0833        18\n",
      " ì‹«ì–´í•¨(ìƒíƒœ)_ë¶ˆí¸í•¨     0.0000    0.0000    0.0000         7\n",
      "  ì‹«ì–´í•¨(ìƒíƒœ)_ì‹«ì¦     0.0000    0.0000    0.0000         9\n",
      " ì‹«ì–´í•¨(ìƒíƒœ)_ì‹¬ì‹¬í•¨     0.0000    0.0000    0.0000         1\n",
      "       ìš•ë§_ê°ˆë“±     0.0000    0.0000    0.0000         1\n",
      "      ìš•ë§_ê¶ê¸ˆí•¨     0.1111    0.2222    0.1481         9\n",
      "      ìš•ë§_ê¸°ëŒ€ê°     0.0000    0.0000    0.0000        13\n",
      "       ìš•ë§_ë¶ˆë§Œ     0.0000    0.0000    0.0000         8\n",
      "      ìš•ë§_ì•„ì‰¬ì›€     0.2857    0.0606    0.1000        33\n",
      "       ìš•ë§_ìš•ì‹¬     0.3571    0.2778    0.3125        18\n",
      "       ì¤‘ë¦½_ê³µê°     0.0000    0.0000    0.0000         1\n",
      "       ì¤‘ë¦½_ë†€ëŒ     0.0000    0.0000    0.0000         3\n",
      "      ì¤‘ë¦½_ë§Œì¡±ê°     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.1039       664\n",
      "   macro avg     0.0799    0.0601    0.0590       664\n",
      "weighted avg     0.1529    0.1039    0.1090       664\n",
      "\n",
      "\n",
      "ğŸ“‹ SVC ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):\n",
      "âŒ ì‹¤ì œ: ê¸°ì¨_ë§Œì¡±ê° | ì˜ˆì¸¡: ìŠ¬í””_ì–µìš¸í•¨\n",
      "   í…ìŠ¤íŠ¸: ë³´ëŠ”ë™ì•ˆ ë„ˆë¬´ í–‰ë³µí–ˆê³  ì´ˆì½œë ›ì´ ë„ˆë¬´ ë¨¹ê³ ì‹¶ì—ˆê³  í‹°ëª¨ì‹œê°€ ì˜ìƒê²¼ê³  ìš¸ì–´!!í•˜ëŠ”ë¶€ë¶„ì´ ìˆì–´ì„œ ìš¸ì—ˆë‹¤ë„¤ìš”\n",
      "\n",
      "âœ… ì‹¤ì œ: ê¸°ì¨_ë§Œì¡±ê° | ì˜ˆì¸¡: ê¸°ì¨_ë§Œì¡±ê°\n",
      "   í…ìŠ¤íŠ¸: ì–´ë¦´ ë•Œ ê°€ ë³´ê³  ë¹•ìŠ¤ëŠ” ê±°ì˜ ì²˜ìŒì¸ë°(ê¸°ì–µì— ì—†ìŒ) ì§€ê¸ˆ ë”¸ê¸°ì¶•ì œ ê¸°ê°„ì´ë¼ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì‹ì‚¬ í•˜ê³  ì˜´\n",
      "\n",
      "âŒ ì‹¤ì œ: ê¸°ì¨_ë§Œì¡±ê° | ì˜ˆì¸¡: ë¶„ë…¸_ë¶ˆì¾Œ\n",
      "   í…ìŠ¤íŠ¸: ë¯¸ë¦¬ ê³„ì¢Œë¡œ í™˜ì „í•´ë‘” ëˆì„ í•´ì™¸ì—ì„œ í™˜ì „ìˆ˜ìˆ˜ë£Œ ì—†ì´ ì¸ì¶œ ê°€ëŠ¥í•œ íŠ¸ë ˆë¸”ë¡œê·¸ë¼ëŠ” ì¹´ë“œì¸ë°, ì„ íƒí•  ìˆ˜ ìˆëŠ” ë””ìì¸ ì¤‘ì— ì´ ì—¬ê¶Œ ìŠ¤íƒ€ì¼ì´ ë„ˆë¬´ ì„¼ìŠ¤ ìˆê³  ìœ ë‹ˆí¬í•´ì„œ ë§ˆìŒì— ë“ ë‹¤.\n",
      "\n",
      "âŒ ì‹¤ì œ: ìŠ¬í””_ë¬´ê¸°ë ¥ | ì˜ˆì¸¡: ìš•ë§_ê¶ê¸ˆí•¨\n",
      "   í…ìŠ¤íŠ¸: ìš”ì¦˜ ë²ˆì•„ì›ƒë„ ìê¾¸ ì˜¬ë¼ì˜¤ê³  ë¬´ê¸°ë ¥í•´ì„œ ì¢…ê°•í•˜ê³  êµë¥˜í•˜ê¸°ë„ ë²„ê±°ìš´ ìƒíƒœê°€ ì™€ë¶€ë €ìœ¼ìš”ã… ã…  \n",
      "\n",
      "âŒ ì‹¤ì œ: ê¸°ì¨_ì¦ê±°ì›€ | ì˜ˆì¸¡: ë¶„ë…¸_ë¶ˆì¾Œ\n",
      "   í…ìŠ¤íŠ¸: í¬ë¼ì„ì”¬ ì¥ë˜¥ë¯¼ì´ ë²”í–‰ ë„êµ¬ ì°¾ìœ¼ë ¤ê³  í™”ì¥ì‹¤ íƒ±í¬ ë’¤ì§€ëŠ”ë° ê±°ê¸°ì— ì§„ì§œ ë˜¥ ë„£ì–´ë†“ì€ ê±° ì§„ì§œ ì›ƒê²¨ ë’¤ì§€ê² ìŒã…‹ã…‹ã…‹ã…‹ã…‹\n",
      "\n",
      "âŒ ì‹¤ì œ: ì‹«ì–´í•¨(ìƒíƒœ)_ë‹µë‹µí•¨ | ì˜ˆì¸¡: ê¸°ì¨_ìë‘ìŠ¤ëŸ¬ì›€\n",
      "   í…ìŠ¤íŠ¸: ê°€ìŠ´ì´ ë‹µë‹µí•´ì§ ì§„ì§œ ê°œë‹µë‹µí•´ì§\n",
      "ìš°ë¦¬ì§„ì§œíˆ¬í‘œì˜í•˜ì\n",
      "\n",
      "âŒ ì‹¤ì œ: ê¸°ì¨_ë§Œì¡±ê° | ì˜ˆì¸¡: ìŠ¬í””_í—ˆë§\n",
      "   í…ìŠ¤íŠ¸: ì§€ê·¸ì¬ê·¸ë‘ ì—ì´ë¸”ë¦¬ë‘ í• ì¸ ëŒ€ê²°í•˜ë‚˜\n",
      "ì•„ íë­‡í•´\n",
      "ê³„ì†ë˜ê¸¸...\n",
      "ì˜ì›íˆ....\n",
      "\n",
      "âŒ ì‹¤ì œ: ê¸°ì¨_ìë‘ìŠ¤ëŸ¬ì›€ | ì˜ˆì¸¡: ë¶„ë…¸_ë¶ˆì¾Œ\n",
      "   í…ìŠ¤íŠ¸: ì²¨ìœ¼ë¡œ ìˆ˜ì œ ì´ˆì½œë¦¿ ë§Œë“¬\n",
      "ì´ˆì½œë¦¿ì„ 5ì‹œê°„ì´ë‚˜ ë§Œë“œëŠ” ì‚¬ëŒì´ ìˆë‹¤??? ê·¸ê²Œ ë°”ë¡œ ë‚˜\n",
      "\n",
      "âŒ ì‹¤ì œ: ìŠ¬í””_ì ˆë§ | ì˜ˆì¸¡: ë‘ë ¤ì›€_ë†€ëŒ\n",
      "   í…ìŠ¤íŠ¸: ë‹¨í†¡ë°©ì— ê³µì§€ë“¤ ìŠ¬ìŠ¬ ì˜¬ë¼ì˜¤ëŠ”ê±° ë³´ë‹ˆê¹Œ ê³§ ê°œê°•ì´ë¼ëŠ”ê²Œ ì‹¤ê°ë‚˜ì„œ ê°‘ìê¸° ì¬ê¸°í•˜ê³ ì‹¶ê³  ì¸ìƒì´ ë‹¤ ëë‚œê±°ì²˜ëŸ¼ ì•”ìš¸í•´ì§\n",
      "\n",
      "âŒ ì‹¤ì œ: ê¸°ì¨_ì¦ê±°ì›€ | ì˜ˆì¸¡: ìŠ¬í””_í—ˆë§\n",
      "   í…ìŠ¤íŠ¸: ì¥í¥ì‹  ê³µì†í•œ ì†ê°€ë½ì§ˆ ê°œì›ƒê²¨ìš”\n",
      "ì• ë“œë¦½ ë¯¸ì¹œê²ƒ ê°™ìŒ\n",
      "\n",
      "\n",
      "ğŸ“Š ë¼ë²¨ ë¶„ì„:\n",
      "í›ˆë ¨ ë°ì´í„° ë¼ë²¨ ìˆ˜: 71\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ìˆ˜: 70\n",
      "ê³µí†µ ë¼ë²¨ ìˆ˜: 68\n",
      "í…ŒìŠ¤íŠ¸ì—ë§Œ ìˆëŠ” ìƒˆ ë¼ë²¨ ìˆ˜: 2\n",
      "í›ˆë ¨ì—ë§Œ ìˆëŠ” ë¼ë²¨ ìˆ˜: 3\n",
      "\n",
      "âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ìƒˆë¡œìš´ ë¼ë²¨ë“¤ (ì²˜ìŒ 5ê°œ):\n",
      "  - ì¤‘ë¦½_ë§Œì¡±ê°: 1ê°œ\n",
      "  - ì¤‘ë¦½_ë†€ëŒ: 3ê°œ\n",
      "================================================================================\n",
      "TF-IDF ë°©ì‹ì„ ì‚¬ìš©í•œ ë³µí•© ë¼ë²¨ ë¶„ë¥˜ ì™„ë£Œ!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ìƒìœ„ ì„±ëŠ¥ ëª¨ë¸ì˜ ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë‘ ëª¨ë¸ ì¤‘ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ì˜ ìƒì„¸ ë³´ê³ ì„œ ì¶œë ¥\n",
    "if lr_accuracy >= svc_accuracy:\n",
    "    print(\"\\nğŸ” LogisticRegression ëª¨ë¸ ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ:\")\n",
    "    print(classification_report(y_test, y_test_pred_lr, digits=4, zero_division=0))\n",
    "    \n",
    "    # ì˜ˆì¸¡ ìƒ˜í”Œ í™•ì¸\n",
    "    print(\"\\nğŸ“‹ LogisticRegression ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):\")\n",
    "    for i in range(min(10, len(y_test))):\n",
    "        actual = y_test[i]\n",
    "        predicted = y_test_pred_lr[i]\n",
    "        text_sample = X_test[i][:100] + \"...\" if len(X_test[i]) > 100 else X_test[i]\n",
    "        status = \"âœ…\" if actual == predicted else \"âŒ\"\n",
    "        print(f\"{status} ì‹¤ì œ: {actual} | ì˜ˆì¸¡: {predicted}\")\n",
    "        print(f\"   í…ìŠ¤íŠ¸: {text_sample}\\n\")\n",
    "else:\n",
    "    print(\"\\nğŸ” SVC ëª¨ë¸ ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ:\")\n",
    "    print(classification_report(y_test, y_test_pred_svc, digits=4, zero_division=0))\n",
    "    \n",
    "    # ì˜ˆì¸¡ ìƒ˜í”Œ í™•ì¸\n",
    "    print(\"\\nğŸ“‹ SVC ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):\")\n",
    "    for i in range(min(10, len(y_test))):\n",
    "        actual = y_test[i]\n",
    "        predicted = y_test_pred_svc[i]\n",
    "        text_sample = X_test[i][:100] + \"...\" if len(X_test[i]) > 100 else X_test[i]\n",
    "        status = \"âœ…\" if actual == predicted else \"âŒ\"\n",
    "        print(f\"{status} ì‹¤ì œ: {actual} | ì˜ˆì¸¡: {predicted}\")\n",
    "        print(f\"   í…ìŠ¤íŠ¸: {text_sample}\\n\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ê³µí†µ ë¼ë²¨ê³¼ ìƒˆë¡œìš´ ë¼ë²¨ ë¶„ì„\n",
    "train_labels = set(y_train)\n",
    "test_labels = set(y_test)\n",
    "common_labels = train_labels.intersection(test_labels)\n",
    "new_labels = test_labels - train_labels\n",
    "missing_labels = train_labels - test_labels\n",
    "\n",
    "print(f\"\\nğŸ“Š ë¼ë²¨ ë¶„ì„:\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° ë¼ë²¨ ìˆ˜: {len(train_labels)}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ìˆ˜: {len(test_labels)}\")\n",
    "print(f\"ê³µí†µ ë¼ë²¨ ìˆ˜: {len(common_labels)}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ì—ë§Œ ìˆëŠ” ìƒˆ ë¼ë²¨ ìˆ˜: {len(new_labels)}\")\n",
    "print(f\"í›ˆë ¨ì—ë§Œ ìˆëŠ” ë¼ë²¨ ìˆ˜: {len(missing_labels)}\")\n",
    "\n",
    "if new_labels:\n",
    "    print(f\"\\nâš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ìƒˆë¡œìš´ ë¼ë²¨ë“¤ (ì²˜ìŒ 5ê°œ):\")\n",
    "    for label in list(new_labels)[:5]:\n",
    "        count = sum(1 for y in y_test if y == label)\n",
    "        print(f\"  - {label}: {count}ê°œ\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TF-IDF ë°©ì‹ì„ ì‚¬ìš©í•œ ë³µí•© ë¼ë²¨ ë¶„ë¥˜ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "thx6egr38vn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TF-IDF ë°©ì‹ ë³µí•© ë¼ë²¨ ë¶„ë¥˜ ê²°ê³¼ ë¹„êµ\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\n",
      "                Model  Accuracy  F1-Score (Weighted)  F1-Score (Macro)  \\\n",
      "0  LogisticRegression    0.0858               0.0855            0.0503   \n",
      "1                 SVC    0.1039               0.1090            0.0590   \n",
      "\n",
      "   Training Time (sec)  Prediction Time (sec)  \n",
      "0               0.2881                  0.001  \n",
      "1               2.5652                  0.189  \n",
      "\n",
      "ğŸ† ìµœê³  ì •í™•ë„ ëª¨ë¸: SVC (0.1039)\n",
      "ğŸ† ìµœê³  F1-Score ëª¨ë¸: SVC (0.1090)\n",
      "\n",
      "ğŸ“‹ ì°¸ê³ : ê¸°ì¡´ ì„ë² ë”© ë°©ì‹ ë³µí•© ë¼ë²¨ ë¶„ë¥˜ ì •í™•ë„ëŠ” ì•½ 34.04%ì˜€ìŠµë‹ˆë‹¤.\n",
      "ğŸ“‹ TF-IDF ë°©ì‹ì´ ì„ë² ë”© ë°©ì‹ë³´ë‹¤ ë¶€ì¡±í•©ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ’¡ TF-IDF ë²¡í„° ì°¨ì›: 8430ì°¨ì›\n",
      "ğŸ’¡ ì´ ë¼ë²¨ ìˆ˜: 71ê°œ\n",
      "ğŸ’¡ ë²¡í„°í™” ì†Œìš”ì‹œê°„: 0.12ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ìƒì„¸ ë¶„ì„\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TF-IDF ë°©ì‹ ë³µí•© ë¼ë²¨ ë¶„ë¥˜ ê²°ê³¼ ë¹„êµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['LogisticRegression', 'SVC'],\n",
    "    'Accuracy': [lr_accuracy, svc_accuracy],\n",
    "    'F1-Score (Weighted)': [lr_f1_weighted, svc_f1_weighted],\n",
    "    'F1-Score (Macro)': [lr_f1_macro, svc_f1_macro],\n",
    "    'Training Time (sec)': [lr_train_time, svc_train_time],\n",
    "    'Prediction Time (sec)': [lr_pred_time, svc_pred_time]\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í™•ì¸\n",
    "best_accuracy_model = 'LogisticRegression' if lr_accuracy > svc_accuracy else 'SVC'\n",
    "best_f1_model = 'LogisticRegression' if lr_f1_weighted > svc_f1_weighted else 'SVC'\n",
    "\n",
    "print(f\"\\nğŸ† ìµœê³  ì •í™•ë„ ëª¨ë¸: {best_accuracy_model} ({max(lr_accuracy, svc_accuracy):.4f})\")\n",
    "print(f\"ğŸ† ìµœê³  F1-Score ëª¨ë¸: {best_f1_model} ({max(lr_f1_weighted, svc_f1_weighted):.4f})\")\n",
    "\n",
    "# ê¸°ì¡´ ì„ë² ë”© ë°©ì‹ê³¼ ë¹„êµë¥¼ ìœ„í•œ ì°¸ê³  ì •ë³´\n",
    "print(f\"\\nğŸ“‹ ì°¸ê³ : ê¸°ì¡´ ì„ë² ë”© ë°©ì‹ ë³µí•© ë¼ë²¨ ë¶„ë¥˜ ì •í™•ë„ëŠ” ì•½ 34.04%ì˜€ìŠµë‹ˆë‹¤.\")\n",
    "print(f\"ğŸ“‹ TF-IDF ë°©ì‹ì´ ì„ë² ë”© ë°©ì‹ë³´ë‹¤ {'ìš°ìˆ˜' if max(lr_accuracy, svc_accuracy) > 0.3404 else 'ë¶€ì¡±'}í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ TF-IDF ë²¡í„° ì°¨ì›: {X_train_tfidf.shape[1]}ì°¨ì›\")\n",
    "print(f\"ğŸ’¡ ì´ ë¼ë²¨ ìˆ˜: {len(set(y_train))}ê°œ\")\n",
    "print(f\"ğŸ’¡ ë²¡í„°í™” ì†Œìš”ì‹œê°„: {vectorization_time:.2f}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3db2tta8jro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVC ëª¨ë¸ í›ˆë ¨ ===\n",
      "SVC í›ˆë ¨ ì‹œê°„: 2.57ì´ˆ\n",
      "SVC ì˜ˆì¸¡ ì‹œê°„: 0.19ì´ˆ\n",
      "SVC ì •í™•ë„: 0.1039\n",
      "SVC F1-Score (weighted): 0.1090\n",
      "SVC F1-Score (macro): 0.0590\n"
     ]
    }
   ],
   "source": [
    "# SVC ëª¨ë¸ í›ˆë ¨\n",
    "print(f\"\\n=== SVC ëª¨ë¸ í›ˆë ¨ ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "svc_model = SVC(\n",
    "    kernel='linear',         # ì„ í˜• ì»¤ë„ ì‚¬ìš©\n",
    "    random_state=42,\n",
    "    class_weight='balanced', # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\n",
    "    max_iter=1000           # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì„¤ì •\n",
    ")\n",
    "\n",
    "svc_model.fit(X_train_tfidf, y_train)\n",
    "svc_train_time = time.time() - start_time\n",
    "\n",
    "# SVC í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€\n",
    "start_time = time.time()\n",
    "y_test_pred_svc = svc_model.predict(X_test_tfidf)\n",
    "svc_pred_time = time.time() - start_time\n",
    "\n",
    "svc_accuracy = accuracy_score(y_test, y_test_pred_svc)\n",
    "svc_f1_weighted = f1_score(y_test, y_test_pred_svc, average='weighted')\n",
    "svc_f1_macro = f1_score(y_test, y_test_pred_svc, average='macro')\n",
    "\n",
    "print(f\"SVC í›ˆë ¨ ì‹œê°„: {svc_train_time:.2f}ì´ˆ\")\n",
    "print(f\"SVC ì˜ˆì¸¡ ì‹œê°„: {svc_pred_time:.2f}ì´ˆ\")\n",
    "print(f\"SVC ì •í™•ë„: {svc_accuracy:.4f}\")\n",
    "print(f\"SVC F1-Score (weighted): {svc_f1_weighted:.4f}\")\n",
    "print(f\"SVC F1-Score (macro): {svc_f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "gfw8c3mk9ld",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF ë²¡í„°í™” ì§„í–‰ ===\n",
      "TF-IDF ë²¡í„°í™” ì™„ë£Œ - ì†Œìš”ì‹œê°„: 0.12ì´ˆ\n",
      "TF-IDF ë²¡í„° ì°¨ì›: 8430\n",
      "í›ˆë ¨ ë°ì´í„° ë²¡í„° í˜•íƒœ: (3353, 8430)\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²¡í„° í˜•íƒœ: (664, 8430)\n",
      "\n",
      "=== LogisticRegression ëª¨ë¸ í›ˆë ¨ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\skn_after_study\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression í›ˆë ¨ ì‹œê°„: 0.29ì´ˆ\n",
      "LogisticRegression ì˜ˆì¸¡ ì‹œê°„: 0.00ì´ˆ\n",
      "LogisticRegression ì •í™•ë„: 0.0858\n",
      "LogisticRegression F1-Score (weighted): 0.0855\n",
      "LogisticRegression F1-Score (macro): 0.0503\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF ë²¡í„°í™” ë° LogisticRegression ëª¨ë¸\n",
    "print(\"\\n=== TF-IDF ë²¡í„°í™” ì§„í–‰ ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# TF-IDF ë²¡í„°ë¼ì´ì € ì„¤ì •\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,      # ìµœëŒ€ íŠ¹ì„± ìˆ˜ ì œí•œ\n",
    "    ngram_range=(1, 2),      # ìœ ë‹ˆê·¸ë¨ê³¼ ë°”ì´ê·¸ë¨ ì‚¬ìš©\n",
    "    min_df=2,                # ìµœì†Œ 2ê°œ ë¬¸ì„œì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ë§Œ ì‚¬ìš©\n",
    "    max_df=0.95,             # 95% ì´ìƒ ë¬¸ì„œì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ëŠ” ì œì™¸\n",
    "    stop_words=None,         # í•œêµ­ì–´ëŠ” ë³„ë„ ì²˜ë¦¬\n",
    "    lowercase=True,          # ì†Œë¬¸ì ë³€í™˜\n",
    "    sublinear_tf=True        # TFì— ë¡œê·¸ ìŠ¤ì¼€ì¼ë§ ì ìš©\n",
    ")\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„°ë¡œ TF-IDF í•™ìŠµ ë° ë³€í™˜\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "vectorization_time = time.time() - start_time\n",
    "print(f\"TF-IDF ë²¡í„°í™” ì™„ë£Œ - ì†Œìš”ì‹œê°„: {vectorization_time:.2f}ì´ˆ\")\n",
    "print(f\"TF-IDF ë²¡í„° ì°¨ì›: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° ë²¡í„° í˜•íƒœ: {X_train_tfidf.shape}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²¡í„° í˜•íƒœ: {X_test_tfidf.shape}\")\n",
    "\n",
    "# LogisticRegression ëª¨ë¸ í›ˆë ¨\n",
    "print(f\"\\n=== LogisticRegression ëª¨ë¸ í›ˆë ¨ ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',  # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\n",
    "    solver='liblinear'        # ë‹¤ì¤‘ë¶„ë¥˜ì— ì í•©í•œ ì†”ë²„\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "lr_train_time = time.time() - start_time\n",
    "\n",
    "# LogisticRegression í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€\n",
    "start_time = time.time()\n",
    "y_test_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "lr_pred_time = time.time() - start_time\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, y_test_pred_lr)\n",
    "lr_f1_weighted = f1_score(y_test, y_test_pred_lr, average='weighted')\n",
    "lr_f1_macro = f1_score(y_test, y_test_pred_lr, average='macro')\n",
    "\n",
    "print(f\"LogisticRegression í›ˆë ¨ ì‹œê°„: {lr_train_time:.2f}ì´ˆ\")\n",
    "print(f\"LogisticRegression ì˜ˆì¸¡ ì‹œê°„: {lr_pred_time:.2f}ì´ˆ\")\n",
    "print(f\"LogisticRegression ì •í™•ë„: {lr_accuracy:.4f}\")\n",
    "print(f\"LogisticRegression F1-Score (weighted): {lr_f1_weighted:.4f}\")\n",
    "print(f\"LogisticRegression F1-Score (macro): {lr_f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10de4ij5wm1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TF-IDF ë²¡í„°í™”ë¥¼ ì‚¬ìš©í•œ ë³µí•© ë¼ë²¨ ë¶„ë¥˜ ===\n",
      "í›ˆë ¨ ë°ì´í„° ë¼ë²¨ë³„ ìƒ˜í”Œ ìˆ˜ ë¶„í¬:\n",
      "combined_label\n",
      "ìŠ¬í””_ë¬´ê¸°ë ¥      98\n",
      "ìˆ˜ì¹˜ì‹¬_ë¶€ë„ëŸ¬ì›€    96\n",
      "ê¸°ì¨_í¸ì•ˆí•¨      92\n",
      "ìŠ¬í””_ì™¸ë¡œì›€      90\n",
      "ë‘ë ¤ì›€_ë†€ëŒ      84\n",
      "ìŠ¬í””_í—ˆë§       78\n",
      "ê¸°ì¨_ì•ˆì •ê°      78\n",
      "ê¸°ì¨_ê³µê°       77\n",
      "ê¸°ì¨_ê¸°ëŒ€ê°      77\n",
      "ë¶„ë…¸_ë¶ˆì¾Œ       72\n",
      "Name: count, dtype: int64\n",
      "ì „ì²´ ë¼ë²¨ ìˆ˜: 77\n",
      "ìƒ˜í”Œì´ 1ê°œì¸ ë¼ë²¨ ìˆ˜: 6\n",
      "\n",
      "í•„í„°ë§ í›„:\n",
      "ìœ íš¨ ë¼ë²¨ ìˆ˜: 71\n",
      "í›ˆë ¨ìš© ë°ì´í„° ìˆ˜: 3353\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ:\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: (664, 5)\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ëŸ¼: ['index', 'context', 'annotations_split', 'category1', 'category2']\n",
      "\n",
      "ìµœì¢… ë°ì´í„°:\n",
      "í›ˆë ¨ ë°ì´í„°: 3353ê°œ\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„°: 664ê°œ\n",
      "í›ˆë ¨ ë¼ë²¨ ìˆ˜: 71ê°œ\n",
      "í…ŒìŠ¤íŠ¸ ë¼ë²¨ ìˆ˜: 70ê°œ\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF ë°©ì‹ì„ ì‚¬ìš©í•œ ë³µí•© ë¼ë²¨ ë¶„ë¥˜ - ìˆ˜ì •ëœ ë²„ì „\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print(\"=== TF-IDF ë²¡í„°í™”ë¥¼ ì‚¬ìš©í•œ ë³µí•© ë¼ë²¨ ë¶„ë¥˜ ===\")\n",
    "\n",
    "# ë³µí•© ë¼ë²¨ ìƒì„± (ê¸°ì¡´ ì„ë² ë”© ë°©ì‹ê³¼ ë™ì¼í•œ ë¼ë²¨ êµ¬ì¡° ì‚¬ìš©)\n",
    "data['combined_label'] = data['re_category1'] + \"_\" + data['re_category2']\n",
    "\n",
    "# ë¼ë²¨ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸ ë° í•„í„°ë§\n",
    "label_counts = data['combined_label'].value_counts()\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° ë¼ë²¨ë³„ ìƒ˜í”Œ ìˆ˜ ë¶„í¬:\")\n",
    "print(label_counts.head(10))\n",
    "print(f\"ì „ì²´ ë¼ë²¨ ìˆ˜: {len(label_counts)}\")\n",
    "print(f\"ìƒ˜í”Œì´ 1ê°œì¸ ë¼ë²¨ ìˆ˜: {sum(label_counts == 1)}\")\n",
    "\n",
    "# ìµœì†Œ 2ê°œ ì´ìƒì˜ ìƒ˜í”Œì„ ê°€ì§„ ë¼ë²¨ë§Œ ì‚¬ìš©\n",
    "min_samples = 2\n",
    "valid_labels = label_counts[label_counts >= min_samples].index\n",
    "filtered_data = data[data['combined_label'].isin(valid_labels)]\n",
    "\n",
    "print(f\"\\ní•„í„°ë§ í›„:\")\n",
    "print(f\"ìœ íš¨ ë¼ë²¨ ìˆ˜: {len(valid_labels)}\")\n",
    "print(f\"í›ˆë ¨ìš© ë°ì´í„° ìˆ˜: {len(filtered_data)}\")\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ (dataì—ì„œ)\n",
    "X_train = filtered_data['generator_context'].values\n",
    "y_train = filtered_data['combined_label'].values\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„\n",
    "test_data = pd.read_excel(r'C:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\data\\ì¦ê°•í• ë°ì´í„°33.xlsx')\n",
    "print(f\"\\ní…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ:\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {test_data.shape}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ëŸ¼: {list(test_data.columns)}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë³µí•© ë¼ë²¨ ìƒì„±\n",
    "test_data['combined_label'] = test_data['category1'] + \"_\" + test_data['category2']\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ - context ì»¬ëŸ¼ ì‚¬ìš©\n",
    "X_test = test_data['context'].fillna('').astype(str).values\n",
    "y_test = test_data['combined_label'].values\n",
    "\n",
    "print(f\"\\nìµœì¢… ë°ì´í„°:\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ\")\n",
    "print(f\"í›ˆë ¨ ë¼ë²¨ ìˆ˜: {len(set(y_train))}ê°œ\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë¼ë²¨ ìˆ˜: {len(set(y_test))}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13495b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\skn_after_study\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f72f27ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í•„í„°ë§ ì „: 3360ê°œ\n",
      "re_category1ì—ì„œ ì¤‘ë¦½: 3ê°œ\n",
      "re_category2ì—ì„œ ì¤‘ë¦½: 1ê°œ\n",
      "ë°ì´í„° í•„í„°ë§ í›„: 3359ê°œ\n",
      "ì œê±°ëœ ë°ì´í„°: 1ê°œ\n",
      "ë‚¨ì€ re_category1 í´ë˜ìŠ¤ (10ê°œ): ['ê¸°ì¨', 'ë‘ë ¤ì›€', 'ë¯¸ì›€(ìƒëŒ€ë°©)', 'ë¶„ë…¸', 'ì‚¬ë‘', 'ìˆ˜ì¹˜ì‹¬', 'ìŠ¬í””', 'ì‹«ì–´í•¨(ìƒíƒœ)', 'ìš•ë§', 'ì¤‘ë¦½']\n",
      "ë‚¨ì€ re_category2 í´ë˜ìŠ¤ (64ê°œ): ['ê°ˆë“±', 'ê°ë™', 'ê±±ì •', 'ê²½ë©¸', 'ê³ ë§ˆì›€', 'ê³ í†µ', 'ê³µê°', 'ê³µí¬', 'ê¶ê¸ˆí•¨', 'ê·€ì¤‘í•¨', 'ê·¸ë¦¬ì›€', 'ê¸°ëŒ€ê°', 'ë‚œì²˜í•¨', 'ë‚ ì¹´ë¡œì›€', 'ëƒ‰ë‹´', 'ë„ˆê·¸ëŸ¬ì›€', 'ë†€ëŒ', 'ë‹¤ì •í•¨', 'ë‹µë‹µí•¨', 'ë™ì •(ìŠ¬í””)', 'ë‘ê·¼ê±°ë¦¼', 'ë§Œì¡±ê°', 'ë§¤ë ¥ì ', 'ë¬´ê¸°ë ¥', 'ë¯¸ì•ˆí•¨', 'ë°˜ê°€ì›€', 'ë°˜ê°', 'ë°œì—´', 'ë¶€ë„ëŸ¬ì›€', 'ë¶ˆë§Œ', 'ë¶ˆì‹ ê°', 'ë¶ˆì¾Œ', 'ë¶ˆí¸í•¨', 'ë¹„ìœ„ìƒí•¨', 'ì‚¬ë‚˜ì›€', 'ìˆ˜ì¹˜ì‹¬', 'ì‹œê¸°ì‹¬', 'ì‹ ë¢°ê°', 'ì‹ ëª…ë‚¨', 'ì‹¤ë§', 'ì‹«ì¦', 'ì‹¬ì‹¬í•¨', 'ì•„ì‰¬ì›€', 'ì•„í””', 'ì•ˆì •ê°', 'ì–µìš¸í•¨', 'ì™¸ë¡œì›€', 'ì™¸ë©´', 'ìš•ì‹¬', 'ì›ë§', 'ìœ„ì¶•ê°', 'ìë‘ìŠ¤ëŸ¬ì›€', 'ìì‹ ê°', 'ì ˆë§', 'ì£„ì±…ê°', 'ì¦ê±°ì›€', 'ì´ˆì¡°í•¨', 'ì¹˜ì‚¬í•¨', 'íƒ€ì˜¤ë¦„', 'í†µì¾Œí•¨', 'í¸ì•ˆí•¨', 'í—ˆë§', 'í˜¸ê°', 'í›„íšŒ']\n",
      "í•„í„°ë§ í›„ re_category1 ì¤‘ë¦½: 2ê°œ\n",
      "í•„í„°ë§ í›„ re_category2 ì¤‘ë¦½: 0ê°œ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>generator_context</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>input_context</th>\n",
       "      <th>original_index</th>\n",
       "      <th>augmentation_index</th>\n",
       "      <th>re_category1</th>\n",
       "      <th>re_category2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ê°‘ìê¸° ë‚´ ì±…ìƒ ìœ„ì— ë†“ì¸ ë”°ëœ»í•œ ì†í¸ì§€ì— ë§ˆìŒì´ ë­‰í´í•´ì¡Œë‹¤.</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "      <td>ì„¤íƒ• ìŠ¤í‹± ê»´ì¤€ê±° ì„¼ìŠ¤ ë°±ì  ë§Œì ì— ì²œì </td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ë¹„ê°€ ì˜¤ëŠ”ë°ë„ ì¹œêµ¬ê°€ ë‚´ ì¢‹ì•„í•˜ëŠ” ì¹´í˜ê¹Œì§€ ìš°ì‚° ë“¤ê³  ë”°ë¼ì™€ì¤˜ì„œ ë§ˆìŒì´ ë”°ëœ»í•´ì¡Œì–´.</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "      <td>ì•„ì“° ì‚°ì°¨ì´ ê¸°ë¶„ ì•ˆ ì¢‹ì€ ê±° ì•Œì•„ì±„ê³  ì‚°ì°¨ì´ê°€ ê°€ê³  ì‹¶ë‹¤ë˜ í† ë¼ì§‘ ë°ë ¤ì˜¨ ê±° ê°ë™</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>í–‡ì‚´ ì•„ë˜ ë°˜ì§ì´ëŠ” ì•„ì´ì˜ ëˆˆë™ìê°€ ë§ˆì¹˜ ì‘ì€ ë³´ì„ì²˜ëŸ¼ ë¹›ë‚¬ë‹¤. ê·¸ ìˆœê°„, ì„¸ìƒ ëª¨...</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "      <td>ì‹ ë°ë ë¼ ë“œë ˆìŠ¤ëŠ” ë‹¤ì‹œ ë´ë„ ë„ˆë¬´ ì•„ë¦„ë‹¤ì›Œ. ì‚¬ëŒì—ê²Œ ê¿ˆì˜ ë¬¼ê²°ì„ ì…íˆë‹¤ë‹ˆìš”.</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ì´ë²ˆ ì „ì‹œíšŒ ì¤€ë¹„í•˜ë©´ì„œ ì² ì €í•˜ê²Œ ì„¸ë¶€ê¹Œì§€ ì±™ê²¨ì¤€ ë•ë¶„ì— ëª¨ë“  ê²Œ ì™„ë²½í•˜ê²Œ ë§ˆë¬´ë¦¬ë¼ì„œ...</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "      <td>ì™€ ë¯¼í¬ì§„ ì”¨ ì• ë“¤ ìˆ™ì†Œ ìŠ¤íƒ€ì¼ë§ê¹Œì§€ ë§¡ê¸°ë©´ì„œ ì‹ ê²½ì¨ ì¤€ ê±° ì§„ì§œ ì¢€ ëŒ€ë‹¨í•˜ë„¤</td>\n",
       "      <td>107.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ë¹„ ì˜¤ëŠ” ë‚  ë‚¯ì„  ì‚¬ëŒì´ ë‚´ê²Œ ë‹´ìš”ë¥¼ ê±´ë„¤ë©° ì¶”ìœ„ ê±±ì •í•´ ì¤¬ë‹¤. ë§ˆìŒì´ ë”°ëœ»í•´ì ¸ì„œ ...</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "      <td>ê°œê°ë™ì¸ ê±° ìê¸°ê°€ ì“°ê³  ìˆë˜ ìš°ì‚° ë‚˜ ì£¼ê³ \\nìê¸°ê°€ ë¹„ ë§ì•„ê°€ë©´ì„œ ë’¤ì§‘ì–´ì¤€ ê±°ì•¼\\...</td>\n",
       "      <td>137.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê¸°ì¨</td>\n",
       "      <td>ê°ë™</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                  generator_context category1  \\\n",
       "0           0                 ê°‘ìê¸° ë‚´ ì±…ìƒ ìœ„ì— ë†“ì¸ ë”°ëœ»í•œ ì†í¸ì§€ì— ë§ˆìŒì´ ë­‰í´í•´ì¡Œë‹¤.        ê¸°ì¨   \n",
       "1           1     ë¹„ê°€ ì˜¤ëŠ”ë°ë„ ì¹œêµ¬ê°€ ë‚´ ì¢‹ì•„í•˜ëŠ” ì¹´í˜ê¹Œì§€ ìš°ì‚° ë“¤ê³  ë”°ë¼ì™€ì¤˜ì„œ ë§ˆìŒì´ ë”°ëœ»í•´ì¡Œì–´.        ê¸°ì¨   \n",
       "2           2  í–‡ì‚´ ì•„ë˜ ë°˜ì§ì´ëŠ” ì•„ì´ì˜ ëˆˆë™ìê°€ ë§ˆì¹˜ ì‘ì€ ë³´ì„ì²˜ëŸ¼ ë¹›ë‚¬ë‹¤. ê·¸ ìˆœê°„, ì„¸ìƒ ëª¨...        ê¸°ì¨   \n",
       "3           3  ì´ë²ˆ ì „ì‹œíšŒ ì¤€ë¹„í•˜ë©´ì„œ ì² ì €í•˜ê²Œ ì„¸ë¶€ê¹Œì§€ ì±™ê²¨ì¤€ ë•ë¶„ì— ëª¨ë“  ê²Œ ì™„ë²½í•˜ê²Œ ë§ˆë¬´ë¦¬ë¼ì„œ...        ê¸°ì¨   \n",
       "4           4  ë¹„ ì˜¤ëŠ” ë‚  ë‚¯ì„  ì‚¬ëŒì´ ë‚´ê²Œ ë‹´ìš”ë¥¼ ê±´ë„¤ë©° ì¶”ìœ„ ê±±ì •í•´ ì¤¬ë‹¤. ë§ˆìŒì´ ë”°ëœ»í•´ì ¸ì„œ ...        ê¸°ì¨   \n",
       "\n",
       "  category2                                      input_context  \\\n",
       "0        ê°ë™                             ì„¤íƒ• ìŠ¤í‹± ê»´ì¤€ê±° ì„¼ìŠ¤ ë°±ì  ë§Œì ì— ì²œì    \n",
       "1        ê°ë™     ì•„ì“° ì‚°ì°¨ì´ ê¸°ë¶„ ì•ˆ ì¢‹ì€ ê±° ì•Œì•„ì±„ê³  ì‚°ì°¨ì´ê°€ ê°€ê³  ì‹¶ë‹¤ë˜ í† ë¼ì§‘ ë°ë ¤ì˜¨ ê±° ê°ë™   \n",
       "2        ê°ë™        ì‹ ë°ë ë¼ ë“œë ˆìŠ¤ëŠ” ë‹¤ì‹œ ë´ë„ ë„ˆë¬´ ì•„ë¦„ë‹¤ì›Œ. ì‚¬ëŒì—ê²Œ ê¿ˆì˜ ë¬¼ê²°ì„ ì…íˆë‹¤ë‹ˆìš”.   \n",
       "3        ê°ë™        ì™€ ë¯¼í¬ì§„ ì”¨ ì• ë“¤ ìˆ™ì†Œ ìŠ¤íƒ€ì¼ë§ê¹Œì§€ ë§¡ê¸°ë©´ì„œ ì‹ ê²½ì¨ ì¤€ ê±° ì§„ì§œ ì¢€ ëŒ€ë‹¨í•˜ë„¤   \n",
       "4        ê°ë™  ê°œê°ë™ì¸ ê±° ìê¸°ê°€ ì“°ê³  ìˆë˜ ìš°ì‚° ë‚˜ ì£¼ê³ \\nìê¸°ê°€ ë¹„ ë§ì•„ê°€ë©´ì„œ ë’¤ì§‘ì–´ì¤€ ê±°ì•¼\\...   \n",
       "\n",
       "   original_index  augmentation_index re_category1 re_category2  \n",
       "0            20.0                 NaN           ê¸°ì¨           ê°ë™  \n",
       "1            79.0                 NaN           ê¸°ì¨           ê°ë™  \n",
       "2           104.0                 NaN           ê¸°ì¨           ê°ë™  \n",
       "3           107.0                 NaN           ê¸°ì¨           ê°ë™  \n",
       "4           137.0                 NaN           ê¸°ì¨           ê°ë™  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(r'C:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\data\\retest_augmentation.xlsx')\n",
    "\n",
    "# re_category2ì—ì„œë§Œ ì¤‘ë¦½ ë°ì´í„° ì œê±° (re_category1ì€ ì¤‘ë¦½ ìœ ì§€)\n",
    "print(f\"ë°ì´í„° í•„í„°ë§ ì „: {len(data)}ê°œ\")\n",
    "print(f\"re_category1ì—ì„œ ì¤‘ë¦½: {(data['re_category1'] == 'ì¤‘ë¦½').sum()}ê°œ\")\n",
    "print(f\"re_category2ì—ì„œ ì¤‘ë¦½: {(data['re_category2'] == 'ì¤‘ë¦½').sum()}ê°œ\")\n",
    "\n",
    "# re_category2ì—ì„œë§Œ ì¤‘ë¦½ ë°ì´í„° ì œê±° (Category2ì— ì¤‘ë¦½ì´ ì—†ìœ¼ë¯€ë¡œ ë¼ë²¨ ë¶ˆì¼ì¹˜ ë°©ì§€)\n",
    "original_count = len(data)\n",
    "data = data[data['re_category2'] != 'ì¤‘ë¦½'].copy()\n",
    "\n",
    "# ì¸ë±ìŠ¤ ì¬ì„¤ì •\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print(f\"ë°ì´í„° í•„í„°ë§ í›„: {len(data)}ê°œ\")\n",
    "print(f\"ì œê±°ëœ ë°ì´í„°: {original_count - len(data)}ê°œ\")\n",
    "\n",
    "# í•„í„°ë§ëœ ë°ì´í„° í™•ì¸\n",
    "print(f\"ë‚¨ì€ re_category1 í´ë˜ìŠ¤ ({len(data['re_category1'].unique())}ê°œ): {sorted(data['re_category1'].unique())}\")\n",
    "print(f\"ë‚¨ì€ re_category2 í´ë˜ìŠ¤ ({len(data['re_category2'].unique())}ê°œ): {sorted(data['re_category2'].unique())}\")\n",
    "\n",
    "# ì¤‘ë¦½ í™•ì¸\n",
    "print(f\"í•„í„°ë§ í›„ re_category1 ì¤‘ë¦½: {(data['re_category1'] == 'ì¤‘ë¦½').sum()}ê°œ\")\n",
    "print(f\"í•„í„°ë§ í›„ re_category2 ì¤‘ë¦½: {(data['re_category2'] == 'ì¤‘ë¦½').sum()}ê°œ\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbf27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_model():\n",
    "  \"\"\"\n",
    "  ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "  \"\"\"\n",
    "  model = SentenceTransformer(\"dragonkue/snowflake-arctic-embed-l-v2.0-ko\") \n",
    "  vec_dim = len(model.encode(\"dummy_text\"))\n",
    "  print(f\"ëª¨ë¸ ì°¨ì›: {vec_dim}\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c873e343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ì°¨ì›: 1024\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = embeddings_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4926f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê¸°ì¡´ ë³€ìˆ˜ë“¤ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3359 entries, 0 to 3358\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0          3359 non-null   int64  \n",
      " 1   generator_context   3359 non-null   object \n",
      " 2   category1           3359 non-null   object \n",
      " 3   category2           3359 non-null   object \n",
      " 4   input_context       3359 non-null   object \n",
      " 5   original_index      664 non-null    float64\n",
      " 6   augmentation_index  2695 non-null   float64\n",
      " 7   re_category1        3359 non-null   object \n",
      " 8   re_category2        3359 non-null   object \n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 236.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ì¡´ ë³€ìˆ˜ ì´ˆê¸°í™” (ì¤‘ë¦½ ë°ì´í„° ì œê±°ë¡œ ì¸í•œ í¬ê¸° ë¶ˆì¼ì¹˜ ë°©ì§€)\n",
    "vars_to_reset = ['X', 'y', 'y_encoded', 'X_combined', 'y_cat2', 'y_cat2_encoded', 'le', 'le_cat2', 'cat1_encoder']\n",
    "for var_name in vars_to_reset:\n",
    "    if var_name in locals():\n",
    "        del locals()[var_name]\n",
    "        print(f\"ë³€ìˆ˜ {var_name} ì´ˆê¸°í™”ë¨\")\n",
    "\n",
    "print(\"âœ… ê¸°ì¡´ ë³€ìˆ˜ë“¤ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ë°ì´í„° ì •ë³´ í™•ì¸\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd04d86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ í•„í„°ë§ëœ ë°ì´í„°ë¡œ ë²¡í„° ìƒì„±...\n",
      "âœ… ë²¡í„° ìƒì„± ì™„ë£Œ: 3359ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ì¤‘ë¦½ ë°ì´í„° ì œê±° í›„ ë²¡í„° ìƒì„±\n",
    "print(\"ğŸ“ í•„í„°ë§ëœ ë°ì´í„°ë¡œ ë²¡í„° ìƒì„±...\")\n",
    "data['vector'] = data['generator_context'].apply(lambda x: embeddings_model.encode(x).tolist())\n",
    "print(f\"âœ… ë²¡í„° ìƒì„± ì™„ë£Œ: {len(data)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b7dde",
   "metadata": {},
   "source": [
    "### category1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c27bb42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ìƒíƒœ í™•ì¸:\n",
      "í•„í„°ë§ëœ ë°ì´í„° ê°œìˆ˜: 3359\n",
      "ë²¡í„° íƒ€ì…: <class 'list'>\n",
      "ë²¡í„° ê¸¸ì´: 1024\n",
      "ë²¡í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì…ë‹ˆë‹¤. ì§ì ‘ ë³€í™˜í•©ë‹ˆë‹¤...\n",
      "X shape: (3359, 1024)\n",
      "y shape: (3359,)\n",
      "âœ… Xì™€ yì˜ í¬ê¸°ê°€ ì¼ì¹˜í•©ë‹ˆë‹¤!\n",
      "âœ… ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "# í•„í„°ë§ëœ ë°ì´í„°ë¡œ ë²¡í„°ì™€ ë¼ë²¨ ìƒì„±\n",
    "print(\"ë°ì´í„° ìƒíƒœ í™•ì¸:\")\n",
    "print(f\"í•„í„°ë§ëœ ë°ì´í„° ê°œìˆ˜: {len(data)}\")\n",
    "print(f\"ë²¡í„° íƒ€ì…: {type(data['vector'].iloc[0])}\")\n",
    "print(f\"ë²¡í„° ê¸¸ì´: {len(data['vector'].iloc[0])}\")\n",
    "\n",
    "# ë²¡í„°ê°€ ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¼ë©´ ì§ì ‘ numpy arrayë¡œ ë³€í™˜\n",
    "if isinstance(data['vector'].iloc[0], list):\n",
    "    print(\"ë²¡í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì…ë‹ˆë‹¤. ì§ì ‘ ë³€í™˜í•©ë‹ˆë‹¤...\")\n",
    "    X = np.vstack(data['vector'].values)\n",
    "    y = data['re_category1'].values  # ë³€ê²½: category1 â†’ re_category1\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "    # í¬ê¸° ì¼ì¹˜ í™•ì¸\n",
    "    if X.shape[0] == y.shape[0]:\n",
    "        print(\"âœ… Xì™€ yì˜ í¬ê¸°ê°€ ì¼ì¹˜í•©ë‹ˆë‹¤!\")\n",
    "    else:\n",
    "        print(f\"âŒ í¬ê¸° ë¶ˆì¼ì¹˜: X {X.shape[0]} vs y {y.shape[0]}\")\n",
    "    \n",
    "    print(\"âœ… ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"ë²¡í„° í˜•íƒœì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wqea3alqoni",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ì‹¤ì œ test_data ë¡œë“œ ë° í‰ê°€\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ë³¸ ì •ë³´:\n",
      "ë°ì´í„° í¬ê¸°: (664, 5)\n",
      "ì»¬ëŸ¼ë“¤: ['index', 'context', 'annotations_split', 'category1', 'category2']\n",
      "\n",
      "ë°ì´í„° ìƒ˜í”Œ:\n",
      "   index                                            context  \\\n",
      "0      0  ë³´ëŠ”ë™ì•ˆ ë„ˆë¬´ í–‰ë³µí–ˆê³  ì´ˆì½œë ›ì´ ë„ˆë¬´ ë¨¹ê³ ì‹¶ì—ˆê³  í‹°ëª¨ì‹œê°€ ì˜ìƒê²¼ê³  ìš¸ì–´!!í•˜ëŠ”ë¶€ë¶„ì´...   \n",
      "1      1  ì–´ë¦´ ë•Œ ê°€ ë³´ê³  ë¹•ìŠ¤ëŠ” ê±°ì˜ ì²˜ìŒì¸ë°(ê¸°ì–µì— ì—†ìŒ) ì§€ê¸ˆ ë”¸ê¸°ì¶•ì œ ê¸°ê°„ì´ë¼ ë§Œì¡±ìŠ¤...   \n",
      "2      2  ë¯¸ë¦¬ ê³„ì¢Œë¡œ í™˜ì „í•´ë‘” ëˆì„ í•´ì™¸ì—ì„œ í™˜ì „ìˆ˜ìˆ˜ë£Œ ì—†ì´ ì¸ì¶œ ê°€ëŠ¥í•œ íŠ¸ë ˆë¸”ë¡œê·¸ë¼ëŠ” ì¹´ë“œ...   \n",
      "3      3  ìš”ì¦˜ ë²ˆì•„ì›ƒë„ ìê¾¸ ì˜¬ë¼ì˜¤ê³  ë¬´ê¸°ë ¥í•´ì„œ ì¢…ê°•í•˜ê³  êµë¥˜í•˜ê¸°ë„ ë²„ê±°ìš´ ìƒíƒœê°€ ì™€ë¶€ë €ìœ¼ìš”ã… ã…     \n",
      "4      4  í¬ë¼ì„ì”¬ ì¥ë˜¥ë¯¼ì´ ë²”í–‰ ë„êµ¬ ì°¾ìœ¼ë ¤ê³  í™”ì¥ì‹¤ íƒ±í¬ ë’¤ì§€ëŠ”ë° ê±°ê¸°ì— ì§„ì§œ ë˜¥ ë„£ì–´ë†“ì€...   \n",
      "\n",
      "                                   annotations_split category1 category2  \n",
      "0  [['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ê°ë™'], [...        ê¸°ì¨       ë§Œì¡±ê°  \n",
      "1  [['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ...        ê¸°ì¨       ë§Œì¡±ê°  \n",
      "2  [['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ['ê¸°ì¨', 'ë§Œì¡±ê°'], ...        ê¸°ì¨       ë§Œì¡±ê°  \n",
      "3  [['ìŠ¬í””', 'ë¬´ê¸°ë ¥'], ['ì‹«ì–´í•¨(ìƒíƒœ)', 'ë¬´ê¸°ë ¥'], ['ìŠ¬í””', 'ë¬´ê¸°...        ìŠ¬í””       ë¬´ê¸°ë ¥  \n",
      "4  [['ê¸°ì¨', 'ì¦ê±°ì›€'], ['ê¸°ì¨', 'í†µì¾Œí•¨'], ['ê¸°ì¨', 'í†µì¾Œí•¨'], ...        ê¸°ì¨       ì¦ê±°ì›€  \n",
      "\n",
      "test_data ì»¬ëŸ¼ í™•ì¸:\n",
      "- index: int64\n",
      "- context: object\n",
      "- annotations_split: object\n",
      "- category1: object\n",
      "- category2: object\n",
      "\n",
      "ì‹ë³„ëœ ì»¬ëŸ¼:\n",
      "í…ìŠ¤íŠ¸ ì»¬ëŸ¼: context\n",
      "Category1 ì»¬ëŸ¼: category1\n",
      "\n",
      "âœ… í•„ìš”í•œ ì»¬ëŸ¼ë“¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: 664\n",
      "Category1 í´ë˜ìŠ¤ë“¤: ['ê¸°ì¨' 'ìŠ¬í””' 'ì‹«ì–´í•¨(ìƒíƒœ)' 'ë¯¸ì›€(ìƒëŒ€ë°©)' 'ë‘ë ¤ì›€' 'ìˆ˜ì¹˜ì‹¬' 'ìš•ë§' 'ë¶„ë…¸' 'ì‚¬ë‘' 'ì¤‘ë¦½']\n"
     ]
    }
   ],
   "source": [
    "# 9. ì‹¤ì œ test_dataë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
    "\n",
    "print(\"ğŸ“ ì‹¤ì œ test_data ë¡œë“œ ë° í‰ê°€\\n\")\n",
    "\n",
    "# test_data ë¡œë“œ\n",
    "test_data = pd.read_excel(r'C:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\data\\ì¦ê°•í• ë°ì´í„°33.xlsx')\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ë³¸ ì •ë³´:\")\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {test_data.shape}\")\n",
    "print(f\"ì»¬ëŸ¼ë“¤: {list(test_data.columns)}\")\n",
    "print(\"\\në°ì´í„° ìƒ˜í”Œ:\")\n",
    "print(test_data.head())\n",
    "\n",
    "# test_dataì—ì„œ í…ìŠ¤íŠ¸ì™€ category1 ì»¬ëŸ¼ í™•ì¸\n",
    "print(f\"\\ntest_data ì»¬ëŸ¼ í™•ì¸:\")\n",
    "for col in test_data.columns:\n",
    "    print(f\"- {col}: {test_data[col].dtype}\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì»¬ëŸ¼ê³¼ category1 ì»¬ëŸ¼ ì‹ë³„ (ì»¬ëŸ¼ëª…ì— ë”°ë¼ ì¡°ì • í•„ìš”)\n",
    "text_column = None\n",
    "category1_column = None\n",
    "\n",
    "# ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…ë“¤\n",
    "possible_text_columns = ['context', 'text', 'content', 'sentence', 'ë‚´ìš©', 'ë¬¸ì¥']\n",
    "for col in test_data.columns:\n",
    "    if any(keyword in col.lower() for keyword in possible_text_columns):\n",
    "        text_column = col\n",
    "        break\n",
    "\n",
    "# ê°€ëŠ¥í•œ category1 ì»¬ëŸ¼ëª…ë“¤\n",
    "possible_cat1_columns = ['category1', 'cat1', 'label', 'ê°ì •', 'ì¹´í…Œê³ ë¦¬1']\n",
    "for col in test_data.columns:\n",
    "    if any(keyword in col.lower() for keyword in possible_cat1_columns):\n",
    "        category1_column = col\n",
    "        break\n",
    "\n",
    "print(f\"\\nì‹ë³„ëœ ì»¬ëŸ¼:\")\n",
    "print(f\"í…ìŠ¤íŠ¸ ì»¬ëŸ¼: {text_column}\")\n",
    "print(f\"Category1 ì»¬ëŸ¼: {category1_column}\")\n",
    "\n",
    "if text_column and category1_column:\n",
    "    print(f\"\\nâœ… í•„ìš”í•œ ì»¬ëŸ¼ë“¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: {len(test_data)}\")\n",
    "    print(f\"Category1 í´ë˜ìŠ¤ë“¤: {test_data[category1_column].unique()}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ í•„ìš”í•œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì§€ì •í•´ì£¼ì„¸ìš”.\")\n",
    "    print(\"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤:\")\n",
    "    for i, col in enumerate(test_data.columns):\n",
    "        print(f\"{i}: {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed4954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Category1 ëª¨ë¸ì˜ test_data ì„±ëŠ¥ í‰ê°€\n",
      "\n",
      "y_encoded ë³€ìˆ˜ë¥¼ ì¬ì •ì˜í•©ë‹ˆë‹¤...\n",
      "y_encoded shape: (3359,)\n",
      "ğŸ“ test_data í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘...\n",
      "âœ… ì„ë² ë”© ì™„ë£Œ: (664, 1024)\n",
      "ì‹¤ì œ ë¼ë²¨: 664\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤‘ë¦½ ê°œìˆ˜: 5ê°œ\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° category1 í´ë˜ìŠ¤ (10ê°œ): ['ê¸°ì¨', 'ë‘ë ¤ì›€', 'ë¯¸ì›€(ìƒëŒ€ë°©)', 'ë¶„ë…¸', 'ì‚¬ë‘', 'ìˆ˜ì¹˜ì‹¬', 'ìŠ¬í””', 'ì‹«ì–´í•¨(ìƒíƒœ)', 'ìš•ë§', 'ì¤‘ë¦½']\n",
      "\n",
      "ğŸ”„ ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ...\n",
      "âœ… ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n",
      "\n",
      "ğŸ¯ test_data ì˜ˆì¸¡ ìˆ˜í–‰...\n",
      "\n",
      "ğŸ“‹ í´ë˜ìŠ¤ ì •ë³´:\n",
      "í•™ìŠµ í´ë˜ìŠ¤ ìˆ˜: 10\n",
      "í•™ìŠµ í´ë˜ìŠ¤: ['ê¸°ì¨', 'ë‘ë ¤ì›€', 'ë¯¸ì›€(ìƒëŒ€ë°©)', 'ë¶„ë…¸', 'ì‚¬ë‘', 'ìˆ˜ì¹˜ì‹¬', 'ìŠ¬í””', 'ì‹«ì–´í•¨(ìƒíƒœ)', 'ìš•ë§', 'ì¤‘ë¦½']\n",
      "í…ŒìŠ¤íŠ¸ ì‹¤ì œ í´ë˜ìŠ¤ ìˆ˜: 10\n",
      "í…ŒìŠ¤íŠ¸ ì‹¤ì œ í´ë˜ìŠ¤: ['ê¸°ì¨', 'ë‘ë ¤ì›€', 'ë¯¸ì›€(ìƒëŒ€ë°©)', 'ë¶„ë…¸', 'ì‚¬ë‘', 'ìˆ˜ì¹˜ì‹¬', 'ìŠ¬í””', 'ì‹«ì–´í•¨(ìƒíƒœ)', 'ìš•ë§', 'ì¤‘ë¦½']\n",
      "ê³µí†µ í´ë˜ìŠ¤ ìˆ˜: 10\n",
      "âœ… í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ Category1 í´ë˜ìŠ¤ê°€ ì™„ë²½íˆ ì¼ì¹˜í•©ë‹ˆë‹¤!\n",
      "\n",
      "ğŸ“Š Classification Report:\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ê¸°ì¨       0.65      0.78      0.71       188\n",
      "         ë‘ë ¤ì›€       0.92      0.15      0.26        72\n",
      "     ë¯¸ì›€(ìƒëŒ€ë°©)       0.37      0.70      0.48        43\n",
      "          ë¶„ë…¸       0.25      0.33      0.29        36\n",
      "          ì‚¬ë‘       0.43      0.06      0.11        47\n",
      "         ìˆ˜ì¹˜ì‹¬       0.33      0.08      0.13        25\n",
      "          ìŠ¬í””       0.52      0.65      0.58       117\n",
      "     ì‹«ì–´í•¨(ìƒíƒœ)       0.26      0.10      0.15        49\n",
      "          ìš•ë§       0.33      0.48      0.39        82\n",
      "          ì¤‘ë¦½       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.49       664\n",
      "   macro avg       0.41      0.33      0.31       664\n",
      "weighted avg       0.51      0.49      0.45       664\n",
      "\n",
      "\n",
      "ğŸ¯ ì „ì²´ ì •í™•ë„: 0.4880 (48.80%)\n",
      "í‰ê°€ ë°ì´í„°: 664ê°œ ëª¨ë‘ í‰ê°€\n",
      "\n",
      "ğŸ” ì˜ˆì¸¡ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):\n",
      "------------------------------------------------------------------------------------------\n",
      " 1. âœ… ì‹¤ì œ: ê¸°ì¨           ì˜ˆì¸¡: ê¸°ì¨          \n",
      "    í…ìŠ¤íŠ¸: ë³´ëŠ”ë™ì•ˆ ë„ˆë¬´ í–‰ë³µí–ˆê³  ì´ˆì½œë ›ì´ ë„ˆë¬´ ë¨¹ê³ ì‹¶ì—ˆê³  í‹°ëª¨ì‹œê°€ ì˜ìƒê²¼ê³  ìš¸ì–´!!í•˜ëŠ”ë¶€ë¶„ì´ ìˆì–´ì„œ ìš¸ì—ˆë‹¤ë„¤ìš”\n",
      "\n",
      " 2. âœ… ì‹¤ì œ: ê¸°ì¨           ì˜ˆì¸¡: ê¸°ì¨          \n",
      "    í…ìŠ¤íŠ¸: ì–´ë¦´ ë•Œ ê°€ ë³´ê³  ë¹•ìŠ¤ëŠ” ê±°ì˜ ì²˜ìŒì¸ë°(ê¸°ì–µì— ì—†ìŒ) ì§€ê¸ˆ ë”¸ê¸°ì¶•ì œ ê¸°ê°„ì´ë¼ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì‹ì‚¬ í•˜ê³  ì˜´\n",
      "\n",
      " 3. âœ… ì‹¤ì œ: ê¸°ì¨           ì˜ˆì¸¡: ê¸°ì¨          \n",
      "    í…ìŠ¤íŠ¸: ë¯¸ë¦¬ ê³„ì¢Œë¡œ í™˜ì „í•´ë‘” ëˆì„ í•´ì™¸ì—ì„œ í™˜ì „ìˆ˜ìˆ˜ë£Œ ì—†ì´ ì¸ì¶œ ê°€ëŠ¥í•œ íŠ¸ë ˆë¸”ë¡œê·¸ë¼ëŠ” ì¹´ë“œì¸ë°, ì„ íƒí•  ìˆ˜ ìˆëŠ” ë””...\n",
      "\n",
      " 4. âŒ ì‹¤ì œ: ìŠ¬í””           ì˜ˆì¸¡: ì‹«ì–´í•¨(ìƒíƒœ)     \n",
      "    í…ìŠ¤íŠ¸: ìš”ì¦˜ ë²ˆì•„ì›ƒë„ ìê¾¸ ì˜¬ë¼ì˜¤ê³  ë¬´ê¸°ë ¥í•´ì„œ ì¢…ê°•í•˜ê³  êµë¥˜í•˜ê¸°ë„ ë²„ê±°ìš´ ìƒíƒœê°€ ì™€ë¶€ë €ìœ¼ìš”ã… ã…  \n",
      "\n",
      " 5. âŒ ì‹¤ì œ: ê¸°ì¨           ì˜ˆì¸¡: ë¯¸ì›€(ìƒëŒ€ë°©)     \n",
      "    í…ìŠ¤íŠ¸: í¬ë¼ì„ì”¬ ì¥ë˜¥ë¯¼ì´ ë²”í–‰ ë„êµ¬ ì°¾ìœ¼ë ¤ê³  í™”ì¥ì‹¤ íƒ±í¬ ë’¤ì§€ëŠ”ë° ê±°ê¸°ì— ì§„ì§œ ë˜¥ ë„£ì–´ë†“ì€ ê±° ì§„ì§œ ì›ƒê²¨ ë’¤ì§€ê² ìŒã…‹...\n",
      "\n",
      " 6. âŒ ì‹¤ì œ: ì‹«ì–´í•¨(ìƒíƒœ)      ì˜ˆì¸¡: ìŠ¬í””          \n",
      "    í…ìŠ¤íŠ¸: ê°€ìŠ´ì´ ë‹µë‹µí•´ì§ ì§„ì§œ ê°œë‹µë‹µí•´ì§\n",
      "ìš°ë¦¬ì§„ì§œíˆ¬í‘œì˜í•˜ì\n",
      "\n",
      " 7. âŒ ì‹¤ì œ: ê¸°ì¨           ì˜ˆì¸¡: ìš•ë§          \n",
      "    í…ìŠ¤íŠ¸: ì§€ê·¸ì¬ê·¸ë‘ ì—ì´ë¸”ë¦¬ë‘ í• ì¸ ëŒ€ê²°í•˜ë‚˜\n",
      "ì•„ íë­‡í•´\n",
      "ê³„ì†ë˜ê¸¸...\n",
      "ì˜ì›íˆ....\n",
      "\n",
      " 8. âŒ ì‹¤ì œ: ê¸°ì¨           ì˜ˆì¸¡: ìš•ë§          \n",
      "    í…ìŠ¤íŠ¸: ì²¨ìœ¼ë¡œ ìˆ˜ì œ ì´ˆì½œë¦¿ ë§Œë“¬\n",
      "ì´ˆì½œë¦¿ì„ 5ì‹œê°„ì´ë‚˜ ë§Œë“œëŠ” ì‚¬ëŒì´ ìˆë‹¤??? ê·¸ê²Œ ë°”ë¡œ ë‚˜\n",
      "\n",
      " 9. âŒ ì‹¤ì œ: ìŠ¬í””           ì˜ˆì¸¡: ê¸°ì¨          \n",
      "    í…ìŠ¤íŠ¸: ë‹¨í†¡ë°©ì— ê³µì§€ë“¤ ìŠ¬ìŠ¬ ì˜¬ë¼ì˜¤ëŠ”ê±° ë³´ë‹ˆê¹Œ ê³§ ê°œê°•ì´ë¼ëŠ”ê²Œ ì‹¤ê°ë‚˜ì„œ ê°‘ìê¸° ì¬ê¸°í•˜ê³ ì‹¶ê³  ì¸ìƒì´ ë‹¤ ëë‚œê±°ì²˜ëŸ¼ ì•”...\n",
      "\n",
      "10. âœ… ì‹¤ì œ: ê¸°ì¨           ì˜ˆì¸¡: ê¸°ì¨          \n",
      "    í…ìŠ¤íŠ¸: ì¥í¥ì‹  ê³µì†í•œ ì†ê°€ë½ì§ˆ ê°œì›ƒê²¨ìš”\n",
      "ì• ë“œë¦½ ë¯¸ì¹œê²ƒ ê°™ìŒ\n",
      "\n",
      "\n",
      "ğŸ“ˆ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ìš”ì•½:\n",
      "------------------------------------------------------------\n",
      "í´ë˜ìŠ¤             ì „ì²´       ì •ë‹µ       ì •í™•ë„       \n",
      "--------------------------------------------------\n",
      "ê¸°ì¨              188      146      0.7766\n",
      "ë‘ë ¤ì›€             72       11       0.1528\n",
      "ë¯¸ì›€(ìƒëŒ€ë°©)         43       30       0.6977\n",
      "ë¶„ë…¸              36       12       0.3333\n",
      "ì‚¬ë‘              47       3        0.0638\n",
      "ìˆ˜ì¹˜ì‹¬             25       2        0.0800\n",
      "ìŠ¬í””              117      76       0.6496\n",
      "ì‹«ì–´í•¨(ìƒíƒœ)         49       5        0.1020\n",
      "ìš•ë§              82       39       0.4756\n",
      "ì¤‘ë¦½              5        0        0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\skn_after_study\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\user\\anaconda3\\envs\\skn_after_study\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\user\\anaconda3\\envs\\skn_after_study\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# K-Foldë¡œ í‰ê°€í•œ Category1 ëª¨ë¸ë¡œ test_data ì˜ˆì¸¡ ë° classification_report\n",
    "\n",
    "print(\"ğŸ¯ Category1 ëª¨ë¸ì˜ test_data ì„±ëŠ¥ í‰ê°€\\n\")\n",
    "\n",
    "# 1. í•™ìŠµ ë°ì´í„° X, y ë³€ìˆ˜ ì •ì˜ (í•„ìš”ì‹œ)\n",
    "if 'X' not in locals():\n",
    "    print(\"X ë³€ìˆ˜ë¥¼ ì¬ì •ì˜í•©ë‹ˆë‹¤...\")\n",
    "    X = np.vstack(data['vector'].values)\n",
    "    y = data['re_category1'].values  # ë³€ê²½: category1 â†’ re_category1 (ì¤‘ë¦½ í¬í•¨)\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"í›ˆë ¨ ë°ì´í„° re_category1 í´ë˜ìŠ¤ ({len(np.unique(y))}ê°œ): {sorted(np.unique(y))}\")\n",
    "\n",
    "# 2. y_encoded ì •ì˜ (í•„ìš”ì‹œ)\n",
    "if 'y_encoded' not in locals():\n",
    "    print(\"y_encoded ë³€ìˆ˜ë¥¼ ì¬ì •ì˜í•©ë‹ˆë‹¤...\")\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    print(f\"y_encoded shape: {y_encoded.shape}\")\n",
    "\n",
    "# 3. test_dataì˜ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
    "print(\"ğŸ“ test_data í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘...\")\n",
    "test_texts = test_data['context'].fillna('').astype(str).tolist()\n",
    "test_vectors = []\n",
    "\n",
    "for text in test_texts:\n",
    "    vector = embeddings_model.encode(text)\n",
    "    test_vectors.append(vector)\n",
    "\n",
    "test_X = np.vstack(test_vectors)\n",
    "test_y_actual = test_data['category1'].values\n",
    "\n",
    "print(f\"âœ… ì„ë² ë”© ì™„ë£Œ: {test_X.shape}\")\n",
    "print(f\"ì‹¤ì œ ë¼ë²¨: {len(test_y_actual)}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì¤‘ë¦½ í™•ì¸\n",
    "test_neutral_count = (test_y_actual == 'ì¤‘ë¦½').sum()\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤‘ë¦½ ê°œìˆ˜: {test_neutral_count}ê°œ\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° category1 í´ë˜ìŠ¤ ({len(np.unique(test_y_actual))}ê°œ): {sorted(np.unique(test_y_actual))}\")\n",
    "\n",
    "# 4. ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ\n",
    "print(\"\\nğŸ”„ ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ...\")\n",
    "final_cat1_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ\n",
    "final_cat1_model.fit(X, y_encoded)\n",
    "print(\"âœ… ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# 5. test_dataë¡œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "print(\"\\nğŸ¯ test_data ì˜ˆì¸¡ ìˆ˜í–‰...\")\n",
    "test_y_pred_encoded = final_cat1_model.predict(test_X)\n",
    "test_y_pred = le.inverse_transform(test_y_pred_encoded)\n",
    "\n",
    "# 6. í•™ìŠµ í´ë˜ìŠ¤ì™€ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¹„êµ\n",
    "train_classes = set(le.classes_)\n",
    "test_actual_classes = set(test_y_actual)\n",
    "test_pred_classes = set(test_y_pred)\n",
    "\n",
    "print(f\"\\nğŸ“‹ í´ë˜ìŠ¤ ì •ë³´:\")\n",
    "print(f\"í•™ìŠµ í´ë˜ìŠ¤ ìˆ˜: {len(train_classes)}\")\n",
    "print(f\"í•™ìŠµ í´ë˜ìŠ¤: {sorted(train_classes)}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ì‹¤ì œ í´ë˜ìŠ¤ ìˆ˜: {len(test_actual_classes)}\")  \n",
    "print(f\"í…ŒìŠ¤íŠ¸ ì‹¤ì œ í´ë˜ìŠ¤: {sorted(test_actual_classes)}\")\n",
    "\n",
    "# í•™ìŠµì— ì—†ëŠ” í´ë˜ìŠ¤ í™•ì¸\n",
    "unseen_classes = test_actual_classes - train_classes\n",
    "if unseen_classes:\n",
    "    print(f\"âš ï¸ í•™ìŠµì— ì—†ë˜ í´ë˜ìŠ¤ë“¤: {unseen_classes}\")\n",
    "    \n",
    "common_classes = train_classes & test_actual_classes\n",
    "print(f\"ê³µí†µ í´ë˜ìŠ¤ ìˆ˜: {len(common_classes)}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ë§¤ì¹˜ í™•ì¸\n",
    "if len(train_classes) == len(test_actual_classes) == len(common_classes):\n",
    "    print(\"âœ… í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ Category1 í´ë˜ìŠ¤ê°€ ì™„ë²½íˆ ì¼ì¹˜í•©ë‹ˆë‹¤!\")\n",
    "    perfect_match = True\n",
    "else:\n",
    "    print(\"âŒ í´ë˜ìŠ¤ ë¶ˆì¼ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    perfect_match = False\n",
    "\n",
    "# 7. classification_report ìƒì„±\n",
    "print(f\"\\nğŸ“Š Classification Report:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if perfect_match:\n",
    "    # ëª¨ë“  í´ë˜ìŠ¤ê°€ ê³µí†µì¸ ê²½ìš° - ì „ì²´ í‰ê°€ ê°€ëŠ¥\n",
    "    test_y_actual_encoded = le.transform(test_y_actual)\n",
    "    report = classification_report(\n",
    "        test_y_actual_encoded, \n",
    "        test_y_pred_encoded, \n",
    "        target_names=le.classes_\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # ì „ì²´ ì •í™•ë„\n",
    "    accuracy = (test_y_pred == test_y_actual).mean()\n",
    "    print(f\"\\nğŸ¯ ì „ì²´ ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"í‰ê°€ ë°ì´í„°: {len(test_y_actual)}ê°œ ëª¨ë‘ í‰ê°€\")\n",
    "    \n",
    "else:\n",
    "    # ê³µí†µ í´ë˜ìŠ¤ë§Œ í•„í„°ë§í•´ì„œ í‰ê°€\n",
    "    mask = np.array([actual in common_classes for actual in test_y_actual])\n",
    "    filtered_actual = test_y_actual[mask]\n",
    "    filtered_pred = test_y_pred[mask]\n",
    "    \n",
    "    print(f\"ê³µí†µ í´ë˜ìŠ¤ í‰ê°€: {len(filtered_actual)}/{len(test_y_actual)}ê°œ\")\n",
    "    \n",
    "    filtered_actual_encoded = le.transform(filtered_actual)\n",
    "    filtered_pred_encoded = le.transform(filtered_pred)\n",
    "    \n",
    "    # ê³µí†µ í´ë˜ìŠ¤ì— ëŒ€í•œ ë¼ë²¨ê³¼ ì¸ë±ìŠ¤ ë§¤í•‘\n",
    "    common_class_labels = [cls for cls in le.classes_ if cls in common_classes]\n",
    "    common_class_indices = [le.transform([cls])[0] for cls in common_class_labels]\n",
    "    \n",
    "    report = classification_report(\n",
    "        filtered_actual_encoded,\n",
    "        filtered_pred_encoded,\n",
    "        target_names=common_class_labels,\n",
    "        labels=common_class_indices\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # í•„í„°ë§ëœ ë°ì´í„°ì˜ ì •í™•ë„\n",
    "    accuracy = (filtered_pred == filtered_actual).mean()\n",
    "    print(f\"\\nğŸ¯ ì •í™•ë„ (ê³µí†µ í´ë˜ìŠ¤ë§Œ): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"í‰ê°€ ë°ì´í„°: {len(filtered_actual)}/{len(test_y_actual)}ê°œ\")\n",
    "\n",
    "# 8. ì˜ˆì¸¡ ìƒ˜í”Œ ì¶œë ¥\n",
    "print(f\"\\nğŸ” ì˜ˆì¸¡ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for i in range(min(10, len(test_texts))):\n",
    "    text = test_texts[i][:60] + \"...\" if len(test_texts[i]) > 60 else test_texts[i]\n",
    "    actual = test_y_actual[i]\n",
    "    predicted = test_y_pred[i]\n",
    "    status = \"âœ…\" if actual == predicted else \"âŒ\"\n",
    "    \n",
    "    print(f\"{i+1:2d}. {status} ì‹¤ì œ: {actual:<12} ì˜ˆì¸¡: {predicted:<12}\")\n",
    "    print(f\"    í…ìŠ¤íŠ¸: {text}\")\n",
    "    print()\n",
    "\n",
    "# 9. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ìš”ì•½\n",
    "print(f\"\\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ìš”ì•½:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "from collections import defaultdict\n",
    "class_stats = defaultdict(lambda: {'total': 0, 'correct': 0})\n",
    "\n",
    "for actual, pred in zip(test_y_actual, test_y_pred):\n",
    "    if actual in common_classes:  # ê³µí†µ í´ë˜ìŠ¤ë§Œ ê³„ì‚°\n",
    "        class_stats[actual]['total'] += 1\n",
    "        if actual == pred:\n",
    "            class_stats[actual]['correct'] += 1\n",
    "\n",
    "print(f\"{'í´ë˜ìŠ¤':<15} {'ì „ì²´':<8} {'ì •ë‹µ':<8} {'ì •í™•ë„':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for class_name, stats in sorted(class_stats.items()):\n",
    "    if stats['total'] > 0:\n",
    "        class_accuracy = stats['correct'] / stats['total']\n",
    "        print(f\"{class_name:<15} {stats['total']:<8} {stats['correct']:<8} {class_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53gspj90vxl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¯ Category2 ëª¨ë¸ì˜ test_data ì„±ëŠ¥ í‰ê°€\n",
      "================================================================================\n",
      "Category2 í•™ìŠµìš© ë³€ìˆ˜ë“¤ì„ ì¬ì •ì˜í•©ë‹ˆë‹¤...\n",
      "ğŸ“Š Category2 ë°ì´í„° í™•ì¸:\n",
      "  - ì „ì²´ Category2 ë°ì´í„°: 3359ê°œ\n",
      "  - ê³ ìœ  Category2 í´ë˜ìŠ¤: 64ê°œ\n",
      "  - Category2 í´ë˜ìŠ¤ ëª©ë¡: ['ê°ˆë“±', 'ê°ë™', 'ê±±ì •', 'ê²½ë©¸', 'ê³ ë§ˆì›€', 'ê³ í†µ', 'ê³µê°', 'ê³µí¬', 'ê¶ê¸ˆí•¨', 'ê·€ì¤‘í•¨', 'ê·¸ë¦¬ì›€', 'ê¸°ëŒ€ê°', 'ë‚œì²˜í•¨', 'ë‚ ì¹´ë¡œì›€', 'ëƒ‰ë‹´', 'ë„ˆê·¸ëŸ¬ì›€', 'ë†€ëŒ', 'ë‹¤ì •í•¨', 'ë‹µë‹µí•¨', 'ë™ì •(ìŠ¬í””)', 'ë‘ê·¼ê±°ë¦¼', 'ë§Œì¡±ê°', 'ë§¤ë ¥ì ', 'ë¬´ê¸°ë ¥', 'ë¯¸ì•ˆí•¨', 'ë°˜ê°€ì›€', 'ë°˜ê°', 'ë°œì—´', 'ë¶€ë„ëŸ¬ì›€', 'ë¶ˆë§Œ', 'ë¶ˆì‹ ê°', 'ë¶ˆì¾Œ', 'ë¶ˆí¸í•¨', 'ë¹„ìœ„ìƒí•¨', 'ì‚¬ë‚˜ì›€', 'ìˆ˜ì¹˜ì‹¬', 'ì‹œê¸°ì‹¬', 'ì‹ ë¢°ê°', 'ì‹ ëª…ë‚¨', 'ì‹¤ë§', 'ì‹«ì¦', 'ì‹¬ì‹¬í•¨', 'ì•„ì‰¬ì›€', 'ì•„í””', 'ì•ˆì •ê°', 'ì–µìš¸í•¨', 'ì™¸ë¡œì›€', 'ì™¸ë©´', 'ìš•ì‹¬', 'ì›ë§', 'ìœ„ì¶•ê°', 'ìë‘ìŠ¤ëŸ¬ì›€', 'ìì‹ ê°', 'ì ˆë§', 'ì£„ì±…ê°', 'ì¦ê±°ì›€', 'ì´ˆì¡°í•¨', 'ì¹˜ì‚¬í•¨', 'íƒ€ì˜¤ë¦„', 'í†µì¾Œí•¨', 'í¸ì•ˆí•¨', 'í—ˆë§', 'í˜¸ê°', 'í›„íšŒ']\n",
      "âœ… Category2ì—ì„œ ì¤‘ë¦½ì´ ì„±ê³µì ìœ¼ë¡œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "  - ì¸ì½”ë”©ëœ Category2 í´ë˜ìŠ¤: 64ê°œ\n",
      "X shape: (3359, 1024)\n",
      "y shape: (3359,)\n",
      "y_cat1_onehot shape: (3359, 10)\n",
      "X_combined shape: (3359, 1034)\n",
      "y_cat2 shape: (3359,)\n",
      "y_cat2_encoded shape: (3359,)\n",
      "\n",
      "ğŸ“ Category2 ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„...\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„°:\n",
      "- Category1 ì‹¤ì œê°’: 664ê°œ\n",
      "- Category2 ì‹¤ì œê°’: 664ê°œ\n",
      "- í…ŒìŠ¤íŠ¸ Category2 í´ë˜ìŠ¤: 64ê°œ\n",
      "- í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤‘ë¦½: Category1=5ê°œ, Category2=0ê°œ\n",
      "\n",
      "ğŸ”§ Category1 ì˜ˆì¸¡ê°’ìœ¼ë¡œ Category2 ì˜ˆì¸¡ìš© íŠ¹ì„± ìƒì„±...\n",
      "test_X shape: (664, 1024)\n",
      "test_cat1_onehot shape: (664, 10)\n",
      "âœ… ê²°í•©ëœ íŠ¹ì„±: (664, 1034)\n",
      "\n",
      "ğŸ”„ ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ Category2 ìµœì¢… ëª¨ë¸ í•™ìŠµ...\n",
      "í•™ìŠµ ë°ì´í„° í™•ì¸: X_combined (3359, 1034), y_cat2_encoded (3359,)\n",
      "âœ… Category2 ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n",
      "\n",
      "ğŸ¯ test_data Category2 ì˜ˆì¸¡ ìˆ˜í–‰...\n",
      "\n",
      "ğŸ“‹ Category2 í´ë˜ìŠ¤ ì •ë³´:\n",
      "í•™ìŠµ í´ë˜ìŠ¤ ìˆ˜: 64\n",
      "í…ŒìŠ¤íŠ¸ ì‹¤ì œ í´ë˜ìŠ¤ ìˆ˜: 64\n",
      "ê³µí†µ í´ë˜ìŠ¤ ìˆ˜: 64\n",
      "âœ… í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ Category2 í´ë˜ìŠ¤ê°€ ì™„ë²½íˆ ì¼ì¹˜í•©ë‹ˆë‹¤!\n",
      "\n",
      "ğŸ“Š Category2 Classification Report:\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ê°ˆë“±       0.00      0.00      0.00         1\n",
      "          ê°ë™       0.29      0.40      0.34        30\n",
      "          ê±±ì •       0.43      0.13      0.20        23\n",
      "          ê²½ë©¸       0.20      0.44      0.27        18\n",
      "         ê³ ë§ˆì›€       0.46      0.63      0.53        19\n",
      "          ê³ í†µ       0.00      0.00      0.00         9\n",
      "          ê³µê°       0.00      0.00      0.00         7\n",
      "          ê³µí¬       0.00      0.00      0.00        17\n",
      "         ê¶ê¸ˆí•¨       0.12      0.56      0.20         9\n",
      "         ê·€ì¤‘í•¨       0.00      0.00      0.00         5\n",
      "         ê·¸ë¦¬ì›€       0.50      0.09      0.15        11\n",
      "         ê¸°ëŒ€ê°       0.50      0.39      0.44        18\n",
      "         ë‚œì²˜í•¨       0.25      0.07      0.11        14\n",
      "        ë‚ ì¹´ë¡œì›€       0.00      0.00      0.00         3\n",
      "          ëƒ‰ë‹´       0.00      0.00      0.00         3\n",
      "        ë„ˆê·¸ëŸ¬ì›€       0.00      0.00      0.00         1\n",
      "          ë†€ëŒ       0.33      0.19      0.24        31\n",
      "         ë‹¤ì •í•¨       0.00      0.00      0.00         9\n",
      "         ë‹µë‹µí•¨       0.22      0.11      0.15        18\n",
      "      ë™ì •(ìŠ¬í””)       0.57      0.48      0.52        27\n",
      "        ë‘ê·¼ê±°ë¦¼       0.00      0.00      0.00         2\n",
      "         ë§Œì¡±ê°       0.35      0.35      0.35        49\n",
      "         ë§¤ë ¥ì        1.00      0.08      0.14        13\n",
      "         ë¬´ê¸°ë ¥       0.25      0.36      0.29        14\n",
      "         ë¯¸ì•ˆí•¨       1.00      0.29      0.44         7\n",
      "         ë°˜ê°€ì›€       0.20      0.06      0.10        16\n",
      "          ë°˜ê°       0.20      0.20      0.20         5\n",
      "          ë°œì—´       0.00      0.00      0.00         3\n",
      "        ë¶€ë„ëŸ¬ì›€       0.00      0.00      0.00        16\n",
      "          ë¶ˆë§Œ       0.20      0.12      0.15         8\n",
      "         ë¶ˆì‹ ê°       0.20      0.12      0.15         8\n",
      "          ë¶ˆì¾Œ       0.11      0.15      0.13        20\n",
      "         ë¶ˆí¸í•¨       0.40      0.29      0.33         7\n",
      "        ë¹„ìœ„ìƒí•¨       0.00      0.00      0.00         1\n",
      "         ì‚¬ë‚˜ì›€       0.20      0.25      0.22         4\n",
      "         ìˆ˜ì¹˜ì‹¬       0.00      0.00      0.00         3\n",
      "         ì‹œê¸°ì‹¬       0.20      0.50      0.29         2\n",
      "         ì‹ ë¢°ê°       0.00      0.00      0.00         1\n",
      "         ì‹ ëª…ë‚¨       1.00      0.20      0.33         5\n",
      "          ì‹¤ë§       0.11      0.31      0.16        13\n",
      "          ì‹«ì¦       0.00      0.00      0.00         9\n",
      "         ì‹¬ì‹¬í•¨       0.00      0.00      0.00         1\n",
      "         ì•„ì‰¬ì›€       0.43      0.30      0.36        33\n",
      "          ì•„í””       0.00      0.00      0.00         3\n",
      "         ì•ˆì •ê°       0.20      0.12      0.15         8\n",
      "         ì–µìš¸í•¨       0.18      0.46      0.26        13\n",
      "         ì™¸ë¡œì›€       0.50      0.17      0.25         6\n",
      "          ì™¸ë©´       0.00      0.00      0.00         1\n",
      "          ìš•ì‹¬       0.21      0.44      0.29        18\n",
      "          ì›ë§       0.00      0.00      0.00         3\n",
      "         ìœ„ì¶•ê°       0.00      0.00      0.00         4\n",
      "       ìë‘ìŠ¤ëŸ¬ì›€       0.16      0.42      0.23        12\n",
      "         ìì‹ ê°       0.00      0.00      0.00         1\n",
      "          ì ˆë§       0.00      0.00      0.00         7\n",
      "         ì£„ì±…ê°       0.00      0.00      0.00         2\n",
      "         ì¦ê±°ì›€       0.40      0.44      0.42        27\n",
      "         ì´ˆì¡°í•¨       0.00      0.00      0.00         5\n",
      "         ì¹˜ì‚¬í•¨       0.00      0.00      0.00         5\n",
      "         íƒ€ì˜¤ë¦„       0.00      0.00      0.00         3\n",
      "         í†µì¾Œí•¨       0.00      0.00      0.00         1\n",
      "         í¸ì•ˆí•¨       0.50      0.25      0.33         4\n",
      "          í—ˆë§       0.30      0.27      0.29        11\n",
      "          í˜¸ê°       0.33      0.10      0.15        10\n",
      "          í›„íšŒ       0.33      0.29      0.31         7\n",
      "\n",
      "    accuracy                           0.24       664\n",
      "   macro avg       0.20      0.16      0.15       664\n",
      "weighted avg       0.28      0.24      0.23       664\n",
      "\n",
      "\n",
      "ğŸ¯ Category2 ì „ì²´ ì •í™•ë„: 0.2425 (24.25%)\n",
      "í‰ê°€ ë°ì´í„°: 664ê°œ ëª¨ë‘ í‰ê°€\n",
      "\n",
      "ğŸ” Category2 ì˜ˆì¸¡ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):\n",
      "----------------------------------------------------------------------------------------------------\n",
      " 1. âŒ Cat1: ê¸°ì¨ â†’ ê¸°ì¨ | Cat2: ë§Œì¡±ê°          â†’ ì¦ê±°ì›€         \n",
      "    í…ìŠ¤íŠ¸: ë³´ëŠ”ë™ì•ˆ ë„ˆë¬´ í–‰ë³µí–ˆê³  ì´ˆì½œë ›ì´ ë„ˆë¬´ ë¨¹ê³ ì‹¶ì—ˆê³  í‹°ëª¨ì‹œê°€ ì˜ìƒê²¼ê³  ìš¸ì–´!!í•˜ëŠ”ë¶€ë¶„ì´ ìˆì–´ì„œ...\n",
      "\n",
      " 2. âŒ Cat1: ê¸°ì¨ â†’ ê¸°ì¨ | Cat2: ë§Œì¡±ê°          â†’ ê¸°ëŒ€ê°         \n",
      "    í…ìŠ¤íŠ¸: ì–´ë¦´ ë•Œ ê°€ ë³´ê³  ë¹•ìŠ¤ëŠ” ê±°ì˜ ì²˜ìŒì¸ë°(ê¸°ì–µì— ì—†ìŒ) ì§€ê¸ˆ ë”¸ê¸°ì¶•ì œ ê¸°ê°„ì´ë¼ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì‹...\n",
      "\n",
      " 3. âœ… Cat1: ê¸°ì¨ â†’ ê¸°ì¨ | Cat2: ë§Œì¡±ê°          â†’ ë§Œì¡±ê°         \n",
      "    í…ìŠ¤íŠ¸: ë¯¸ë¦¬ ê³„ì¢Œë¡œ í™˜ì „í•´ë‘” ëˆì„ í•´ì™¸ì—ì„œ í™˜ì „ìˆ˜ìˆ˜ë£Œ ì—†ì´ ì¸ì¶œ ê°€ëŠ¥í•œ íŠ¸ë ˆë¸”ë¡œê·¸ë¼ëŠ” ì¹´ë“œì¸ë°, ...\n",
      "\n",
      " 4. âŒ Cat1: ìŠ¬í”” â†’ ì‹«ì–´í•¨(ìƒíƒœ) | Cat2: ë¬´ê¸°ë ¥          â†’ ë‚œì²˜í•¨         \n",
      "    í…ìŠ¤íŠ¸: ìš”ì¦˜ ë²ˆì•„ì›ƒë„ ìê¾¸ ì˜¬ë¼ì˜¤ê³  ë¬´ê¸°ë ¥í•´ì„œ ì¢…ê°•í•˜ê³  êµë¥˜í•˜ê¸°ë„ ë²„ê±°ìš´ ìƒíƒœê°€ ì™€ë¶€ë €ìœ¼ìš”ã… ã…  \n",
      "\n",
      " 5. âŒ Cat1: ê¸°ì¨ â†’ ë¯¸ì›€(ìƒëŒ€ë°©) | Cat2: ì¦ê±°ì›€          â†’ ë¹„ìœ„ìƒí•¨        \n",
      "    í…ìŠ¤íŠ¸: í¬ë¼ì„ì”¬ ì¥ë˜¥ë¯¼ì´ ë²”í–‰ ë„êµ¬ ì°¾ìœ¼ë ¤ê³  í™”ì¥ì‹¤ íƒ±í¬ ë’¤ì§€ëŠ”ë° ê±°ê¸°ì— ì§„ì§œ ë˜¥ ë„£ì–´ë†“ì€ ê±° ì§„...\n",
      "\n",
      " 6. âŒ Cat1: ì‹«ì–´í•¨(ìƒíƒœ) â†’ ìŠ¬í”” | Cat2: ë‹µë‹µí•¨          â†’ ë¬´ê¸°ë ¥         \n",
      "    í…ìŠ¤íŠ¸: ê°€ìŠ´ì´ ë‹µë‹µí•´ì§ ì§„ì§œ ê°œë‹µë‹µí•´ì§\n",
      "ìš°ë¦¬ì§„ì§œíˆ¬í‘œì˜í•˜ì\n",
      "\n",
      " 7. âŒ Cat1: ê¸°ì¨ â†’ ìš•ë§ | Cat2: ë§Œì¡±ê°          â†’ ìš•ì‹¬          \n",
      "    í…ìŠ¤íŠ¸: ì§€ê·¸ì¬ê·¸ë‘ ì—ì´ë¸”ë¦¬ë‘ í• ì¸ ëŒ€ê²°í•˜ë‚˜\n",
      "ì•„ íë­‡í•´\n",
      "ê³„ì†ë˜ê¸¸...\n",
      "ì˜ì›íˆ....\n",
      "\n",
      " 8. âŒ Cat1: ê¸°ì¨ â†’ ìš•ë§ | Cat2: ìë‘ìŠ¤ëŸ¬ì›€        â†’ ìš•ì‹¬          \n",
      "    í…ìŠ¤íŠ¸: ì²¨ìœ¼ë¡œ ìˆ˜ì œ ì´ˆì½œë¦¿ ë§Œë“¬\n",
      "ì´ˆì½œë¦¿ì„ 5ì‹œê°„ì´ë‚˜ ë§Œë“œëŠ” ì‚¬ëŒì´ ìˆë‹¤??? ê·¸ê²Œ ë°”ë¡œ ë‚˜\n",
      "\n",
      " 9. âŒ Cat1: ìŠ¬í”” â†’ ê¸°ì¨ | Cat2: ì ˆë§           â†’ ê¸°ëŒ€ê°         \n",
      "    í…ìŠ¤íŠ¸: ë‹¨í†¡ë°©ì— ê³µì§€ë“¤ ìŠ¬ìŠ¬ ì˜¬ë¼ì˜¤ëŠ”ê±° ë³´ë‹ˆê¹Œ ê³§ ê°œê°•ì´ë¼ëŠ”ê²Œ ì‹¤ê°ë‚˜ì„œ ê°‘ìê¸° ì¬ê¸°í•˜ê³ ì‹¶ê³  ì¸ìƒì´...\n",
      "\n",
      "10. âŒ Cat1: ê¸°ì¨ â†’ ê¸°ì¨ | Cat2: ì¦ê±°ì›€          â†’ ê°ë™          \n",
      "    í…ìŠ¤íŠ¸: ì¥í¥ì‹  ê³µì†í•œ ì†ê°€ë½ì§ˆ ê°œì›ƒê²¨ìš”\n",
      "ì• ë“œë¦½ ë¯¸ì¹œê²ƒ ê°™ìŒ\n",
      "\n",
      "\n",
      "ğŸ“ˆ Category2 í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ìš”ì•½:\n",
      "------------------------------------------------------------\n",
      "í´ë˜ìŠ¤             ì „ì²´       ì •ë‹µ       ì •í™•ë„       \n",
      "--------------------------------------------------\n",
      "ê°ˆë“±              1        0        0.0000\n",
      "ê°ë™              30       12       0.4000\n",
      "ê±±ì •              23       3        0.1304\n",
      "ê²½ë©¸              18       8        0.4444\n",
      "ê³ ë§ˆì›€             19       12       0.6316\n",
      "ê³ í†µ              9        0        0.0000\n",
      "ê³µê°              7        0        0.0000\n",
      "ê³µí¬              17       0        0.0000\n",
      "ê¶ê¸ˆí•¨             9        5        0.5556\n",
      "ê·€ì¤‘í•¨             5        0        0.0000\n",
      "ê·¸ë¦¬ì›€             11       1        0.0909\n",
      "ê¸°ëŒ€ê°             18       7        0.3889\n",
      "ë‚œì²˜í•¨             14       1        0.0714\n",
      "ë‚ ì¹´ë¡œì›€            3        0        0.0000\n",
      "ëƒ‰ë‹´              3        0        0.0000\n",
      "ë„ˆê·¸ëŸ¬ì›€            1        0        0.0000\n",
      "ë†€ëŒ              31       6        0.1935\n",
      "ë‹¤ì •í•¨             9        0        0.0000\n",
      "ë‹µë‹µí•¨             18       2        0.1111\n",
      "ë™ì •(ìŠ¬í””)          27       13       0.4815\n",
      "ë‘ê·¼ê±°ë¦¼            2        0        0.0000\n",
      "ë§Œì¡±ê°             49       17       0.3469\n",
      "ë§¤ë ¥ì              13       1        0.0769\n",
      "ë¬´ê¸°ë ¥             14       5        0.3571\n",
      "ë¯¸ì•ˆí•¨             7        2        0.2857\n",
      "ë°˜ê°€ì›€             16       1        0.0625\n",
      "ë°˜ê°              5        1        0.2000\n",
      "ë°œì—´              3        0        0.0000\n",
      "ë¶€ë„ëŸ¬ì›€            16       0        0.0000\n",
      "ë¶ˆë§Œ              8        1        0.1250\n",
      "ë¶ˆì‹ ê°             8        1        0.1250\n",
      "ë¶ˆì¾Œ              20       3        0.1500\n",
      "ë¶ˆí¸í•¨             7        2        0.2857\n",
      "ë¹„ìœ„ìƒí•¨            1        0        0.0000\n",
      "ì‚¬ë‚˜ì›€             4        1        0.2500\n",
      "ìˆ˜ì¹˜ì‹¬             3        0        0.0000\n",
      "ì‹œê¸°ì‹¬             2        1        0.5000\n",
      "ì‹ ë¢°ê°             1        0        0.0000\n",
      "ì‹ ëª…ë‚¨             5        1        0.2000\n",
      "ì‹¤ë§              13       4        0.3077\n",
      "ì‹«ì¦              9        0        0.0000\n",
      "ì‹¬ì‹¬í•¨             1        0        0.0000\n",
      "ì•„ì‰¬ì›€             33       10       0.3030\n",
      "ì•„í””              3        0        0.0000\n",
      "ì•ˆì •ê°             8        1        0.1250\n",
      "ì–µìš¸í•¨             13       6        0.4615\n",
      "ì™¸ë¡œì›€             6        1        0.1667\n",
      "ì™¸ë©´              1        0        0.0000\n",
      "ìš•ì‹¬              18       8        0.4444\n",
      "ì›ë§              3        0        0.0000\n",
      "ìœ„ì¶•ê°             4        0        0.0000\n",
      "ìë‘ìŠ¤ëŸ¬ì›€           12       5        0.4167\n",
      "ìì‹ ê°             1        0        0.0000\n",
      "ì ˆë§              7        0        0.0000\n",
      "ì£„ì±…ê°             2        0        0.0000\n",
      "ì¦ê±°ì›€             27       12       0.4444\n",
      "ì´ˆì¡°í•¨             5        0        0.0000\n",
      "ì¹˜ì‚¬í•¨             5        0        0.0000\n",
      "íƒ€ì˜¤ë¦„             3        0        0.0000\n",
      "í†µì¾Œí•¨             1        0        0.0000\n",
      "í¸ì•ˆí•¨             4        1        0.2500\n",
      "í—ˆë§              11       3        0.2727\n",
      "í˜¸ê°              10       1        0.1000\n",
      "í›„íšŒ              7        2        0.2857\n",
      "\n",
      "ğŸ“Š ìµœì¢… ì„±ëŠ¥ ë¹„êµ:\n",
      "============================================================\n",
      "Category1 ì •í™•ë„: 0.4880 (48.80%)\n",
      "Category2 ì •í™•ë„: 0.2425 (24.25%)\n",
      "âœ… Category1 ë¶„ë¥˜ê°€ ë” ì •í™•í•©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\skn_after_study\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\user\\anaconda3\\envs\\skn_after_study\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\user\\anaconda3\\envs\\skn_after_study\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Category2 ëª¨ë¸ë¡œ test_data ì˜ˆì¸¡ ë° classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ Category2 ëª¨ë¸ì˜ test_data ì„±ëŠ¥ í‰ê°€\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. í•„ìš”í•œ ë³€ìˆ˜ë“¤ ì •ì˜ (í•„ìš”ì‹œ)\n",
    "if 'X_combined' not in locals():\n",
    "    print(\"Category2 í•™ìŠµìš© ë³€ìˆ˜ë“¤ì„ ì¬ì •ì˜í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    # OneHotEncoder for Category1\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    cat1_encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_cat1_onehot = cat1_encoder.fit_transform(y.reshape(-1, 1))\n",
    "    \n",
    "    # Combined features for Category2\n",
    "    X_combined = np.hstack([X, y_cat1_onehot])\n",
    "    \n",
    "    # Category2 encoder - ë³€ê²½: category2 â†’ re_category2 (ì¤‘ë¦½ ì œê±°ëœ ë°ì´í„° ì‚¬ìš©)\n",
    "    y_cat2 = data['re_category2'].values\n",
    "    print(f\"ğŸ“Š Category2 ë°ì´í„° í™•ì¸:\")\n",
    "    print(f\"  - ì „ì²´ Category2 ë°ì´í„°: {len(y_cat2)}ê°œ\")\n",
    "    print(f\"  - ê³ ìœ  Category2 í´ë˜ìŠ¤: {len(np.unique(y_cat2))}ê°œ\")\n",
    "    print(f\"  - Category2 í´ë˜ìŠ¤ ëª©ë¡: {sorted(np.unique(y_cat2))}\")\n",
    "    \n",
    "    # ì¤‘ë¦½ ì œê±° í™•ì¸\n",
    "    neutral_count = (y_cat2 == 'ì¤‘ë¦½').sum()\n",
    "    if neutral_count > 0:\n",
    "        print(f\"âš ï¸ ê²½ê³ : Category2ì— ì—¬ì „íˆ ì¤‘ë¦½ì´ {neutral_count}ê°œ ìˆìŠµë‹ˆë‹¤!\")\n",
    "    else:\n",
    "        print(\"âœ… Category2ì—ì„œ ì¤‘ë¦½ì´ ì„±ê³µì ìœ¼ë¡œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    le_cat2 = LabelEncoder()\n",
    "    y_cat2_encoded = le_cat2.fit_transform(y_cat2)\n",
    "    \n",
    "    print(f\"  - ì¸ì½”ë”©ëœ Category2 í´ë˜ìŠ¤: {len(le_cat2.classes_)}ê°œ\")\n",
    "    \n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"y_cat1_onehot shape: {y_cat1_onehot.shape}\")\n",
    "    print(f\"X_combined shape: {X_combined.shape}\")\n",
    "    print(f\"y_cat2 shape: {y_cat2.shape}\")\n",
    "    print(f\"y_cat2_encoded shape: {y_cat2_encoded.shape}\")\n",
    "    \n",
    "    # í¬ê¸° í™•ì¸ ë° ìˆ˜ì •\n",
    "    if X_combined.shape[0] != y_cat2_encoded.shape[0]:\n",
    "        print(f\"âš ï¸ í¬ê¸° ë¶ˆì¼ì¹˜ ê°ì§€: X_combined {X_combined.shape[0]} vs y_cat2_encoded {y_cat2_encoded.shape[0]}\")\n",
    "        min_size = min(X_combined.shape[0], y_cat2_encoded.shape[0])\n",
    "        X_combined = X_combined[:min_size]\n",
    "        y_cat2_encoded = y_cat2_encoded[:min_size]\n",
    "        print(f\"âœ… í¬ê¸° ì¡°ì • ì™„ë£Œ: {X_combined.shape[0]} rows\")\n",
    "\n",
    "# 2. Category1ì„ ë¨¼ì € ì˜ˆì¸¡í•´ì•¼ Category2ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŒ\n",
    "print(\"\\nğŸ“ Category2 ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„...\")\n",
    "\n",
    "# test_dataì˜ ì‹¤ì œ category1ê³¼ category2\n",
    "test_y_actual_cat1 = test_data['category1'].values\n",
    "test_y_actual_cat2 = test_data['category2'].values\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°:\")\n",
    "print(f\"- Category1 ì‹¤ì œê°’: {len(test_y_actual_cat1)}ê°œ\")\n",
    "print(f\"- Category2 ì‹¤ì œê°’: {len(test_y_actual_cat2)}ê°œ\")\n",
    "print(f\"- í…ŒìŠ¤íŠ¸ Category2 í´ë˜ìŠ¤: {len(np.unique(test_y_actual_cat2))}ê°œ\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì¤‘ë¦½ í™•ì¸ ë° í•„í„°ë§ ì •ë³´\n",
    "test_cat1_neutral_count = (test_y_actual_cat1 == 'ì¤‘ë¦½').sum()\n",
    "test_cat2_neutral_count = (test_y_actual_cat2 == 'ì¤‘ë¦½').sum()\n",
    "print(f\"- í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤‘ë¦½: Category1={test_cat1_neutral_count}ê°œ, Category2={test_cat2_neutral_count}ê°œ\")\n",
    "\n",
    "# Category1 ì˜ˆì¸¡ê°’ì„ ì‚¬ìš©í•˜ì—¬ Category2 ì˜ˆì¸¡ìš© íŠ¹ì„± ìƒì„±\n",
    "print(\"\\nğŸ”§ Category1 ì˜ˆì¸¡ê°’ìœ¼ë¡œ Category2 ì˜ˆì¸¡ìš© íŠ¹ì„± ìƒì„±...\")\n",
    "\n",
    "# Category1 ì˜ˆì¸¡ê°’ì„ ì›í•«ì¸ì½”ë”©\n",
    "test_cat1_onehot = cat1_encoder.transform(test_y_pred.reshape(-1, 1))\n",
    "test_X_combined = np.hstack([test_X, test_cat1_onehot])\n",
    "\n",
    "print(f\"test_X shape: {test_X.shape}\")\n",
    "print(f\"test_cat1_onehot shape: {test_cat1_onehot.shape}\")\n",
    "print(f\"âœ… ê²°í•©ëœ íŠ¹ì„±: {test_X_combined.shape}\")\n",
    "\n",
    "# 3. ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ Category2 ìµœì¢… ëª¨ë¸ í•™ìŠµ\n",
    "print(\"\\nğŸ”„ ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ Category2 ìµœì¢… ëª¨ë¸ í•™ìŠµ...\")\n",
    "print(f\"í•™ìŠµ ë°ì´í„° í™•ì¸: X_combined {X_combined.shape}, y_cat2_encoded {y_cat2_encoded.shape}\")\n",
    "\n",
    "final_cat2_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ Category2 ëª¨ë¸ í•™ìŠµ\n",
    "final_cat2_model.fit(X_combined, y_cat2_encoded)\n",
    "print(\"âœ… Category2 ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# 4. test_dataë¡œ Category2 ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "print(\"\\nğŸ¯ test_data Category2 ì˜ˆì¸¡ ìˆ˜í–‰...\")\n",
    "test_y_pred_cat2_encoded = final_cat2_model.predict(test_X_combined)\n",
    "test_y_pred_cat2 = le_cat2.inverse_transform(test_y_pred_cat2_encoded)\n",
    "\n",
    "# 5. í•™ìŠµ í´ë˜ìŠ¤ì™€ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¹„êµ (Category2)\n",
    "train_classes_cat2 = set(le_cat2.classes_)\n",
    "test_actual_classes_cat2 = set(test_y_actual_cat2)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Category2 í´ë˜ìŠ¤ ì •ë³´:\")\n",
    "print(f\"í•™ìŠµ í´ë˜ìŠ¤ ìˆ˜: {len(train_classes_cat2)}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ì‹¤ì œ í´ë˜ìŠ¤ ìˆ˜: {len(test_actual_classes_cat2)}\")\n",
    "\n",
    "# í•™ìŠµì— ì—†ëŠ” í´ë˜ìŠ¤ í™•ì¸\n",
    "unseen_classes_cat2 = test_actual_classes_cat2 - train_classes_cat2\n",
    "if unseen_classes_cat2:\n",
    "    print(f\"âš ï¸ í•™ìŠµì— ì—†ë˜ Category2 í´ë˜ìŠ¤ë“¤: {unseen_classes_cat2}\")\n",
    "\n",
    "common_classes_cat2 = train_classes_cat2 & test_actual_classes_cat2\n",
    "print(f\"ê³µí†µ í´ë˜ìŠ¤ ìˆ˜: {len(common_classes_cat2)}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ë§¤ì¹˜ í™•ì¸\n",
    "if len(train_classes_cat2) == len(test_actual_classes_cat2) == len(common_classes_cat2):\n",
    "    print(\"âœ… í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ Category2 í´ë˜ìŠ¤ê°€ ì™„ë²½íˆ ì¼ì¹˜í•©ë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"âŒ í´ë˜ìŠ¤ ë¶ˆì¼ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 6. Category2 Classification Report ìƒì„±\n",
    "print(f\"\\nğŸ“Š Category2 Classification Report:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ëª¨ë“  í´ë˜ìŠ¤ê°€ ì¼ì¹˜í•˜ë¯€ë¡œ ì „ì²´ í‰ê°€ ê°€ëŠ¥\n",
    "test_y_actual_cat2_encoded = le_cat2.transform(test_y_actual_cat2)\n",
    "report = classification_report(\n",
    "    test_y_actual_cat2_encoded,\n",
    "    test_y_pred_cat2_encoded,\n",
    "    target_names=le_cat2.classes_\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# ì „ì²´ ì •í™•ë„\n",
    "accuracy_cat2 = (test_y_pred_cat2 == test_y_actual_cat2).mean()\n",
    "print(f\"\\nğŸ¯ Category2 ì „ì²´ ì •í™•ë„: {accuracy_cat2:.4f} ({accuracy_cat2*100:.2f}%)\")\n",
    "print(f\"í‰ê°€ ë°ì´í„°: {len(test_y_actual_cat2)}ê°œ ëª¨ë‘ í‰ê°€\")\n",
    "\n",
    "# 7. Category2 ì˜ˆì¸¡ ìƒ˜í”Œ ì¶œë ¥\n",
    "print(f\"\\nğŸ” Category2 ì˜ˆì¸¡ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i in range(min(10, len(test_texts))):\n",
    "    text = test_texts[i][:50] + \"...\" if len(test_texts[i]) > 50 else test_texts[i]\n",
    "    actual_cat1 = test_y_actual_cat1[i]\n",
    "    pred_cat1 = test_y_pred[i]\n",
    "    actual_cat2 = test_y_actual_cat2[i]\n",
    "    pred_cat2 = test_y_pred_cat2[i]\n",
    "    status_cat2 = \"âœ…\" if actual_cat2 == pred_cat2 else \"âŒ\"\n",
    "    \n",
    "    print(f\"{i+1:2d}. {status_cat2} Cat1: {actual_cat1} â†’ {pred_cat1} | Cat2: {actual_cat2:<12} â†’ {pred_cat2:<12}\")\n",
    "    print(f\"    í…ìŠ¤íŠ¸: {text}\")\n",
    "    print()\n",
    "\n",
    "# 8. Category2 í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ìš”ì•½\n",
    "print(f\"\\nğŸ“ˆ Category2 í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ìš”ì•½:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "class_stats_cat2 = defaultdict(lambda: {'total': 0, 'correct': 0})\n",
    "\n",
    "for actual, pred in zip(test_y_actual_cat2, test_y_pred_cat2):\n",
    "    class_stats_cat2[actual]['total'] += 1\n",
    "    if actual == pred:\n",
    "        class_stats_cat2[actual]['correct'] += 1\n",
    "\n",
    "print(f\"{'í´ë˜ìŠ¤':<15} {'ì „ì²´':<8} {'ì •ë‹µ':<8} {'ì •í™•ë„':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for class_name, stats in sorted(class_stats_cat2.items()):\n",
    "    if stats['total'] > 0:\n",
    "        class_accuracy = stats['correct'] / stats['total']\n",
    "        print(f\"{class_name:<15} {stats['total']:<8} {stats['correct']:<8} {class_accuracy:.4f}\")\n",
    "\n",
    "# 9. Category1 vs Category2 ì„±ëŠ¥ ë¹„êµ\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ì„±ëŠ¥ ë¹„êµ:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Category1 ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Category2 ì •í™•ë„: {accuracy_cat2:.4f} ({accuracy_cat2*100:.2f}%)\")\n",
    "\n",
    "if accuracy > accuracy_cat2:\n",
    "    print(\"âœ… Category1 ë¶„ë¥˜ê°€ ë” ì •í™•í•©ë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âœ… Category2 ë¶„ë¥˜ê°€ ë” ì •í™•í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "404da083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì–˜ë“¤ì•„ ë”¸ê¸°ì¶•ì œì¸ê°€ ë”¸ê¸°ì¥ë¡€ì‹ì¸ê°€ëŠ” ë­ ê°€ì§€ë§ˆë¼\n",
      "ì–´ìš° ë³„ë¡œë¥¼ ë„˜ì–´ì„œ ì•„ê¹ë‹¤ ê·¸ëƒ¥\n",
      "í•¸ë“œí° ë°”ê¿¨ëŠ”ë°.. ì–´ì°Œë‚˜ ì „ë‡Œì´ì‹ì´ ì˜ë˜ëŠ”ì§€ ì´ì „ í°ì—ì„œ ë“£ë˜ ìŒì•… ë©ˆì¶°ë‘” ë¶€ë¶„ê¹Œì§€ ì‚´ë ¤ë†”ì„œ ìƒˆë¡œ ì‚° ê¸°ë¶„ì´ ì „í˜€ ì•ˆë‚¨..\n",
      "ì¹œêµ¬ê°€ ë§Œë“¤ì–´ì¤¬ì–´ ì–´ì´ì—†ì–´ì„œ ë°›ìë§ˆì ì˜¤ì—´í•¨\n",
      "ì•„ë‹ˆ ë§ê¸´í•œë°\n",
      "ì†”ì§íˆ ì£¼ë§ ê»´ì„œ 4ì¼â€¦ì—°íœ´ë¼ê¸°ì—” ë„ˆë¬´ ëˆˆì†ì„ì„â€¦ì‚¬ì‹¤ìƒ ì´í‹€ ì‰° ê±°ì–ì•„\n",
      "ì†Œì¸ í¸ì˜ì ì—ì„œ ë§¤ì¼ìš°ìœ  í¬ë¦¼ë¹µì„ ë§Œì› ì–´ì¹˜ ì‚¬ë ¤ê³  ê°”ëŠ”ë° 3ê°œë¥¼ ì‚¬ê¸°ì—ëŠ” ë¶€ì¡±í•œ ëˆì´ë¼ ìŠ¬íì†Œì´ë‹¤. ë¬¼ê°€ê°€ ë„ˆë¬´ ë¹„ì‹¼ê²ƒ ê°™ì†Œ.\n",
      "ì—°íœ´ ì´í‹€ì´ ë‹¤ ì£¼ë§ì— ê²¹ì³ì ¸ ìˆì—ˆëŠ”ë° ì™œ ëŒ€ì²´ ê³µíœ´ì¼ì€ í•˜ë£¨ë§Œ ì£¼ëŠ”ê±°ì•¼. ë¶€ì¡±í•´, ì£¼ë§ ìƒê´€ì—†ì´ ì„¤ ì—°íœ´ 3ì¼ ë‹¤ ì™„ë²½í•˜ê²Œ ë³´ì¥í•´ ì¤˜.\n",
      "ì´ë ‡ê²Œ ìê·¹ì ì¸ ë“œë¼ë§ˆ ì•„ì´ë“¤ë„ ë‹¤ ì ‘í• í…ë° ì´ì   ìˆ˜ìœ„ì¡°ì ˆë„ ì•ˆí•˜ê³  ë§‰ì°ëŠ”ê±°ê°™ì•„ì„œ ì¢€ ì•ˆíƒ€ê¹Œì›€ì´ ìƒê¸°ë„¤ìš”â€¦ ã… ã… \n",
      "ì´ì²œ í–…ì‚´ ì»¤í”¼í”„ë¼í”„ì¹˜ë…¸~! ì™ ì§€ ê³ ì†Œí•œ ì»¤í”¼ë§›ì¼êº¼ë€ ê¸°ëŒ€ë¥¼ ì—„ì²­ì•ˆê³  ì£¼ë¬¸í–ˆëŠ”ë°.. ìê·¸ë§ˆì¹˜ 6300ì›ã…œã…œ ë¹„ì‹¼ ê¸ˆì•¡ì¸ë° ë§Œì¡±ìŠ¤ëŸ½ì§„ ëª»í–ˆë‹¤ëˆˆã…œã…œ\n"
     ]
    }
   ],
   "source": [
    "for context in test_data[test_data['category2']=='ë¶ˆë§Œ']['context']:\n",
    "  print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "448dfaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ê°ì • ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”...\n",
      "âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# 10. í†µí•© ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ êµ¬í˜„\n",
    "\n",
    "class EmotionClassificationPipeline:\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ì…ë ¥ â†’ Category1 ì˜ˆì¸¡ â†’ Category2 ì˜ˆì¸¡ â†’ ì¢…í•© í‰ê°€ íŒŒì´í”„ë¼ì¸\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_model, cat1_model, cat2_model, \n",
    "                 cat1_encoder, cat2_encoder, cat1_onehot_encoder):\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.cat1_model = cat1_model\n",
    "        self.cat2_model = cat2_model\n",
    "        self.cat1_encoder = cat1_encoder\n",
    "        self.cat2_encoder = cat2_encoder\n",
    "        self.cat1_onehot_encoder = cat1_onehot_encoder\n",
    "        \n",
    "    def predict_single(self, text):\n",
    "        \"\"\"\n",
    "        ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì¹´í…Œê³ ë¦¬1ê³¼ ì¹´í…Œê³ ë¦¬2ë¥¼ ì˜ˆì¸¡\n",
    "        \n",
    "        Args:\n",
    "            text (str): ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸\n",
    "        \n",
    "        Returns:\n",
    "            dict: ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬\n",
    "        \"\"\"\n",
    "        # 1. í…ìŠ¤íŠ¸ ì„ë² ë”©\n",
    "        text_vector = self.embeddings_model.encode(text).reshape(1, -1)\n",
    "        \n",
    "        # 2. Category1 ì˜ˆì¸¡\n",
    "        cat1_pred_encoded = self.cat1_model.predict(text_vector)[0]\n",
    "        cat1_pred = self.cat1_encoder.inverse_transform([cat1_pred_encoded])[0]\n",
    "        cat1_prob = self.cat1_model.predict_proba(text_vector)[0].max()\n",
    "        \n",
    "        # 3. Category1 ì˜ˆì¸¡ê°’ì„ ì‚¬ìš©í•˜ì—¬ Category2 ì˜ˆì¸¡ìš© íŠ¹ì„± ìƒì„±\n",
    "        cat1_onehot = self.cat1_onehot_encoder.transform([[cat1_pred]])\n",
    "        combined_features = np.hstack([text_vector, cat1_onehot])\n",
    "        \n",
    "        # 4. Category2 ì˜ˆì¸¡\n",
    "        cat2_pred_encoded = self.cat2_model.predict(combined_features)[0]\n",
    "        cat2_pred = self.cat2_encoder.inverse_transform([cat2_pred_encoded])[0]\n",
    "        cat2_prob = self.cat2_model.predict_proba(combined_features)[0].max()\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'category1_predicted': cat1_pred,\n",
    "            'category1_confidence': cat1_prob,\n",
    "            'category2_predicted': cat2_pred,\n",
    "            'category2_confidence': cat2_prob\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, texts):\n",
    "        \"\"\"\n",
    "        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë°°ì¹˜ ì˜ˆì¸¡\n",
    "        \n",
    "        Args:\n",
    "            texts (list): ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "        Returns:\n",
    "            list: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            result = self.predict_single(text)\n",
    "            results.append(result)\n",
    "        return results\n",
    "    \n",
    "    def evaluate_with_ground_truth(self, texts, true_cat1, true_cat2):\n",
    "        \"\"\"\n",
    "        ì‹¤ì œ ì •ë‹µê³¼ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ í‰ê°€\n",
    "        ë‘ ì¹´í…Œê³ ë¦¬ê°€ ëª¨ë‘ ë§ì€ ê²½ìš°ë§Œ ì •ë‹µìœ¼ë¡œ ì²˜ë¦¬\n",
    "        \n",
    "        Args:\n",
    "            texts (list): ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
    "            true_cat1 (list): ì‹¤ì œ category1 ë¼ë²¨\n",
    "            true_cat2 (list): ì‹¤ì œ category2 ë¼ë²¨\n",
    "        \n",
    "        Returns:\n",
    "            dict: í‰ê°€ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        predictions = self.predict_batch(texts)\n",
    "        \n",
    "        total_count = len(texts)\n",
    "        cat1_correct = 0\n",
    "        cat2_correct = 0\n",
    "        both_correct = 0\n",
    "        \n",
    "        detailed_results = []\n",
    "        \n",
    "        for i, (pred, actual_cat1, actual_cat2) in enumerate(zip(predictions, true_cat1, true_cat2)):\n",
    "            cat1_match = pred['category1_predicted'] == actual_cat1\n",
    "            cat2_match = pred['category2_predicted'] == actual_cat2\n",
    "            both_match = cat1_match and cat2_match\n",
    "            \n",
    "            if cat1_match:\n",
    "                cat1_correct += 1\n",
    "            if cat2_match:\n",
    "                cat2_correct += 1\n",
    "            if both_match:\n",
    "                both_correct += 1\n",
    "            \n",
    "            detailed_results.append({\n",
    "                'index': i,\n",
    "                'text': pred['text'],\n",
    "                'actual_cat1': actual_cat1,\n",
    "                'predicted_cat1': pred['category1_predicted'],\n",
    "                'cat1_match': cat1_match,\n",
    "                'cat1_confidence': pred['category1_confidence'],\n",
    "                'actual_cat2': actual_cat2,\n",
    "                'predicted_cat2': pred['category2_predicted'],\n",
    "                'cat2_match': cat2_match,\n",
    "                'cat2_confidence': pred['category2_confidence'],\n",
    "                'both_correct': both_match\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'total_samples': total_count,\n",
    "            'category1_accuracy': cat1_correct / total_count,\n",
    "            'category2_accuracy': cat2_correct / total_count,\n",
    "            'both_correct_accuracy': both_correct / total_count,  # í•µì‹¬ ì§€í‘œ\n",
    "            'category1_correct_count': cat1_correct,\n",
    "            'category2_correct_count': cat2_correct,\n",
    "            'both_correct_count': both_correct,\n",
    "            'detailed_results': detailed_results\n",
    "        }\n",
    "\n",
    "# ë³€ìˆ˜ ì´ˆê¸°í™” í™•ì¸ ë° íŒŒì´í”„ë¼ì¸ ê°ì²´ ìƒì„±\n",
    "print(\"ğŸš€ ê°ì • ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”...\")\n",
    "\n",
    "# í•„ìš”í•œ ë³€ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "required_vars = ['final_cat1_model', 'final_cat2_model', 'le', 'le_cat2', 'cat1_encoder']\n",
    "missing_vars = [var for var in required_vars if var not in locals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"âš ï¸ ë‹¤ìŒ ë³€ìˆ˜ë“¤ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {missing_vars}\")\n",
    "    print(\"ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    pipeline = EmotionClassificationPipeline(\n",
    "        embeddings_model=embeddings_model,\n",
    "        cat1_model=final_cat1_model,\n",
    "        cat2_model=final_cat2_model,\n",
    "        cat1_encoder=le,\n",
    "        cat2_encoder=le_cat2,\n",
    "        cat1_onehot_encoder=cat1_encoder\n",
    "    )\n",
    "    print(\"âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c20c6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ test_data ì¢…í•© í‰ê°€\n",
      "================================================================================\n",
      "í‰ê°€ ë°ì´í„°: 664ê°œ\n",
      "Category1 í´ë˜ìŠ¤ ìˆ˜: 10\n",
      "Category2 í´ë˜ìŠ¤ ìˆ˜: 64\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤‘ë¦½: Category1=5ê°œ, Category2=0ê°œ\n",
      "\n",
      "ğŸ”„ íŒŒì´í”„ë¼ì¸ í‰ê°€ ì‹¤í–‰ ì¤‘...\n",
      "Category1: ì¤‘ë¦½ í¬í•¨í•˜ì—¬ í‰ê°€\n",
      "Category2: ì¤‘ë¦½ ì—†ìŒ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ë„ ì—†ìŒ)\n",
      "\n",
      "ğŸ“Š íŒŒì´í”„ë¼ì¸ ì¢…í•© í‰ê°€ ê²°ê³¼:\n",
      "============================================================\n",
      "ì „ì²´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: 664\n",
      "\n",
      "ğŸ“ˆ ê°œë³„ ì •í™•ë„:\n",
      "  Category1 ì •í™•ë„: 0.4880 (324/664)\n",
      "  Category2 ì •í™•ë„: 0.2425 (161/664)\n",
      "\n",
      "ğŸ¯ í•µì‹¬ ì§€í‘œ - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ:\n",
      "  ì¢…í•© ì •í™•ë„: 0.2244 (149/664)\n",
      "  ì¢…í•© ì •í™•ë„: 22.44%\n",
      "\n",
      "ğŸ” ìƒì„¸ ë¶„ì„:\n",
      "------------------------------------------------------------\n",
      "ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: 149ê°œ (22.44%)\n",
      "Category1ë§Œ ì •ë‹µ: 175ê°œ (26.36%)\n",
      "Category2ë§Œ ì •ë‹µ: 12ê°œ (1.81%)\n",
      "ë‘˜ ë‹¤ í‹€ë¦¼: 328ê°œ (49.40%)\n",
      "\n",
      "âœ… ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µì¸ ìƒ˜í”Œë“¤ (ì²˜ìŒ 10ê°œ):\n",
      "====================================================================================================\n",
      " 1. Cat1: ê¸°ì¨ âœ“ | Cat2: ë§Œì¡±ê° âœ“\n",
      "    ì‹ ë¢°ë„: Cat1=0.467, Cat2=0.208\n",
      "    í…ìŠ¤íŠ¸: ë¯¸ë¦¬ ê³„ì¢Œë¡œ í™˜ì „í•´ë‘” ëˆì„ í•´ì™¸ì—ì„œ í™˜ì „ìˆ˜ìˆ˜ë£Œ ì—†ì´ ì¸ì¶œ ê°€ëŠ¥í•œ íŠ¸ë ˆë¸”ë¡œê·¸ë¼ëŠ” ì¹´ë“œì¸ë°, ì„ íƒí•  ìˆ˜ ìˆëŠ” ë””...\n",
      "\n",
      " 2. Cat1: ê¸°ì¨ âœ“ | Cat2: ë§Œì¡±ê° âœ“\n",
      "    ì‹ ë¢°ë„: Cat1=0.943, Cat2=0.283\n",
      "    í…ìŠ¤íŠ¸: ìš°ì—°íˆ ë³´ê²Œ ëœ ì˜ìƒì¸ë°, ë…¸ë˜ê°€ ë„ˆë¬´ ì¢‹ì•„ì„œ í”Œë¦¬ì—ë„ ì¶”ê°€í•˜ê³ , ì¹´ì¹´ì˜¤í†¡ í”„ë®¤ë¡œë„ í•´ë†¨ìŒ. ìŒì›ë„ ì¢‹ê¸´ í•œ...\n",
      "\n",
      " 3. Cat1: ìš•ë§ âœ“ | Cat2: ê¶ê¸ˆí•¨ âœ“\n",
      "    ì‹ ë¢°ë„: Cat1=0.653, Cat2=0.422\n",
      "    í…ìŠ¤íŠ¸: ì¼ë³¸ì€ ê·¼ë¬´ì‹œê°„ì— ê°œì¸ë©”ì„¸ì§€ ì•ˆ í•œë‹¤ê³ ??? ì‹ ê¸°\n",
      "ì• ì´ˆì— ê°œì¸ ë©”ì„¸ì§€ ë„êµ¬ì¸ ì¹´ì¹´ì˜¤í†¡ìœ¼ë¡œ ì—…ë¬´ë¥¼ í•˜ëŠ”ë° ì¹œêµ¬...\n",
      "\n",
      " 4. Cat1: ê¸°ì¨ âœ“ | Cat2: ì¦ê±°ì›€ âœ“\n",
      "    ì‹ ë¢°ë„: Cat1=0.415, Cat2=0.367\n",
      "    í…ìŠ¤íŠ¸: ì´ë ‡ê²Œê¹Œì§€ ì¬ë°Œì„ì¤„ì€ ëª°ëìŒ\n",
      "í—‰ ì— ì”¨ ì—†ì´ ì§„í–‰í•˜ë‚˜??\n",
      "í–ˆëŠ”ë° ë‘˜ì˜ í‹°í‚¤íƒ€ì¹´ê°€ ë¯¸ì³¤ìŒ\n",
      "\n",
      " 5. Cat1: ê¸°ì¨ âœ“ | Cat2: ë§Œì¡±ê° âœ“\n",
      "    ì‹ ë¢°ë„: Cat1=0.756, Cat2=0.262\n",
      "    í…ìŠ¤íŠ¸: ì´ˆëŒ€ë°• ê·€ì—¬ìš´ ê°•ì•„ë””ë„ ìˆìŒ ìš°ë¦¬ ìë¦¬ ì™€ì„œ ì¸ì‚¬ë„ í•´ì¤Œ .. ğŸ¥¹\n",
      "ì¢Œì„ì´ ì•½ê°„ ë‹¹í™©ìŠ¤ëŸ¬ìš´ ê²ƒ ë§ê³  ë‹¤ ì¢‹ì•˜ë˜...\n",
      "\n",
      " 6. Cat1: ê¸°ì¨ âœ“ | Cat2: ë§Œì¡±ê° âœ“\n",
      "    ì‹ ë¢°ë„: Cat1=0.641, Cat2=0.773\n",
      "    í…ìŠ¤íŠ¸: í¸ì˜ì  ì´ˆì½œë¦¿ ì¤‘ ê°€ì¥ ë§›ìˆëŠ” ì´ˆì½œë¦¿ì´ë¼ê³  ìƒê°í•˜ì˜¤. ì…ì— ë„£ìë§ˆì ê¸°ë¶„ ì¢‹ì•„ì§€ëŠ” ë§›ì´ì˜¤. \n",
      "\n",
      " 7. Cat1: ë‘ë ¤ì›€ âœ“ | Cat2: ë†€ëŒ âœ“\n",
      "    ì‹ ë¢°ë„: Cat1=0.304, Cat2=0.517\n",
      "    í…ìŠ¤íŠ¸: ì•„ë‹ˆ ë¹„ì˜€ëŠ”ë°!!! ì˜ìƒ8ë„ì˜€ëŠ”ë°!!!! ëˆˆìœ¼ë¡œ ë°”ë€Œì—ˆë‹¤ë‹ˆê¹Œ!!!!! ë‚´ê°€ ê³¼ëª°ì…ì„ í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼!!!! 2...\n",
      "\n",
      " 8. Cat1: ë‘ë ¤ì›€ âœ“ | Cat2: ê±±ì • âœ“\n",
      "    ì‹ ë¢°ë„: Cat1=0.249, Cat2=0.648\n",
      "    í…ìŠ¤íŠ¸: ë‚´ê°€ ì•„ë‹ˆë¼ ë„¤ê°€ ë‹¤ì¹ ê¹Œë´ ê±±ì •ë¼\n",
      "\n",
      " 9. Cat1: ìŠ¬í”” âœ“ | Cat2: í›„íšŒ âœ“\n",
      "    ì‹ ë¢°ë„: Cat1=0.476, Cat2=0.167\n",
      "    í…ìŠ¤íŠ¸: ë„ˆë‘ ë˜ ê·¸ë ‡ê²Œ í—¤ì–´ì§€ê¸° ì‹«ì–´ ê·¸ë•Œë¡œ ì¶©ë¶„í•´\n",
      "\n",
      "10. Cat1: ìš•ë§ âœ“ | Cat2: ìš•ì‹¬ âœ“\n",
      "    ì‹ ë¢°ë„: Cat1=0.706, Cat2=0.293\n",
      "    í…ìŠ¤íŠ¸: ìœ ì£¼ì–¸ë‹ˆ í™˜ì—°ê³ ì • íŒ¨ë„ë¡œ ì„­ì™¸í•´ì•¼ í•œë‹¤ê³  ë´„\n",
      "ì œë°œìš” ë‚´ê°€ í•˜ê³  ì‹¶ì€ ë§ ìœ ì£¼ì–¸ë‹ˆê°€ ë‹¤ í•´ì£¼ì‹¬\n",
      "\n",
      "\n",
      "âŒ ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ í‹€ë¦° ìƒ˜í”Œë“¤ (ì²˜ìŒ 10ê°œ):\n",
      "====================================================================================================\n",
      " 1. Cat1: ìŠ¬í”” â†’ ì‹«ì–´í•¨(ìƒíƒœ) | Cat2: ë¬´ê¸°ë ¥ â†’ ë‚œì²˜í•¨\n",
      "    ì‹ ë¢°ë„: Cat1=0.428, Cat2=0.295\n",
      "    í…ìŠ¤íŠ¸: ìš”ì¦˜ ë²ˆì•„ì›ƒë„ ìê¾¸ ì˜¬ë¼ì˜¤ê³  ë¬´ê¸°ë ¥í•´ì„œ ì¢…ê°•í•˜ê³  êµë¥˜í•˜ê¸°ë„ ë²„ê±°ìš´ ìƒíƒœê°€ ì™€ë¶€ë €ìœ¼ìš”ã… ã…  \n",
      "\n",
      " 2. Cat1: ê¸°ì¨ â†’ ë¯¸ì›€(ìƒëŒ€ë°©) | Cat2: ì¦ê±°ì›€ â†’ ë¹„ìœ„ìƒí•¨\n",
      "    ì‹ ë¢°ë„: Cat1=0.515, Cat2=0.677\n",
      "    í…ìŠ¤íŠ¸: í¬ë¼ì„ì”¬ ì¥ë˜¥ë¯¼ì´ ë²”í–‰ ë„êµ¬ ì°¾ìœ¼ë ¤ê³  í™”ì¥ì‹¤ íƒ±í¬ ë’¤ì§€ëŠ”ë° ê±°ê¸°ì— ì§„ì§œ ë˜¥ ë„£ì–´ë†“ì€ ê±° ì§„ì§œ ì›ƒê²¨ ë’¤ì§€ê² ìŒã…‹...\n",
      "\n",
      " 3. Cat1: ì‹«ì–´í•¨(ìƒíƒœ) â†’ ìŠ¬í”” | Cat2: ë‹µë‹µí•¨ â†’ ë¬´ê¸°ë ¥\n",
      "    ì‹ ë¢°ë„: Cat1=0.476, Cat2=0.244\n",
      "    í…ìŠ¤íŠ¸: ê°€ìŠ´ì´ ë‹µë‹µí•´ì§ ì§„ì§œ ê°œë‹µë‹µí•´ì§\n",
      "ìš°ë¦¬ì§„ì§œíˆ¬í‘œì˜í•˜ì\n",
      "\n",
      " 4. Cat1: ê¸°ì¨ â†’ ìš•ë§ | Cat2: ë§Œì¡±ê° â†’ ìš•ì‹¬\n",
      "    ì‹ ë¢°ë„: Cat1=0.463, Cat2=0.417\n",
      "    í…ìŠ¤íŠ¸: ì§€ê·¸ì¬ê·¸ë‘ ì—ì´ë¸”ë¦¬ë‘ í• ì¸ ëŒ€ê²°í•˜ë‚˜\n",
      "ì•„ íë­‡í•´\n",
      "ê³„ì†ë˜ê¸¸...\n",
      "ì˜ì›íˆ....\n",
      "\n",
      " 5. Cat1: ê¸°ì¨ â†’ ìš•ë§ | Cat2: ìë‘ìŠ¤ëŸ¬ì›€ â†’ ìš•ì‹¬\n",
      "    ì‹ ë¢°ë„: Cat1=0.798, Cat2=0.434\n",
      "    í…ìŠ¤íŠ¸: ì²¨ìœ¼ë¡œ ìˆ˜ì œ ì´ˆì½œë¦¿ ë§Œë“¬\n",
      "ì´ˆì½œë¦¿ì„ 5ì‹œê°„ì´ë‚˜ ë§Œë“œëŠ” ì‚¬ëŒì´ ìˆë‹¤??? ê·¸ê²Œ ë°”ë¡œ ë‚˜\n",
      "\n",
      " 6. Cat1: ìŠ¬í”” â†’ ê¸°ì¨ | Cat2: ì ˆë§ â†’ ê¸°ëŒ€ê°\n",
      "    ì‹ ë¢°ë„: Cat1=0.689, Cat2=0.471\n",
      "    í…ìŠ¤íŠ¸: ë‹¨í†¡ë°©ì— ê³µì§€ë“¤ ìŠ¬ìŠ¬ ì˜¬ë¼ì˜¤ëŠ”ê±° ë³´ë‹ˆê¹Œ ê³§ ê°œê°•ì´ë¼ëŠ”ê²Œ ì‹¤ê°ë‚˜ì„œ ê°‘ìê¸° ì¬ê¸°í•˜ê³ ì‹¶ê³  ì¸ìƒì´ ë‹¤ ëë‚œê±°ì²˜ëŸ¼ ì•”...\n",
      "\n",
      " 7. Cat1: ë¯¸ì›€(ìƒëŒ€ë°©) â†’ ë¶„ë…¸ | Cat2: ì¹˜ì‚¬í•¨ â†’ ì›ë§\n",
      "    ì‹ ë¢°ë„: Cat1=0.667, Cat2=0.336\n",
      "    í…ìŠ¤íŠ¸: ë¯¸ì¹œìƒˆë¼ 4ê°•ì „ë§Œ ì•ˆì¢‹ì•˜ë˜ê±° ì•„ë‹ˆê³  ê·¸ì „ë¶€í„° ì•ˆì¢‹ì•˜ë˜ê±¸ ì‚¬ëŒë“¤ì´ ë‹¤ë´¤ëŠ”ë° ì§€ ì±…ì„ ì—†ë‹¤ê³  ì™ ë¹ ì ¸ë‚˜ê°€ë ¤ê³  ì§€...\n",
      "\n",
      " 8. Cat1: ë‘ë ¤ì›€ â†’ ì‹«ì–´í•¨(ìƒíƒœ) | Cat2: ê±±ì • â†’ ë‹µë‹µí•¨\n",
      "    ì‹ ë¢°ë„: Cat1=0.518, Cat2=0.882\n",
      "    í…ìŠ¤íŠ¸: ë‚˜ í˜„ì¥ì—ì„œ ì—„ì²­ í—¤ë§¬ ê±° ê°™ì€ë° ã… \n",
      "\n",
      " 9. Cat1: ìˆ˜ì¹˜ì‹¬ â†’ ìš•ë§ | Cat2: ë¶€ë„ëŸ¬ì›€ â†’ ê¶ê¸ˆí•¨\n",
      "    ì‹ ë¢°ë„: Cat1=0.500, Cat2=0.817\n",
      "    í…ìŠ¤íŠ¸: ë³´ë¼ìƒ‰ êµë³µ ì¹˜ë‹ˆê¹Œ ì´ëŸ°ê±°ë§Œ ëœ¨ëŠ”ë° ì§€ê¸ˆ ì´ê±¸ ì…ìœ¼ë¼ëŠ”ê±°ì˜ˆìš”?\n",
      "\n",
      "10. Cat1: ë¶„ë…¸ â†’ ë¯¸ì›€(ìƒëŒ€ë°©) | Cat2: ë¶ˆì¾Œ â†’ ì¹˜ì‚¬í•¨\n",
      "    ì‹ ë¢°ë„: Cat1=0.465, Cat2=0.450\n",
      "    í…ìŠ¤íŠ¸: ê·¸ëƒ¥ êµ­ëŒ€ ì œì™¸ ì•„ë¬´ë„ ëª°ëì–´ì•¼ í•  ì‚¬ì‹¤ì„ ì•Œê²Œ ë¼ì„œ ì´ ê¼¬ë½ì„œë‹ˆ ë‚œ ê²Œ ì§œì¦ë‚¨\n",
      "\n",
      "\n",
      "âš–ï¸ ì¤‘ë¦½ ë°ì´í„° ë¶„ì„ (Category1):\n",
      "------------------------------------------------------------\n",
      "ì¤‘ë¦½ ë°ì´í„° ì´ 5ê°œ ì¤‘ 0ê°œ ì •ë‹µ (0.0%)\n",
      "\n",
      "ì¤‘ë¦½ ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ (ì²˜ìŒ 5ê°œ):\n",
      "1. âŒ ì˜ˆì¸¡: ìš•ë§ (ì‹ ë¢°ë„: 0.667)\n",
      "   í…ìŠ¤íŠ¸: ê°€ë©´ ì“´ ì‚¬ëŒì˜ ì˜ìƒì´ ì¼ë³¸êµ° ì œë³µì—ì„œ ë°”ë€Œì—ˆë„¤ìš” ì˜›ë‚ ì²˜ëŸ¼ ì¼ë³¸ë‚´ ì„±ì ë§Œ ì˜¬ë¦¬ë©´ ê·¸ë§Œì´ë˜ ...\n",
      "2. âŒ ì˜ˆì¸¡: ê¸°ì¨ (ì‹ ë¢°ë„: 0.337)\n",
      "   í…ìŠ¤íŠ¸: ì‚´ì¸ìã…‡ë‚œê° ì‚´ì¸ì¥ë©´ì¤‘ ê°€ì¥ ìƒˆë¡­ê³  ì‹ ì„ í•œ ì—°ì¶œì´ë¼ ê°ë…ë‹˜ì´ ë§í•œ íŒí•˜ë‹¤ëŠ” ì—°ì¶œ ë­”ì§€ ì•Œêº¼...\n",
      "3. âŒ ì˜ˆì¸¡: ê¸°ì¨ (ì‹ ë¢°ë„: 0.912)\n",
      "   í…ìŠ¤íŠ¸: 100ì¼ë„ì•ˆëœ ì•„ê¸°êº¼ í•œë³µì´ ìˆë‹¤ëŠ”ê²Œ ë†€ëë‹¤\n",
      "4. âŒ ì˜ˆì¸¡: ë¯¸ì›€(ìƒëŒ€ë°©) (ì‹ ë¢°ë„: 0.424)\n",
      "   í…ìŠ¤íŠ¸: ì˜ë§Œ ë¬¼ë˜ ìª½ìª½ì´ë¥¼ í•˜ë£¨ ì•„ì¹¨ì— ê±°ë¶€í•˜ë‹ˆ ì—„ë§ˆëŠ” ë‹¹í™©ìŠ¤ëŸ½ë‹¤\n",
      "5. âŒ ì˜ˆì¸¡: ê¸°ì¨ (ì‹ ë¢°ë„: 0.771)\n",
      "   í…ìŠ¤íŠ¸: ë‚˜ë§Œ ë†€ë¼ìš´ ê±´ê°€ìš”? ëª°ì…ì„ ì“°ì‹  í™©ë†ë¬¸êµìˆ˜ë‹˜ê»˜ ë©”ì¼ë³´ëƒˆë‹¤ëŠ” ê²ƒ!?? ëŒ€ë‹¨í•œ í–‰ë™ë ¥ì´ì‹œë„¤ìš”...\n",
      "\n",
      "ğŸ”¥ ê²°ë¡ :\n",
      "ì´ íŒŒì´í”„ë¼ì¸ì—ì„œ ë‘ ì¹´í…Œê³ ë¦¬ê°€ ëª¨ë‘ ì •í™•í•˜ê²Œ ì˜ˆì¸¡ëœ ê²½ìš°ëŠ” ì „ì²´ì˜ 22.44%ì…ë‹ˆë‹¤.\n",
      "â€» Category1ì€ ì¤‘ë¦½ì„ í¬í•¨í•˜ì—¬ í‰ê°€, Category2ëŠ” ì¤‘ë¦½ì´ ì—†ì–´ ì¼ì¹˜í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 11. íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ test_data í‰ê°€ (ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ë§ì€ ê²½ìš°ë§Œ ì •ë‹µ ì²˜ë¦¬)\n",
    "\n",
    "print(\"ğŸ¯ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ test_data ì¢…í•© í‰ê°€\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# test_data ì¤€ë¹„\n",
    "test_texts = test_data['context'].fillna('').astype(str).tolist()\n",
    "test_true_cat1 = test_data['category1'].values\n",
    "test_true_cat2 = test_data['category2'].values\n",
    "\n",
    "print(f\"í‰ê°€ ë°ì´í„°: {len(test_texts)}ê°œ\")\n",
    "print(f\"Category1 í´ë˜ìŠ¤ ìˆ˜: {len(np.unique(test_true_cat1))}\")\n",
    "print(f\"Category2 í´ë˜ìŠ¤ ìˆ˜: {len(np.unique(test_true_cat2))}\")\n",
    "\n",
    "# ì¤‘ë¦½ ë°ì´í„° í™•ì¸\n",
    "neutral_cat1_count = (test_true_cat1 == 'ì¤‘ë¦½').sum()\n",
    "neutral_cat2_count = (test_true_cat2 == 'ì¤‘ë¦½').sum()\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤‘ë¦½: Category1={neutral_cat1_count}ê°œ, Category2={neutral_cat2_count}ê°œ\")\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì¢…í•© í‰ê°€ ì‹¤í–‰ (ëª¨ë“  ë°ì´í„° ì‚¬ìš© - Category1ì— ì¤‘ë¦½ í¬í•¨)\n",
    "print(f\"\\nğŸ”„ íŒŒì´í”„ë¼ì¸ í‰ê°€ ì‹¤í–‰ ì¤‘...\")\n",
    "print(f\"Category1: ì¤‘ë¦½ í¬í•¨í•˜ì—¬ í‰ê°€\")\n",
    "print(f\"Category2: ì¤‘ë¦½ ì—†ìŒ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ë„ ì—†ìŒ)\")\n",
    "\n",
    "evaluation_results = pipeline.evaluate_with_ground_truth(\n",
    "    test_texts, test_true_cat1, test_true_cat2\n",
    ")\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"\\nğŸ“Š íŒŒì´í”„ë¼ì¸ ì¢…í•© í‰ê°€ ê²°ê³¼:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ì „ì²´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {evaluation_results['total_samples']}\")\n",
    "print(f\"\")\n",
    "print(f\"ğŸ“ˆ ê°œë³„ ì •í™•ë„:\")\n",
    "print(f\"  Category1 ì •í™•ë„: {evaluation_results['category1_accuracy']:.4f} ({evaluation_results['category1_correct_count']}/{evaluation_results['total_samples']})\")\n",
    "print(f\"  Category2 ì •í™•ë„: {evaluation_results['category2_accuracy']:.4f} ({evaluation_results['category2_correct_count']}/{evaluation_results['total_samples']})\")\n",
    "print(f\"\")\n",
    "print(f\"ğŸ¯ í•µì‹¬ ì§€í‘œ - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ:\")\n",
    "print(f\"  ì¢…í•© ì •í™•ë„: {evaluation_results['both_correct_accuracy']:.4f} ({evaluation_results['both_correct_count']}/{evaluation_results['total_samples']})\")\n",
    "print(f\"  ì¢…í•© ì •í™•ë„: {evaluation_results['both_correct_accuracy']*100:.2f}%\")\n",
    "\n",
    "# ìƒì„¸ ë¶„ì„\n",
    "print(f\"\\nğŸ” ìƒì„¸ ë¶„ì„:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¹˜ íŒ¨í„´ ë¶„ì„\n",
    "both_correct = sum(1 for r in evaluation_results['detailed_results'] if r['both_correct'])\n",
    "only_cat1_correct = sum(1 for r in evaluation_results['detailed_results'] if r['cat1_match'] and not r['cat2_match'])\n",
    "only_cat2_correct = sum(1 for r in evaluation_results['detailed_results'] if not r['cat1_match'] and r['cat2_match'])\n",
    "both_wrong = sum(1 for r in evaluation_results['detailed_results'] if not r['cat1_match'] and not r['cat2_match'])\n",
    "\n",
    "print(f\"ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: {both_correct}ê°œ ({both_correct/evaluation_results['total_samples']*100:.2f}%)\")\n",
    "print(f\"Category1ë§Œ ì •ë‹µ: {only_cat1_correct}ê°œ ({only_cat1_correct/evaluation_results['total_samples']*100:.2f}%)\")\n",
    "print(f\"Category2ë§Œ ì •ë‹µ: {only_cat2_correct}ê°œ ({only_cat2_correct/evaluation_results['total_samples']*100:.2f}%)\")\n",
    "print(f\"ë‘˜ ë‹¤ í‹€ë¦¼: {both_wrong}ê°œ ({both_wrong/evaluation_results['total_samples']*100:.2f}%)\")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥ - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ë§ì€ ì¼€ì´ìŠ¤\n",
    "print(f\"\\nâœ… ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µì¸ ìƒ˜í”Œë“¤ (ì²˜ìŒ 10ê°œ):\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "correct_samples = [r for r in evaluation_results['detailed_results'] if r['both_correct']]\n",
    "for i, result in enumerate(correct_samples[:10]):\n",
    "    text = result['text'][:60] + \"...\" if len(result['text']) > 60 else result['text']\n",
    "    print(f\"{i+1:2d}. Cat1: {result['actual_cat1']} âœ“ | Cat2: {result['actual_cat2']} âœ“\")\n",
    "    print(f\"    ì‹ ë¢°ë„: Cat1={result['cat1_confidence']:.3f}, Cat2={result['cat2_confidence']:.3f}\")\n",
    "    print(f\"    í…ìŠ¤íŠ¸: {text}\")\n",
    "    print()\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥ - ë‘˜ ë‹¤ í‹€ë¦° ì¼€ì´ìŠ¤  \n",
    "print(f\"\\nâŒ ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ í‹€ë¦° ìƒ˜í”Œë“¤ (ì²˜ìŒ 10ê°œ):\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "wrong_samples = [r for r in evaluation_results['detailed_results'] if not r['both_correct'] and not r['cat1_match'] and not r['cat2_match']]\n",
    "for i, result in enumerate(wrong_samples[:10]):\n",
    "    text = result['text'][:60] + \"...\" if len(result['text']) > 60 else result['text']\n",
    "    print(f\"{i+1:2d}. Cat1: {result['actual_cat1']} â†’ {result['predicted_cat1']} | Cat2: {result['actual_cat2']} â†’ {result['predicted_cat2']}\")\n",
    "    print(f\"    ì‹ ë¢°ë„: Cat1={result['cat1_confidence']:.3f}, Cat2={result['cat2_confidence']:.3f}\")\n",
    "    print(f\"    í…ìŠ¤íŠ¸: {text}\")\n",
    "    print()\n",
    "\n",
    "# ì¤‘ë¦½ ë°ì´í„° íŠ¹ë³„ ë¶„ì„\n",
    "print(f\"\\nâš–ï¸ ì¤‘ë¦½ ë°ì´í„° ë¶„ì„ (Category1):\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "neutral_results = [r for r in evaluation_results['detailed_results'] if r['actual_cat1'] == 'ì¤‘ë¦½']\n",
    "if len(neutral_results) > 0:\n",
    "    neutral_correct = sum(1 for r in neutral_results if r['cat1_match'])\n",
    "    print(f\"ì¤‘ë¦½ ë°ì´í„° ì´ {len(neutral_results)}ê°œ ì¤‘ {neutral_correct}ê°œ ì •ë‹µ ({neutral_correct/len(neutral_results)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nì¤‘ë¦½ ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ (ì²˜ìŒ 5ê°œ):\")\n",
    "    for i, result in enumerate(neutral_results[:5]):\n",
    "        text = result['text'][:50] + \"...\" if len(result['text']) > 50 else result['text']\n",
    "        status = \"âœ…\" if result['cat1_match'] else \"âŒ\"\n",
    "        print(f\"{i+1}. {status} ì˜ˆì¸¡: {result['predicted_cat1']} (ì‹ ë¢°ë„: {result['cat1_confidence']:.3f})\")\n",
    "        print(f\"   í…ìŠ¤íŠ¸: {text}\")\n",
    "else:\n",
    "    print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì¤‘ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nğŸ”¥ ê²°ë¡ :\")\n",
    "print(f\"ì´ íŒŒì´í”„ë¼ì¸ì—ì„œ ë‘ ì¹´í…Œê³ ë¦¬ê°€ ëª¨ë‘ ì •í™•í•˜ê²Œ ì˜ˆì¸¡ëœ ê²½ìš°ëŠ” ì „ì²´ì˜ {evaluation_results['both_correct_accuracy']*100:.2f}%ì…ë‹ˆë‹¤.\")\n",
    "print(f\"â€» Category1ì€ ì¤‘ë¦½ì„ í¬í•¨í•˜ì—¬ í‰ê°€, Category2ëŠ” ì¤‘ë¦½ì´ ì—†ì–´ ì¼ì¹˜í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "623b7374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ íŒŒì´í”„ë¼ì¸ ì˜ˆì¸¡ ì‹œì—°\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ ì˜ˆì œ 1: ì¹œêµ¬ê°€ ìƒì¼ íŒŒí‹°ë¥¼ ì¤€ë¹„í•´ì¤˜ì„œ ë„ˆë¬´ ê°ë™ë°›ì•˜ì–´\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼:\n",
      "  Category1: ê¸°ì¨ (ì‹ ë¢°ë„: 0.994)\n",
      "  Category2: ê³ ë§ˆì›€ (ì‹ ë¢°ë„: 0.495)\n",
      "\n",
      "ğŸ“ ì˜ˆì œ 2: ì‹œí—˜ ê²°ê³¼ê°€ ë‚˜ì˜ê²Œ ë‚˜ì™€ì„œ ì •ë§ ì‹¤ë§ìŠ¤ëŸ½ë‹¤\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼:\n",
      "  Category1: ìŠ¬í”” (ì‹ ë¢°ë„: 0.584)\n",
      "  Category2: ì‹¤ë§ (ì‹ ë¢°ë„: 0.804)\n",
      "\n",
      "ğŸ“ ì˜ˆì œ 3: ìƒˆë¡œìš´ ì§ì¥ì´ í™•ì •ë˜ì–´ì„œ ì„¤ë ˆê³  ê¸°ëŒ€ëœë‹¤\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼:\n",
      "  Category1: ê¸°ì¨ (ì‹ ë¢°ë„: 0.983)\n",
      "  Category2: ê¸°ëŒ€ê° (ì‹ ë¢°ë„: 0.978)\n",
      "\n",
      "ğŸ“ ì˜ˆì œ 4: ëˆ„êµ°ê°€ ë‚´ ë’·ë‹´í™”ë¥¼ í•˜ëŠ” ê±¸ ë“¤ì–´ì„œ í™”ê°€ ë‚œë‹¤\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼:\n",
      "  Category1: ë¶„ë…¸ (ì‹ ë¢°ë„: 0.578)\n",
      "  Category2: ë¶ˆì¾Œ (ì‹ ë¢°ë„: 0.826)\n"
     ]
    }
   ],
   "source": [
    "# 12. ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ë°ëª¨ í•¨ìˆ˜\n",
    "\n",
    "def demo_pipeline_prediction():\n",
    "    \"\"\"\n",
    "    ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë“¤ì— ëŒ€í•´ íŒŒì´í”„ë¼ì¸ ì˜ˆì¸¡ ì‹œì—°\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ íŒŒì´í”„ë¼ì¸ ì˜ˆì¸¡ ì‹œì—°\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ì˜ˆì œ í…ìŠ¤íŠ¸ë“¤\n",
    "    demo_texts = [\n",
    "        \"ì¹œêµ¬ê°€ ìƒì¼ íŒŒí‹°ë¥¼ ì¤€ë¹„í•´ì¤˜ì„œ ë„ˆë¬´ ê°ë™ë°›ì•˜ì–´\",\n",
    "        \"ì‹œí—˜ ê²°ê³¼ê°€ ë‚˜ì˜ê²Œ ë‚˜ì™€ì„œ ì •ë§ ì‹¤ë§ìŠ¤ëŸ½ë‹¤\", \n",
    "        \"ìƒˆë¡œìš´ ì§ì¥ì´ í™•ì •ë˜ì–´ì„œ ì„¤ë ˆê³  ê¸°ëŒ€ëœë‹¤\",\n",
    "        \"ëˆ„êµ°ê°€ ë‚´ ë’·ë‹´í™”ë¥¼ í•˜ëŠ” ê±¸ ë“¤ì–´ì„œ í™”ê°€ ë‚œë‹¤\"\n",
    "    ]\n",
    "    \n",
    "    for i, text in enumerate(demo_texts, 1):\n",
    "        print(f\"\\nğŸ“ ì˜ˆì œ {i}: {text}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        # íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì˜ˆì¸¡\n",
    "        result = pipeline.predict_single(text)\n",
    "        \n",
    "        print(f\"ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "        print(f\"  Category1: {result['category1_predicted']} (ì‹ ë¢°ë„: {result['category1_confidence']:.3f})\")\n",
    "        print(f\"  Category2: {result['category2_predicted']} (ì‹ ë¢°ë„: {result['category2_confidence']:.3f})\")\n",
    "\n",
    "# ì‹œì—° ì‹¤í–‰\n",
    "demo_pipeline_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "i821u6i92xl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” PCA ì°¨ì› ì¶•ì†Œ íŒŒì´í”„ë¼ì¸ í‰ê°€\n",
      "================================================================================\n",
      "ì›ë³¸ ë²¡í„° ì°¨ì›: 1024\n",
      "í‰ê°€í•  PCA ì°¨ì›: [128, 256, 512, 768]\n",
      "í›ˆë ¨ ë°ì´í„°: 3359ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°: 664ê°œ\n",
      "\n",
      "\n",
      "==================== PCA 128ì°¨ì› í‰ê°€ ====================\n",
      "ğŸ“ PCA 128ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ ì¤‘...\n",
      "  í›ˆë ¨ ë°ì´í„°: (3359, 1024) â†’ (3359, 128)\n",
      "  í…ŒìŠ¤íŠ¸ ë°ì´í„°: (664, 1024) â†’ (664, 128)\n",
      "  ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚° ë¹„ìœ¨: 0.8185 (81.85%)\n",
      "ğŸ¤– Category1 ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "  Category1 ì •í™•ë„: 0.5015 (50.15%)\n",
      "ğŸ¤– Category2 ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "  Category2 ì •í™•ë„: 0.2485 (24.85%)\n",
      "  ğŸ¯ ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: 152/664 (0.2289, 22.89%)\n",
      "\n",
      "==================== PCA 256ì°¨ì› í‰ê°€ ====================\n",
      "ğŸ“ PCA 256ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ ì¤‘...\n",
      "  í›ˆë ¨ ë°ì´í„°: (3359, 1024) â†’ (3359, 256)\n",
      "  í…ŒìŠ¤íŠ¸ ë°ì´í„°: (664, 1024) â†’ (664, 256)\n",
      "  ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚° ë¹„ìœ¨: 0.9409 (94.09%)\n",
      "ğŸ¤– Category1 ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "  Category1 ì •í™•ë„: 0.4880 (48.80%)\n",
      "ğŸ¤– Category2 ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "  Category2 ì •í™•ë„: 0.2199 (21.99%)\n",
      "  ğŸ¯ ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: 133/664 (0.2003, 20.03%)\n",
      "\n",
      "==================== PCA 512ì°¨ì› í‰ê°€ ====================\n",
      "ğŸ“ PCA 512ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ ì¤‘...\n",
      "  í›ˆë ¨ ë°ì´í„°: (3359, 1024) â†’ (3359, 512)\n",
      "  í…ŒìŠ¤íŠ¸ ë°ì´í„°: (664, 1024) â†’ (664, 512)\n",
      "  ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚° ë¹„ìœ¨: 0.9961 (99.61%)\n",
      "ğŸ¤– Category1 ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "  Category1 ì •í™•ë„: 0.4880 (48.80%)\n",
      "ğŸ¤– Category2 ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "  Category2 ì •í™•ë„: 0.2139 (21.39%)\n",
      "  ğŸ¯ ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: 130/664 (0.1958, 19.58%)\n",
      "\n",
      "==================== PCA 768ì°¨ì› í‰ê°€ ====================\n",
      "ğŸ“ PCA 768ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ ì¤‘...\n",
      "  í›ˆë ¨ ë°ì´í„°: (3359, 1024) â†’ (3359, 768)\n",
      "  í…ŒìŠ¤íŠ¸ ë°ì´í„°: (664, 1024) â†’ (664, 768)\n",
      "  ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚° ë¹„ìœ¨: 0.9994 (99.94%)\n",
      "ğŸ¤– Category1 ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "  Category1 ì •í™•ë„: 0.4895 (48.95%)\n",
      "ğŸ¤– Category2 ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "  Category2 ì •í™•ë„: 0.2184 (21.84%)\n",
      "  ğŸ¯ ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: 134/664 (0.2018, 20.18%)\n",
      "\n",
      "========================= ê²°ê³¼ ë¹„êµ ë¶„ì„ =========================\n",
      "PCA ì°¨ì›     ë¶„ì‚°ë¹„ìœ¨         Cat1 ì •í™•ë„     Cat2 ì •í™•ë„     ì¢…í•© ì •í™•ë„      \n",
      "----------------------------------------------------------------------\n",
      "ì›ë³¸         100.00%      0.4880       0.2425       0.2244      \n",
      "128        81.85      % 0.5015       0.2485       0.2289      \n",
      "256        94.09      % 0.4880       0.2199       0.2003      \n",
      "512        99.61      % 0.4880       0.2139       0.1958      \n",
      "768        99.94      % 0.4895       0.2184       0.2018      \n",
      "\n",
      "ğŸ† ì„±ëŠ¥ ë¶„ì„:\n",
      "  ì›ë³¸ (1024ì°¨ì›) ì¢…í•© ì •í™•ë„: 0.2244 (22.44%)\n",
      "  ìµœê³  PCA (128ì°¨ì›) ì¢…í•© ì •í™•ë„: 0.2289 (22.89%)\n",
      "  âœ… PCA 128ì°¨ì›ì´ ì›ë³¸ë³´ë‹¤ 0.45%p ë” ì¢‹ìŠµë‹ˆë‹¤!\n",
      "\n",
      "ğŸ“Š ì°¨ì›ë³„ ì„±ëŠ¥ ë³€í™”:\n",
      "PCA ì°¨ì›  â†’ ì¢…í•© ì •í™•ë„\n",
      "-------------------------\n",
      "128ì°¨ì›   â†’ 0.229 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘|\n",
      "256ì°¨ì›   â†’ 0.200 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘|\n",
      "512ì°¨ì›   â†’ 0.196 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘|\n",
      "768ì°¨ì›   â†’ 0.202 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘|\n",
      "\n",
      "ğŸ” ìµœê³  ì„±ëŠ¥ PCA 128ì°¨ì› ìƒì„¸ ë¶„ì„:\n",
      "--------------------------------------------------------------------------------\n",
      "ì²˜ìŒ 20ê°œ ìƒ˜í”Œì—ì„œ ì›ë³¸ê³¼ PCA ì˜ˆì¸¡ ì¼ì¹˜ìœ¨: 17/20 (85.0%)\n",
      "\n",
      "ì›ë³¸ vs PCA ì˜ˆì¸¡ì´ ë‹¤ë¥¸ ìƒ˜í”Œë“¤ (ì²˜ìŒ 5ê°œ):\n",
      "1. ì›ë³¸: âŒ | PCA: âœ…\n",
      "   ì‹¤ì œ: Cat1=ê¸°ì¨, Cat2=ë§Œì¡±ê°\n",
      "   í…ìŠ¤íŠ¸: ì–´ë¦´ ë•Œ ê°€ ë³´ê³  ë¹•ìŠ¤ëŠ” ê±°ì˜ ì²˜ìŒì¸ë°(ê¸°ì–µì— ì—†ìŒ) ì§€ê¸ˆ ë”¸ê¸°ì¶•ì œ ê¸°ê°„ì´ë¼ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì‹ì‚¬ í•˜ê³  ì˜´\n",
      "2. ì›ë³¸: âŒ | PCA: âœ…\n",
      "   ì‹¤ì œ: Cat1=ì‹«ì–´í•¨(ìƒíƒœ), Cat2=ë‹µë‹µí•¨\n",
      "   í…ìŠ¤íŠ¸: ê°€ìŠ´ì´ ë‹µë‹µí•´ì§ ì§„ì§œ ê°œë‹µë‹µí•´ì§\n",
      "ìš°ë¦¬ì§„ì§œíˆ¬í‘œì˜í•˜ì\n",
      "3. ì›ë³¸: âœ… | PCA: âŒ\n",
      "   ì‹¤ì œ: Cat1=ê¸°ì¨, Cat2=ë§Œì¡±ê°\n",
      "   í…ìŠ¤íŠ¸: ìš°ì—°íˆ ë³´ê²Œ ëœ ì˜ìƒì¸ë°, ë…¸ë˜ê°€ ë„ˆë¬´ ì¢‹ì•„ì„œ í”Œë¦¬ì—ë„ ì¶”ê°€í•˜ê³ , ì¹´ì¹´ì˜¤í†¡ í”„ë®¤ë¡œë„ í•´ë†¨ìŒ. ìŒì›ë„ ì¢‹ê¸´ í•œ...\n",
      "\n",
      "ğŸ’¡ ê²°ë¡ : PCAë¥¼ í†µí•œ ì°¨ì› ì¶•ì†ŒëŠ” \n",
      "ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ë©ë‹ˆë‹¤. ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ëª¨ë‘ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 13. PCA ì°¨ì› ì¶•ì†Œ íŒŒì´í”„ë¼ì¸ - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ ì •í™•ë„ ë¹„êµ\n",
    "\n",
    "print(\"ğŸ” PCA ì°¨ì› ì¶•ì†Œ íŒŒì´í”„ë¼ì¸ í‰ê°€\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# PCA ì°¨ì› ë¦¬ìŠ¤íŠ¸ ì •ì˜\n",
    "pca_dimensions = [128, 256, 512, 768]\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬\n",
    "pca_results = {}\n",
    "\n",
    "# ì›ë³¸ ë²¡í„° í¬ê¸° í™•ì¸\n",
    "original_dim = X.shape[1]\n",
    "print(f\"ì›ë³¸ ë²¡í„° ì°¨ì›: {original_dim}\")\n",
    "print(f\"í‰ê°€í•  PCA ì°¨ì›: {pca_dimensions}\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„°: {X.shape[0]}ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_texts)}ê°œ\")\n",
    "print()\n",
    "\n",
    "# ê° PCA ì°¨ì›ì— ëŒ€í•´ í‰ê°€\n",
    "for n_components in pca_dimensions:\n",
    "    print(f\"\\n{'='*20} PCA {n_components}ì°¨ì› í‰ê°€ {'='*20}\")\n",
    "    \n",
    "    # 1. PCA ì ìš©\n",
    "    print(f\"ğŸ“ PCA {n_components}ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ ì¤‘...\")\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    \n",
    "    # í›ˆë ¨ ë°ì´í„° PCA ë³€í™˜\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    print(f\"  í›ˆë ¨ ë°ì´í„°: {X.shape} â†’ {X_pca.shape}\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° PCA ë³€í™˜\n",
    "    test_X_pca = pca.transform(test_X)\n",
    "    print(f\"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_X.shape} â†’ {test_X_pca.shape}\")\n",
    "    \n",
    "    # ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚° ë¹„ìœ¨\n",
    "    explained_variance = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"  ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚° ë¹„ìœ¨: {explained_variance:.4f} ({explained_variance*100:.2f}%)\")\n",
    "    \n",
    "    # 2. Category1 ëª¨ë¸ í›ˆë ¨ (PCA ì ìš©)\n",
    "    print(f\"ğŸ¤– Category1 ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "    cat1_model_pca = xgb.XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=8,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    cat1_model_pca.fit(X_pca, y_encoded)\n",
    "    \n",
    "    # Category1 ì˜ˆì¸¡\n",
    "    test_y_pred_cat1_pca_encoded = cat1_model_pca.predict(test_X_pca)\n",
    "    test_y_pred_cat1_pca = le.inverse_transform(test_y_pred_cat1_pca_encoded)\n",
    "    \n",
    "    # Category1 ì •í™•ë„\n",
    "    cat1_accuracy_pca = (test_y_pred_cat1_pca == test_y_actual).mean()\n",
    "    print(f\"  Category1 ì •í™•ë„: {cat1_accuracy_pca:.4f} ({cat1_accuracy_pca*100:.2f}%)\")\n",
    "    \n",
    "    # 3. Category2 ëª¨ë¸ í›ˆë ¨ (PCA ì ìš©)\n",
    "    print(f\"ğŸ¤– Category2 ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "    \n",
    "    # Category1 ì›í•« ì¸ì½”ë”© (PCA ì ìš©ëœ ì˜ˆì¸¡ê°’ ì‚¬ìš©)\n",
    "    cat1_onehot_pca = cat1_encoder.transform(test_y_pred_cat1_pca.reshape(-1, 1))\n",
    "    \n",
    "    # PCA ì ìš©ëœ ë²¡í„°ì™€ Category1 ì›í•« ê²°í•© (í›ˆë ¨ìš©)\n",
    "    y_cat1_onehot_pca = cat1_encoder.transform(y.reshape(-1, 1))\n",
    "    X_combined_pca = np.hstack([X_pca, y_cat1_onehot_pca])\n",
    "    \n",
    "    # PCA ì ìš©ëœ ë²¡í„°ì™€ Category1 ì˜ˆì¸¡ ì›í•« ê²°í•© (í…ŒìŠ¤íŠ¸ìš©)\n",
    "    test_X_combined_pca = np.hstack([test_X_pca, cat1_onehot_pca])\n",
    "    \n",
    "    cat2_model_pca = xgb.XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=8,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    cat2_model_pca.fit(X_combined_pca, y_cat2_encoded)\n",
    "    \n",
    "    # Category2 ì˜ˆì¸¡\n",
    "    test_y_pred_cat2_pca_encoded = cat2_model_pca.predict(test_X_combined_pca)\n",
    "    test_y_pred_cat2_pca = le_cat2.inverse_transform(test_y_pred_cat2_pca_encoded)\n",
    "    \n",
    "    # Category2 ì •í™•ë„\n",
    "    cat2_accuracy_pca = (test_y_pred_cat2_pca == test_y_actual_cat2).mean()\n",
    "    print(f\"  Category2 ì •í™•ë„: {cat2_accuracy_pca:.4f} ({cat2_accuracy_pca*100:.2f}%)\")\n",
    "    \n",
    "    # 4. ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µì¸ ê²½ìš° ê³„ì‚°\n",
    "    both_correct_pca = (test_y_pred_cat1_pca == test_y_actual) & (test_y_pred_cat2_pca == test_y_actual_cat2)\n",
    "    both_accuracy_pca = both_correct_pca.mean()\n",
    "    both_count_pca = both_correct_pca.sum()\n",
    "    \n",
    "    print(f\"  ğŸ¯ ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: {both_count_pca}/{len(test_y_actual)} ({both_accuracy_pca:.4f}, {both_accuracy_pca*100:.2f}%)\")\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    pca_results[n_components] = {\n",
    "        'explained_variance': explained_variance,\n",
    "        'cat1_accuracy': cat1_accuracy_pca,\n",
    "        'cat2_accuracy': cat2_accuracy_pca,\n",
    "        'both_accuracy': both_accuracy_pca,\n",
    "        'both_count': both_count_pca,\n",
    "        'cat1_predictions': test_y_pred_cat1_pca,\n",
    "        'cat2_predictions': test_y_pred_cat2_pca\n",
    "    }\n",
    "\n",
    "# 5. ì „ì²´ ê²°ê³¼ ë¹„êµ ë¶„ì„\n",
    "print(f\"\\n{'='*25} ê²°ê³¼ ë¹„êµ ë¶„ì„ {'='*25}\")\n",
    "print(f\"{'PCA ì°¨ì›':<10} {'ë¶„ì‚°ë¹„ìœ¨':<12} {'Cat1 ì •í™•ë„':<12} {'Cat2 ì •í™•ë„':<12} {'ì¢…í•© ì •í™•ë„':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# ì›ë³¸ ê²°ê³¼ (PCA ì—†ìŒ) ê³„ì‚°\n",
    "original_both_correct = (test_y_pred == test_y_actual) & (test_y_pred_cat2 == test_y_actual_cat2)\n",
    "original_both_accuracy = original_both_correct.mean()\n",
    "\n",
    "print(f\"{'ì›ë³¸':<10} {'100.00%':<12} {accuracy:<12.4f} {accuracy_cat2:<12.4f} {original_both_accuracy:<12.4f}\")\n",
    "\n",
    "# PCA ê²°ê³¼ë“¤\n",
    "for dim in pca_dimensions:\n",
    "    result = pca_results[dim]\n",
    "    print(f\"{dim:<10} {result['explained_variance']*100:<11.2f}% {result['cat1_accuracy']:<12.4f} {result['cat2_accuracy']:<12.4f} {result['both_accuracy']:<12.4f}\")\n",
    "\n",
    "# 6. ìµœê³  ì„±ëŠ¥ ì°¨ì› ì°¾ê¸°\n",
    "print(f\"\\nğŸ† ì„±ëŠ¥ ë¶„ì„:\")\n",
    "best_both_accuracy = max(result['both_accuracy'] for result in pca_results.values())\n",
    "best_pca_dim = max(pca_results.keys(), key=lambda k: pca_results[k]['both_accuracy'])\n",
    "\n",
    "print(f\"  ì›ë³¸ (1024ì°¨ì›) ì¢…í•© ì •í™•ë„: {original_both_accuracy:.4f} ({original_both_accuracy*100:.2f}%)\")\n",
    "print(f\"  ìµœê³  PCA ({best_pca_dim}ì°¨ì›) ì¢…í•© ì •í™•ë„: {best_both_accuracy:.4f} ({best_both_accuracy*100:.2f}%)\")\n",
    "\n",
    "if best_both_accuracy > original_both_accuracy:\n",
    "    improvement = (best_both_accuracy - original_both_accuracy) * 100\n",
    "    print(f\"  âœ… PCA {best_pca_dim}ì°¨ì›ì´ ì›ë³¸ë³´ë‹¤ {improvement:.2f}%p ë” ì¢‹ìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    decline = (original_both_accuracy - best_both_accuracy) * 100\n",
    "    print(f\"  âš ï¸ ì›ë³¸ì´ ìµœê³  PCAë³´ë‹¤ {decline:.2f}%p ë” ì¢‹ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 7. ì°¨ì›ë³„ ì„±ëŠ¥ ë³€í™” ì‹œê°í™” (í…ìŠ¤íŠ¸)\n",
    "print(f\"\\nğŸ“Š ì°¨ì›ë³„ ì„±ëŠ¥ ë³€í™”:\")\n",
    "print(f\"PCA ì°¨ì›  â†’ ì¢…í•© ì •í™•ë„\")\n",
    "print(\"-\" * 25)\n",
    "for dim in sorted(pca_dimensions):\n",
    "    result = pca_results[dim]\n",
    "    bar_length = int(result['both_accuracy'] * 50)  # 50ì¹¸ ê¸°ì¤€\n",
    "    bar = \"â–ˆ\" * bar_length + \"â–‘\" * (50 - bar_length)\n",
    "    print(f\"{dim:>3}ì°¨ì›   â†’ {result['both_accuracy']:.3f} |{bar}|\")\n",
    "\n",
    "# 8. ìƒì„¸ ìƒ˜í”Œ ë¶„ì„ (ìµœê³  ì„±ëŠ¥ PCA ì°¨ì›)\n",
    "print(f\"\\nğŸ” ìµœê³  ì„±ëŠ¥ PCA {best_pca_dim}ì°¨ì› ìƒì„¸ ë¶„ì„:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "best_cat1_preds = pca_results[best_pca_dim]['cat1_predictions']\n",
    "best_cat2_preds = pca_results[best_pca_dim]['cat2_predictions']\n",
    "\n",
    "# ì›ë³¸ vs PCA ì˜ˆì¸¡ ë¹„êµ\n",
    "agreement_count = 0\n",
    "disagreement_examples = []\n",
    "\n",
    "for i in range(min(len(test_y_actual), 20)):  # ì²˜ìŒ 20ê°œ ìƒ˜í”Œë§Œ\n",
    "    original_both = (test_y_pred[i] == test_y_actual[i]) and (test_y_pred_cat2[i] == test_y_actual_cat2[i])\n",
    "    pca_both = (best_cat1_preds[i] == test_y_actual[i]) and (best_cat2_preds[i] == test_y_actual_cat2[i])\n",
    "    \n",
    "    if original_both == pca_both:\n",
    "        agreement_count += 1\n",
    "    else:\n",
    "        disagreement_examples.append({\n",
    "            'index': i,\n",
    "            'text': test_texts[i][:60] + \"...\" if len(test_texts[i]) > 60 else test_texts[i],\n",
    "            'actual_cat1': test_y_actual[i],\n",
    "            'actual_cat2': test_y_actual_cat2[i],\n",
    "            'original_both': original_both,\n",
    "            'pca_both': pca_both\n",
    "        })\n",
    "\n",
    "print(f\"ì²˜ìŒ 20ê°œ ìƒ˜í”Œì—ì„œ ì›ë³¸ê³¼ PCA ì˜ˆì¸¡ ì¼ì¹˜ìœ¨: {agreement_count}/20 ({agreement_count/20*100:.1f}%)\")\n",
    "\n",
    "if disagreement_examples:\n",
    "    print(f\"\\nì›ë³¸ vs PCA ì˜ˆì¸¡ì´ ë‹¤ë¥¸ ìƒ˜í”Œë“¤ (ì²˜ìŒ 5ê°œ):\")\n",
    "    for i, example in enumerate(disagreement_examples[:5]):\n",
    "        status_original = \"âœ…\" if example['original_both'] else \"âŒ\"\n",
    "        status_pca = \"âœ…\" if example['pca_both'] else \"âŒ\"\n",
    "        print(f\"{i+1}. ì›ë³¸: {status_original} | PCA: {status_pca}\")\n",
    "        print(f\"   ì‹¤ì œ: Cat1={example['actual_cat1']}, Cat2={example['actual_cat2']}\")\n",
    "        print(f\"   í…ìŠ¤íŠ¸: {example['text']}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ê²°ë¡ : PCAë¥¼ í†µí•œ ì°¨ì› ì¶•ì†ŒëŠ” \")\n",
    "if best_both_accuracy > original_both_accuracy:\n",
    "    print(\"ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ë©ë‹ˆë‹¤. ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ëª¨ë‘ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"ì„±ëŠ¥ì„ ì•½ê°„ ì €í•˜ì‹œí‚¤ì§€ë§Œ, ê³„ì‚° íš¨ìœ¨ì„±ì€ í¬ê²Œ ê°œì„ ë©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "vb9xxoiufnl",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          16\n",
      "Memory Avail:       15.70 GB / 31.91 GB (49.2%)\n",
      "Disk Space Avail:   89.07 GB / 465.12 GB (19.1%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality_faster_train']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ AutoGluonìœ¼ë¡œ Category1 & Category2 ê°œë³„ ìµœì  ëª¨ë¸ íƒìƒ‰\n",
      "================================================================================\n",
      "âœ… AutoGluon ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ\n",
      "\n",
      "ğŸ“Š AutoGluonìš© ë°ì´í„° ì¤€ë¹„...\n",
      "í›ˆë ¨ ë°ì´í„° í¬ê¸°: (3359, 1026)\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: (664, 1024)\n",
      "Category1 í´ë˜ìŠ¤ ìˆ˜: 10\n",
      "Category2 í´ë˜ìŠ¤ ìˆ˜: 64\n",
      "\n",
      "ğŸ¤– Category1 AutoGluon ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
      "============================================================\n",
      "Category1 ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘... (ì˜ˆìƒ ì†Œìš”ì‹œê°„: 3-5ë¶„)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"c:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\autogluon_category1_model\"\n",
      "Train Data Rows:    3359\n",
      "Train Data Columns: 1024\n",
      "Label Column:       category1\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 9 out of 10 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9994045846978268\n",
      "Train Data Class Count: 9\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16070.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 26.23 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1024 | ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1024 | ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', ...]\n",
      "\t5.4s = Fit runtime\n",
      "\t1024 features in original data used to generate 1024 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.23 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 116.30s of the 174.50s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=1.80%)\n",
      "\t0.825\t = Validation score   (f1_macro)\n",
      "\t11.75s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 101.64s of the 159.83s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=2.80%)\n",
      "\t0.8157\t = Validation score   (f1_macro)\n",
      "\t75.29s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 23.27s of the 81.46s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=2.72%)\n",
      "\t0.7104\t = Validation score   (f1_macro)\n",
      "\t19.85s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 0.52s of the 58.71s of remaining time.\n",
      "\tWarning: Model is expected to require 10.7s to train, which exceeds the maximum time limit of 0.1s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 174.50s of the 57.65s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.467, 'LightGBM_BAG_L1': 0.333, 'LightGBMXT_BAG_L1': 0.2}\n",
      "\t0.8348\t = Validation score   (f1_macro)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 57.47s of the 57.46s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=1.87%)\n",
      "\t0.8343\t = Validation score   (f1_macro)\n",
      "\t14.53s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 38.90s of the 38.89s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=3.06%)\n",
      "\t0.8353\t = Validation score   (f1_macro)\n",
      "\t31.34s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 3.02s of the 3.01s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=2.95%)\n",
      "\t0.7595\t = Validation score   (f1_macro)\n",
      "\t3.36s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 174.50s of the -3.19s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.333, 'NeuralNetFastAI_BAG_L2': 0.333, 'LightGBMXT_BAG_L2': 0.333}\n",
      "\t0.8454\t = Validation score   (f1_macro)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 183.45s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1737.6 rows/s (1119 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\autogluon_category1_model\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Category1 AutoGluon ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“‹ Category1 ëª¨ë¸ ë¦¬ë”ë³´ë“œ:\n",
      "                    model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0     WeightedEnsemble_L3   0.845421    f1_macro       0.645323  152.975757   \n",
      "1       LightGBMXT_BAG_L2   0.835284    f1_macro       0.449279  138.235540   \n",
      "2     WeightedEnsemble_L2   0.834770    f1_macro       0.367259  107.058965   \n",
      "3  NeuralNetFastAI_BAG_L2   0.834273    f1_macro       0.558301  121.429098   \n",
      "4  NeuralNetFastAI_BAG_L1   0.824998    f1_macro       0.156036   11.753919   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.002001           0.207047            3       True   \n",
      "1                0.085021          31.339612            2       True   \n",
      "2                0.003001           0.163037            2       True   \n",
      "3                0.194043          14.533170            2       True   \n",
      "4                0.156036          11.753919            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          8  \n",
      "1          6  \n",
      "2          4  \n",
      "3          5  \n",
      "4          1  \n",
      "ğŸ† Category1 ìµœê³  ëª¨ë¸: WeightedEnsemble_L3 (F1: 0.8454)\n",
      "\n",
      "ğŸ¤– Category2 AutoGluon ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          16\n",
      "Memory Avail:       14.82 GB / 31.91 GB (46.4%)\n",
      "Disk Space Avail:   88.98 GB / 465.12 GB (19.1%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality_faster_train']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category2 ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘... (ì˜ˆìƒ ì†Œìš”ì‹œê°„: 3-5ë¶„)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"c:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\autogluon_category2_model\"\n",
      "Train Data Rows:    3359\n",
      "Train Data Columns: 1025\n",
      "Label Column:       category2\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 64\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    15207.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 26.49 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 1024 | ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', ...]\n",
      "\t\t('object', []) :    1 | ['predicted_category1']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :    1 | ['predicted_category1']\n",
      "\t\t('float', [])    : 1024 | ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', ...]\n",
      "\t5.6s = Fit runtime\n",
      "\t1025 features in original data used to generate 1025 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.25 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 116.16s of the 174.28s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=1.90%)\n",
      "\t0.6369\t = Validation score   (f1_macro)\n",
      "\t12.76s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 100.64s of the 158.76s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=6.59%)\n",
      "\t0.5429\t = Validation score   (f1_macro)\n",
      "\t82.51s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 15.11s of the 73.23s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=6.50%)\n",
      "\t0.3556\t = Validation score   (f1_macro)\n",
      "\t14.4s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 174.28s of the 55.98s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.429, 'NeuralNetFastAI_BAG_L1': 0.286, 'LightGBM_BAG_L1': 0.286}\n",
      "\t0.6532\t = Validation score   (f1_macro)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 55.69s of the 55.67s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=2.09%)\n",
      "\t0.6455\t = Validation score   (f1_macro)\n",
      "\t13.58s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 39.11s of the 39.09s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=7.23%)\n",
      "\t0.5409\t = Validation score   (f1_macro)\n",
      "\t33.34s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2.78s of the 2.77s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=7.15%)\n",
      "\t0.594\t = Validation score   (f1_macro)\n",
      "\t6.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 174.28s of the -6.70s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.333, 'NeuralNetFastAI_BAG_L2': 0.25, 'LightGBM_BAG_L2': 0.25, 'NeuralNetFastAI_BAG_L1': 0.167}\n",
      "\t0.667\t = Validation score   (f1_macro)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 187.11s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1613.9 rows/s (1120 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\autogluon_category2_model\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Category2 AutoGluon ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“‹ Category2 ëª¨ë¸ ë¦¬ë”ë³´ë“œ:\n",
      "                    model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0     WeightedEnsemble_L3   0.667043    f1_macro       0.696630  130.190227   \n",
      "1     WeightedEnsemble_L2   0.653207    f1_macro       0.448575  109.940131   \n",
      "2  NeuralNetFastAI_BAG_L2   0.645488    f1_macro       0.639618  123.253865   \n",
      "3  NeuralNetFastAI_BAG_L1   0.636916    f1_macro       0.168512   12.763404   \n",
      "4         LightGBM_BAG_L2   0.593968    f1_macro       0.498585  116.274353   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.004001           0.339079            3       True   \n",
      "1                0.003000           0.263061            2       True   \n",
      "2                0.194044          13.576795            2       True   \n",
      "3                0.168512          12.763404            1       True   \n",
      "4                0.053010           6.597283            2       True   \n",
      "\n",
      "   fit_order  \n",
      "0          8  \n",
      "1          4  \n",
      "2          5  \n",
      "3          1  \n",
      "4          7  \n",
      "ğŸ† Category2 ìµœê³  ëª¨ë¸: WeightedEnsemble_L3 (F1: 0.6670)\n",
      "\n",
      "ğŸ”„ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
      "============================================================\n",
      "AutoGluon ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰...\n",
      "ğŸ“ˆ Category1 ì˜ˆì¸¡ ì¤‘...\n",
      "ğŸ“ˆ Category2 ì˜ˆì¸¡ ì¤‘...\n",
      "âœ… AutoGluon ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“Š AutoGluon ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
      "============================================================\n",
      "Category1 ì •í™•ë„: 0.5226 (52.26%)\n",
      "Category2 ì •í™•ë„: 0.3283 (32.83%)\n",
      "\n",
      "ğŸ¯ í•µì‹¬ ì§€í‘œ - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ:\n",
      "ì •í™•ë„: 0.2500 (25.00%)\n",
      "ì •ë‹µ ê°œìˆ˜: 166/664\n",
      "\n",
      "ğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\n",
      "------------------------------------------------------------\n",
      "ëª¨ë¸                   Cat1 ì •í™•ë„     Cat2 ì •í™•ë„     ì¢…í•© ì •í™•ë„       ê°œì„         \n",
      "------------------------------------------------------------\n",
      "ê¸°ì¡´ XGBoost           0.4880       0.2425       0.2244       ê¸°ì¤€        \n",
      "AutoGluon            0.5226       0.3283       0.2500       +0.0256   \n",
      "\n",
      "ğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ì„±ëŠ¥ (Classification Report)\n",
      "================================================================================\n",
      "ğŸ”¹ Category1 ë¶„ë¥˜ ì„±ëŠ¥:\n",
      "F1-Score (macro): 0.3853\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ê¸°ì¨       0.75      0.71      0.73       188\n",
      "         ë‘ë ¤ì›€       0.90      0.26      0.41        72\n",
      "     ë¯¸ì›€(ìƒëŒ€ë°©)       0.35      0.63      0.45        43\n",
      "          ë¶„ë…¸       0.27      0.67      0.38        36\n",
      "          ì‚¬ë‘       0.72      0.28      0.40        47\n",
      "         ìˆ˜ì¹˜ì‹¬       0.50      0.16      0.24        25\n",
      "          ìŠ¬í””       0.64      0.65      0.65       117\n",
      "     ì‹«ì–´í•¨(ìƒíƒœ)       0.46      0.12      0.19        49\n",
      "          ìš•ë§       0.32      0.55      0.40        82\n",
      "          ì¤‘ë¦½       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.52       664\n",
      "   macro avg       0.49      0.40      0.39       664\n",
      "weighted avg       0.60      0.52      0.52       664\n",
      "\n",
      "\n",
      "ğŸ”¹ Category2 ë¶„ë¥˜ ì„±ëŠ¥:\n",
      "F1-Score (macro): 0.2147\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ê°ˆë“±       0.00      0.00      0.00         1\n",
      "          ê°ë™       0.48      0.33      0.39        30\n",
      "          ê±±ì •       0.36      0.22      0.27        23\n",
      "          ê²½ë©¸       0.38      0.44      0.41        18\n",
      "         ê³ ë§ˆì›€       0.58      0.79      0.67        19\n",
      "          ê³ í†µ       0.00      0.00      0.00         9\n",
      "          ê³µê°       0.33      0.14      0.20         7\n",
      "          ê³µí¬       1.00      0.06      0.11        17\n",
      "         ê¶ê¸ˆí•¨       0.21      0.67      0.32         9\n",
      "         ê·€ì¤‘í•¨       0.33      0.20      0.25         5\n",
      "         ê·¸ë¦¬ì›€       0.43      0.27      0.33        11\n",
      "         ê¸°ëŒ€ê°       0.29      0.39      0.33        18\n",
      "         ë‚œì²˜í•¨       0.33      0.07      0.12        14\n",
      "        ë‚ ì¹´ë¡œì›€       0.25      0.33      0.29         3\n",
      "          ëƒ‰ë‹´       0.00      0.00      0.00         3\n",
      "        ë„ˆê·¸ëŸ¬ì›€       0.00      0.00      0.00         1\n",
      "          ë†€ëŒ       0.54      0.42      0.47        31\n",
      "         ë‹¤ì •í•¨       0.40      0.22      0.29         9\n",
      "         ë‹µë‹µí•¨       0.62      0.28      0.38        18\n",
      "      ë™ì •(ìŠ¬í””)       0.55      0.59      0.57        27\n",
      "        ë‘ê·¼ê±°ë¦¼       0.00      0.00      0.00         2\n",
      "         ë§Œì¡±ê°       0.55      0.43      0.48        49\n",
      "         ë§¤ë ¥ì        0.50      0.08      0.13        13\n",
      "         ë¬´ê¸°ë ¥       0.42      0.36      0.38        14\n",
      "         ë¯¸ì•ˆí•¨       0.50      0.43      0.46         7\n",
      "         ë°˜ê°€ì›€       0.43      0.19      0.26        16\n",
      "          ë°˜ê°       0.00      0.00      0.00         5\n",
      "          ë°œì—´       0.00      0.00      0.00         3\n",
      "        ë¶€ë„ëŸ¬ì›€       0.40      0.12      0.19        16\n",
      "          ë¶ˆë§Œ       0.00      0.00      0.00         8\n",
      "         ë¶ˆì‹ ê°       0.83      0.62      0.71         8\n",
      "          ë¶ˆì¾Œ       0.17      0.55      0.27        20\n",
      "         ë¶ˆí¸í•¨       0.67      0.29      0.40         7\n",
      "        ë¹„ìœ„ìƒí•¨       0.00      0.00      0.00         1\n",
      "         ì‚¬ë‚˜ì›€       0.09      0.25      0.13         4\n",
      "         ìˆ˜ì¹˜ì‹¬       0.00      0.00      0.00         3\n",
      "         ì‹œê¸°ì‹¬       0.09      0.50      0.15         2\n",
      "         ì‹ ë¢°ê°       0.00      0.00      0.00         1\n",
      "         ì‹ ëª…ë‚¨       0.00      0.00      0.00         5\n",
      "          ì‹¤ë§       0.25      0.31      0.28        13\n",
      "          ì‹«ì¦       0.00      0.00      0.00         9\n",
      "         ì‹¬ì‹¬í•¨       0.00      0.00      0.00         1\n",
      "         ì•„ì‰¬ì›€       0.33      0.48      0.40        33\n",
      "          ì•„í””       0.00      0.00      0.00         3\n",
      "         ì•ˆì •ê°       0.18      0.25      0.21         8\n",
      "         ì–µìš¸í•¨       0.23      0.38      0.29        13\n",
      "         ì™¸ë¡œì›€       0.33      0.17      0.22         6\n",
      "          ì™¸ë©´       0.00      0.00      0.00         1\n",
      "          ìš•ì‹¬       0.24      0.33      0.28        18\n",
      "          ì›ë§       0.00      0.00      0.00         3\n",
      "         ìœ„ì¶•ê°       0.00      0.00      0.00         4\n",
      "       ìë‘ìŠ¤ëŸ¬ì›€       0.31      0.42      0.36        12\n",
      "         ìì‹ ê°       0.00      0.00      0.00         1\n",
      "          ì ˆë§       0.40      0.29      0.33         7\n",
      "         ì£„ì±…ê°       0.00      0.00      0.00         2\n",
      "         ì¦ê±°ì›€       0.41      0.48      0.44        27\n",
      "         ì´ˆì¡°í•¨       1.00      0.40      0.57         5\n",
      "         ì¹˜ì‚¬í•¨       0.04      0.20      0.07         5\n",
      "         íƒ€ì˜¤ë¦„       0.00      0.00      0.00         3\n",
      "         í†µì¾Œí•¨       0.00      0.00      0.00         1\n",
      "         í¸ì•ˆí•¨       0.00      0.00      0.00         4\n",
      "          í—ˆë§       0.50      0.18      0.27        11\n",
      "          í˜¸ê°       0.45      0.50      0.48        10\n",
      "          í›„íšŒ       0.57      0.57      0.57         7\n",
      "\n",
      "    accuracy                           0.33       664\n",
      "   macro avg       0.27      0.22      0.21       664\n",
      "weighted avg       0.39      0.33      0.32       664\n",
      "\n",
      "\n",
      "ğŸ” ì˜ˆì¸¡ ìƒ˜í”Œ ë¶„ì„ (ì²˜ìŒ 15ê°œ):\n",
      "====================================================================================================\n",
      " 1. âŒ Cat1: ê¸°ì¨ â†’ ê¸°ì¨ âœ“ | Cat2: ë§Œì¡±ê° â†’ ì¦ê±°ì›€ âœ—\n",
      "      í…ìŠ¤íŠ¸: ë³´ëŠ”ë™ì•ˆ ë„ˆë¬´ í–‰ë³µí–ˆê³  ì´ˆì½œë ›ì´ ë„ˆë¬´ ë¨¹ê³ ì‹¶ì—ˆê³  í‹°ëª¨ì‹œê°€ ì˜ìƒê²¼ê³  ìš¸ì–´!!í•˜ëŠ”ë¶€ë¶„ì´ ìˆì–´ì„œ...\n",
      "\n",
      " 2. âœ… Cat1: ê¸°ì¨ â†’ ê¸°ì¨ âœ“ | Cat2: ë§Œì¡±ê° â†’ ë§Œì¡±ê° âœ“\n",
      "      í…ìŠ¤íŠ¸: ì–´ë¦´ ë•Œ ê°€ ë³´ê³  ë¹•ìŠ¤ëŠ” ê±°ì˜ ì²˜ìŒì¸ë°(ê¸°ì–µì— ì—†ìŒ) ì§€ê¸ˆ ë”¸ê¸°ì¶•ì œ ê¸°ê°„ì´ë¼ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì‹...\n",
      "\n",
      " 3. âœ… Cat1: ê¸°ì¨ â†’ ê¸°ì¨ âœ“ | Cat2: ë§Œì¡±ê° â†’ ë§Œì¡±ê° âœ“\n",
      "      í…ìŠ¤íŠ¸: ë¯¸ë¦¬ ê³„ì¢Œë¡œ í™˜ì „í•´ë‘” ëˆì„ í•´ì™¸ì—ì„œ í™˜ì „ìˆ˜ìˆ˜ë£Œ ì—†ì´ ì¸ì¶œ ê°€ëŠ¥í•œ íŠ¸ë ˆë¸”ë¡œê·¸ë¼ëŠ” ì¹´ë“œì¸ë°, ...\n",
      "\n",
      " 4. âŒ Cat1: ìŠ¬í”” â†’ ì‹«ì–´í•¨(ìƒíƒœ) âœ— | Cat2: ë¬´ê¸°ë ¥ â†’ ë‚œì²˜í•¨ âœ—\n",
      "      í…ìŠ¤íŠ¸: ìš”ì¦˜ ë²ˆì•„ì›ƒë„ ìê¾¸ ì˜¬ë¼ì˜¤ê³  ë¬´ê¸°ë ¥í•´ì„œ ì¢…ê°•í•˜ê³  êµë¥˜í•˜ê¸°ë„ ë²„ê±°ìš´ ìƒíƒœê°€ ì™€ë¶€ë €ìœ¼ìš”ã… ã…  \n",
      "\n",
      " 5. âŒ Cat1: ê¸°ì¨ â†’ ë¯¸ì›€(ìƒëŒ€ë°©) âœ— | Cat2: ì¦ê±°ì›€ â†’ ì¹˜ì‚¬í•¨ âœ—\n",
      "      í…ìŠ¤íŠ¸: í¬ë¼ì„ì”¬ ì¥ë˜¥ë¯¼ì´ ë²”í–‰ ë„êµ¬ ì°¾ìœ¼ë ¤ê³  í™”ì¥ì‹¤ íƒ±í¬ ë’¤ì§€ëŠ”ë° ê±°ê¸°ì— ì§„ì§œ ë˜¥ ë„£ì–´ë†“ì€ ê±° ì§„...\n",
      "\n",
      " 6. âŒ Cat1: ì‹«ì–´í•¨(ìƒíƒœ) â†’ ë¶„ë…¸ âœ— | Cat2: ë‹µë‹µí•¨ â†’ ë‹µë‹µí•¨ âœ“\n",
      "      í…ìŠ¤íŠ¸: ê°€ìŠ´ì´ ë‹µë‹µí•´ì§ ì§„ì§œ ê°œë‹µë‹µí•´ì§\n",
      "ìš°ë¦¬ì§„ì§œíˆ¬í‘œì˜í•˜ì\n",
      "\n",
      " 7. âŒ Cat1: ê¸°ì¨ â†’ ìš•ë§ âœ— | Cat2: ë§Œì¡±ê° â†’ ì•„ì‰¬ì›€ âœ—\n",
      "      í…ìŠ¤íŠ¸: ì§€ê·¸ì¬ê·¸ë‘ ì—ì´ë¸”ë¦¬ë‘ í• ì¸ ëŒ€ê²°í•˜ë‚˜\n",
      "ì•„ íë­‡í•´\n",
      "ê³„ì†ë˜ê¸¸...\n",
      "ì˜ì›íˆ....\n",
      "\n",
      " 8. âŒ Cat1: ê¸°ì¨ â†’ ìš•ë§ âœ— | Cat2: ìë‘ìŠ¤ëŸ¬ì›€ â†’ ìš•ì‹¬ âœ—\n",
      "      í…ìŠ¤íŠ¸: ì²¨ìœ¼ë¡œ ìˆ˜ì œ ì´ˆì½œë¦¿ ë§Œë“¬\n",
      "ì´ˆì½œë¦¿ì„ 5ì‹œê°„ì´ë‚˜ ë§Œë“œëŠ” ì‚¬ëŒì´ ìˆë‹¤??? ê·¸ê²Œ ë°”ë¡œ ë‚˜\n",
      "\n",
      " 9. âŒ Cat1: ìŠ¬í”” â†’ ìŠ¬í”” âœ“ | Cat2: ì ˆë§ â†’ ë¬´ê¸°ë ¥ âœ—\n",
      "      í…ìŠ¤íŠ¸: ë‹¨í†¡ë°©ì— ê³µì§€ë“¤ ìŠ¬ìŠ¬ ì˜¬ë¼ì˜¤ëŠ”ê±° ë³´ë‹ˆê¹Œ ê³§ ê°œê°•ì´ë¼ëŠ”ê²Œ ì‹¤ê°ë‚˜ì„œ ê°‘ìê¸° ì¬ê¸°í•˜ê³ ì‹¶ê³  ì¸ìƒì´...\n",
      "\n",
      "10. âŒ Cat1: ê¸°ì¨ â†’ ë¯¸ì›€(ìƒëŒ€ë°©) âœ— | Cat2: ì¦ê±°ì›€ â†’ ì¹˜ì‚¬í•¨ âœ—\n",
      "      í…ìŠ¤íŠ¸: ì¥í¥ì‹  ê³µì†í•œ ì†ê°€ë½ì§ˆ ê°œì›ƒê²¨ìš”\n",
      "ì• ë“œë¦½ ë¯¸ì¹œê²ƒ ê°™ìŒ\n",
      "\n",
      "11. âŒ Cat1: ê¸°ì¨ â†’ ë¯¸ì›€(ìƒëŒ€ë°©) âœ— | Cat2: ë§Œì¡±ê° â†’ ë†€ëŒ âœ—\n",
      "      í…ìŠ¤íŠ¸: íšŒì‚¬ì‚¬ëŒì´ ì´ ëª…í•¨ì¼€ì´ìŠ¤ ì£¼ë©´ì„œ í¬ì¥ ëœ¯ì–´ë³´ë©´ ë„˜ êµ¬ë ¤ì„œ ê¹œì§ ë†€ë„ ê±°ë¼ê³  í–ˆëŠ”ë° ë‚˜......\n",
      "\n",
      "12. âŒ Cat1: ë¯¸ì›€(ìƒëŒ€ë°©) â†’ ë¶„ë…¸ âœ— | Cat2: ì¹˜ì‚¬í•¨ â†’ ì›ë§ âœ—\n",
      "      í…ìŠ¤íŠ¸: ë¯¸ì¹œìƒˆë¼ 4ê°•ì „ë§Œ ì•ˆì¢‹ì•˜ë˜ê±° ì•„ë‹ˆê³  ê·¸ì „ë¶€í„° ì•ˆì¢‹ì•˜ë˜ê±¸ ì‚¬ëŒë“¤ì´ ë‹¤ë´¤ëŠ”ë° ì§€ ì±…ì„ ì—†ë‹¤ê³  ...\n",
      "\n",
      "13. âŒ Cat1: ë‘ë ¤ì›€ â†’ ë¶„ë…¸ âœ— | Cat2: ê±±ì • â†’ ë¶ˆì¾Œ âœ—\n",
      "      í…ìŠ¤íŠ¸: ë‚˜ í˜„ì¥ì—ì„œ ì—„ì²­ í—¤ë§¬ ê±° ê°™ì€ë° ã… \n",
      "\n",
      "14. âŒ Cat1: ìˆ˜ì¹˜ì‹¬ â†’ ìš•ë§ âœ— | Cat2: ë¶€ë„ëŸ¬ì›€ â†’ ê¶ê¸ˆí•¨ âœ—\n",
      "      í…ìŠ¤íŠ¸: ë³´ë¼ìƒ‰ êµë³µ ì¹˜ë‹ˆê¹Œ ì´ëŸ°ê±°ë§Œ ëœ¨ëŠ”ë° ì§€ê¸ˆ ì´ê±¸ ì…ìœ¼ë¼ëŠ”ê±°ì˜ˆìš”?\n",
      "\n",
      "15. âŒ Cat1: ê¸°ì¨ â†’ ê¸°ì¨ âœ“ | Cat2: ë§Œì¡±ê° â†’ ì¦ê±°ì›€ âœ—\n",
      "      í…ìŠ¤íŠ¸: ìš°ì—°íˆ ë³´ê²Œ ëœ ì˜ìƒì¸ë°, ë…¸ë˜ê°€ ë„ˆë¬´ ì¢‹ì•„ì„œ í”Œë¦¬ì—ë„ ì¶”ê°€í•˜ê³ , ì¹´ì¹´ì˜¤í†¡ í”„ë®¤ë¡œë„ í•´ë†¨ìŒ...\n",
      "\n",
      "\n",
      "ğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­:\n",
      "============================================================\n",
      "âœ… AutoGluon ëª¨ë¸ì´ ê¸°ì¡´ë³´ë‹¤ 2.56%p í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ê¶Œì¥ì‚¬í•­: ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n",
      "\n",
      "ğŸ’¾ AutoGluon ëª¨ë¸ ì €ì¥ ìœ„ì¹˜:\n",
      "- Category1: autogluon_category1_model\n",
      "- Category2: autogluon_category2_model\n",
      "\n",
      "ğŸ“¥ ëª¨ë¸ ë¡œë“œ ë°©ë²•:\n",
      "from autogluon.tabular import TabularPredictor\n",
      "cat1_predictor = TabularPredictor.load('autogluon_category1_model')\n",
      "cat2_predictor = TabularPredictor.load('autogluon_category2_model')\n",
      "\n",
      "ğŸ”® ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡:\n",
      "cat1_pred = cat1_predictor.predict(new_data)\n",
      "new_data_with_cat1 = new_data.copy()\n",
      "new_data_with_cat1['predicted_category1'] = cat1_pred\n",
      "cat2_pred = cat2_predictor.predict(new_data_with_cat1)\n",
      "\n",
      "ğŸ¯ ìµœì¢… ì„±ê³¼:\n",
      "ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ì •ë‹µë¥ : 25.00% (166/664ê°œ)\n"
     ]
    }
   ],
   "source": [
    "# 14. AutoGluon ê°œë³„ ëª¨ë¸ - Category1 & Category2 ê°ê° ìµœì  ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€\n",
    "\n",
    "print(\"ğŸš€ AutoGluonìœ¼ë¡œ Category1 & Category2 ê°œë³„ ìµœì  ëª¨ë¸ íƒìƒ‰\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    from autogluon.tabular import TabularPredictor\n",
    "    print(\"âœ… AutoGluon ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ\")\n",
    "    autogluon_available = True\n",
    "except ImportError:\n",
    "    print(\"âŒ AutoGluonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:\")\n",
    "    print(\"pip install autogluon\")\n",
    "    autogluon_available = False\n",
    "    \n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. AutoGluonìš© ë°ì´í„° ì¤€ë¹„\n",
    "print(\"\\nğŸ“Š AutoGluonìš© ë°ì´í„° ì¤€ë¹„...\")\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„°: ë²¡í„°ë§Œ ì‚¬ìš© (ë¼ë²¨ ì œì™¸)\n",
    "train_features_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "train_features_df['category1'] = y  # re_category1 ì‚¬ìš©\n",
    "train_features_df['category2'] = data['re_category2'].values\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°: ë²¡í„°ë§Œ\n",
    "test_features_df = pd.DataFrame(test_X, columns=[f'feature_{i}' for i in range(test_X.shape[1])])\n",
    "\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {train_features_df.shape}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {test_features_df.shape}\")\n",
    "print(f\"Category1 í´ë˜ìŠ¤ ìˆ˜: {len(train_features_df['category1'].unique())}\")\n",
    "print(f\"Category2 í´ë˜ìŠ¤ ìˆ˜: {len(train_features_df['category2'].unique())}\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "cat1_save_path = \"autogluon_category1_model\"\n",
    "cat2_save_path = \"autogluon_category2_model\"\n",
    "\n",
    "# ê¸°ì¡´ ëª¨ë¸ í´ë” ì‚­ì œ\n",
    "for path in [cat1_save_path, cat2_save_path]:\n",
    "    if os.path.exists(path):\n",
    "        import shutil\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"ê¸°ì¡´ ëª¨ë¸ í´ë” {path} ì‚­ì œë¨\")\n",
    "\n",
    "# 2. Category1 ëª¨ë¸ í›ˆë ¨\n",
    "print(f\"\\nğŸ¤– Category1 AutoGluon ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cat1_predictor = None\n",
    "if autogluon_available:\n",
    "    try:\n",
    "        # Category1 ì˜ˆì¸¡ê¸° ìƒì„±\n",
    "        cat1_predictor = TabularPredictor(\n",
    "            label='category1',\n",
    "            path=cat1_save_path,\n",
    "            eval_metric='f1_macro',\n",
    "            problem_type='multiclass'\n",
    "        )\n",
    "        \n",
    "        # ëª¨ë¸ í›ˆë ¨\n",
    "        print(\"Category1 ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘... (ì˜ˆìƒ ì†Œìš”ì‹œê°„: 3-5ë¶„)\")\n",
    "        cat1_predictor = cat1_predictor.fit(\n",
    "            train_data=train_features_df.drop(['category2'], axis=1),\n",
    "            time_limit=180,  # 3ë¶„\n",
    "            presets='medium_quality_faster_train',\n",
    "            num_bag_folds=3,\n",
    "            num_bag_sets=1,\n",
    "            num_stack_levels=1\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Category1 AutoGluon ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "        \n",
    "        # Category1 ë¦¬ë”ë³´ë“œ\n",
    "        print(f\"\\nğŸ“‹ Category1 ëª¨ë¸ ë¦¬ë”ë³´ë“œ:\")\n",
    "        cat1_leaderboard = cat1_predictor.leaderboard()\n",
    "        print(cat1_leaderboard.head())\n",
    "        \n",
    "        cat1_best_model = cat1_leaderboard.iloc[0]['model']\n",
    "        cat1_best_score = cat1_leaderboard.iloc[0]['score_val']\n",
    "        print(f\"ğŸ† Category1 ìµœê³  ëª¨ë¸: {cat1_best_model} (F1: {cat1_best_score:.4f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Category1 AutoGluon í›ˆë ¨ ì‹¤íŒ¨: {str(e)}\")\n",
    "        cat1_predictor = None\n",
    "\n",
    "# 3. Category2 ëª¨ë¸ í›ˆë ¨\n",
    "print(f\"\\nğŸ¤– Category2 AutoGluon ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cat2_predictor = None\n",
    "if autogluon_available:\n",
    "    try:\n",
    "        # Category2 ì˜ˆì¸¡ê¸° ìƒì„±\n",
    "        cat2_predictor = TabularPredictor(\n",
    "            label='category2',\n",
    "            path=cat2_save_path,\n",
    "            eval_metric='f1_macro',\n",
    "            problem_type='multiclass'\n",
    "        )\n",
    "        \n",
    "        # Category1 ì˜ˆì¸¡ê°’ì„ íŠ¹ì„±ìœ¼ë¡œ ì¶”ê°€ (ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼í•œ ë°©ì‹)\n",
    "        if cat1_predictor is not None:\n",
    "            # í›ˆë ¨ ë°ì´í„°ì— Category1 ì˜ˆì¸¡ê°’ ì¶”ê°€\n",
    "            train_cat1_pred = cat1_predictor.predict(train_features_df.drop(['category1', 'category2'], axis=1))\n",
    "            train_enhanced_df = train_features_df.drop(['category1'], axis=1).copy()\n",
    "            train_enhanced_df['predicted_category1'] = train_cat1_pred\n",
    "        else:\n",
    "            # Category1 ì˜ˆì¸¡ê¸°ê°€ ì—†ìœ¼ë©´ ì‹¤ì œ Category1 ê°’ ì‚¬ìš©\n",
    "            train_enhanced_df = train_features_df.drop(['category1'], axis=1).copy()\n",
    "            train_enhanced_df['predicted_category1'] = train_features_df['category1']\n",
    "        \n",
    "        # ëª¨ë¸ í›ˆë ¨\n",
    "        print(\"Category2 ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘... (ì˜ˆìƒ ì†Œìš”ì‹œê°„: 3-5ë¶„)\")\n",
    "        cat2_predictor = cat2_predictor.fit(\n",
    "            train_data=train_enhanced_df,\n",
    "            time_limit=180,  # 3ë¶„\n",
    "            presets='medium_quality_faster_train',\n",
    "            num_bag_folds=3,\n",
    "            num_bag_sets=1,\n",
    "            num_stack_levels=1\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Category2 AutoGluon ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "        \n",
    "        # Category2 ë¦¬ë”ë³´ë“œ\n",
    "        print(f\"\\nğŸ“‹ Category2 ëª¨ë¸ ë¦¬ë”ë³´ë“œ:\")\n",
    "        cat2_leaderboard = cat2_predictor.leaderboard()\n",
    "        print(cat2_leaderboard.head())\n",
    "        \n",
    "        cat2_best_model = cat2_leaderboard.iloc[0]['model']\n",
    "        cat2_best_score = cat2_leaderboard.iloc[0]['score_val']\n",
    "        print(f\"ğŸ† Category2 ìµœê³  ëª¨ë¸: {cat2_best_model} (F1: {cat2_best_score:.4f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Category2 AutoGluon í›ˆë ¨ ì‹¤íŒ¨: {str(e)}\")\n",
    "        cat2_predictor = None\n",
    "\n",
    "# 4. XGBoost ëŒ€ì•ˆ ëª¨ë¸ (AutoGluon ì‹¤íŒ¨ì‹œ)\n",
    "if not autogluon_available or cat1_predictor is None or cat2_predictor is None:\n",
    "    print(f\"\\nğŸ”„ XGBoost ëŒ€ì•ˆ ëª¨ë¸ë¡œ ì§„í–‰...\")\n",
    "    \n",
    "    # Category1 XGBoost ëª¨ë¸\n",
    "    print(\"Category1 XGBoost ëª¨ë¸ í›ˆë ¨...\")\n",
    "    cat1_xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=10,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    cat1_xgb_model.fit(X, y_encoded)\n",
    "    \n",
    "    # Category2 XGBoost ëª¨ë¸ (Category1 í¬í•¨)\n",
    "    print(\"Category2 XGBoost ëª¨ë¸ í›ˆë ¨...\")\n",
    "    y_cat1_onehot_xgb = cat1_encoder.transform(y.reshape(-1, 1))\n",
    "    X_combined_xgb = np.hstack([X, y_cat1_onehot_xgb])\n",
    "    \n",
    "    cat2_xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=10,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    cat2_xgb_model.fit(X_combined_xgb, y_cat2_encoded)\n",
    "    \n",
    "    print(\"âœ… XGBoost ëŒ€ì•ˆ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "\n",
    "# 5. ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜\n",
    "print(f\"\\nğŸ”„ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ì‹¤ì œë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "# cat1_loaded_predictor = TabularPredictor.load(cat1_save_path)\n",
    "# cat2_loaded_predictor = TabularPredictor.load(cat2_save_path)\n",
    "\n",
    "if autogluon_available and cat1_predictor is not None and cat2_predictor is not None:\n",
    "    print(\"AutoGluon ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰...\")\n",
    "    \n",
    "    # Category1 ì˜ˆì¸¡\n",
    "    print(\"ğŸ“ˆ Category1 ì˜ˆì¸¡ ì¤‘...\")\n",
    "    test_pred_cat1_ag = cat1_predictor.predict(test_features_df)\n",
    "    test_pred_cat1_proba = cat1_predictor.predict_proba(test_features_df)\n",
    "    \n",
    "    # Category2 ì˜ˆì¸¡ (Category1 ì˜ˆì¸¡ê°’ í¬í•¨)\n",
    "    print(\"ğŸ“ˆ Category2 ì˜ˆì¸¡ ì¤‘...\")\n",
    "    test_enhanced_df = test_features_df.copy()\n",
    "    test_enhanced_df['predicted_category1'] = test_pred_cat1_ag\n",
    "    test_pred_cat2_ag = cat2_predictor.predict(test_enhanced_df)\n",
    "    test_pred_cat2_proba = cat2_predictor.predict_proba(test_enhanced_df)\n",
    "    \n",
    "    model_type = \"AutoGluon\"\n",
    "    \n",
    "else:\n",
    "    print(\"XGBoost ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰...\")\n",
    "    \n",
    "    # Category1 ì˜ˆì¸¡ (XGBoost)\n",
    "    test_pred_cat1_encoded_ag = cat1_xgb_model.predict(test_X)\n",
    "    test_pred_cat1_ag = le.inverse_transform(test_pred_cat1_encoded_ag)\n",
    "    \n",
    "    # Category2 ì˜ˆì¸¡ (XGBoost)\n",
    "    test_cat1_onehot_ag = cat1_encoder.transform(test_pred_cat1_ag.reshape(-1, 1))\n",
    "    test_X_combined_ag = np.hstack([test_X, test_cat1_onehot_ag])\n",
    "    test_pred_cat2_encoded_ag = cat2_xgb_model.predict(test_X_combined_ag)\n",
    "    test_pred_cat2_ag = le_cat2.inverse_transform(test_pred_cat2_encoded_ag)\n",
    "    \n",
    "    model_type = \"XGBoost ê³ ì„±ëŠ¥\"\n",
    "\n",
    "print(f\"âœ… {model_type} ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "\n",
    "# 6. ì„±ëŠ¥ í‰ê°€ - ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼í•œ ë°©ì‹\n",
    "print(f\"\\nğŸ“Š {model_type} ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ê°œë³„ ì¹´í…Œê³ ë¦¬ ì •í™•ë„\n",
    "cat1_accuracy_ag = (test_pred_cat1_ag == test_y_actual).mean()\n",
    "cat2_accuracy_ag = (test_pred_cat2_ag == test_y_actual_cat2).mean()\n",
    "\n",
    "print(f\"Category1 ì •í™•ë„: {cat1_accuracy_ag:.4f} ({cat1_accuracy_ag*100:.2f}%)\")\n",
    "print(f\"Category2 ì •í™•ë„: {cat2_accuracy_ag:.4f} ({cat2_accuracy_ag*100:.2f}%)\")\n",
    "\n",
    "# ğŸ¯ í•µì‹¬ ì§€í‘œ: ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ\n",
    "both_correct_ag = (test_pred_cat1_ag == test_y_actual) & (test_pred_cat2_ag == test_y_actual_cat2)\n",
    "both_accuracy_ag = both_correct_ag.mean()\n",
    "both_count_ag = both_correct_ag.sum()\n",
    "\n",
    "print(f\"\\nğŸ¯ í•µì‹¬ ì§€í‘œ - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ:\")\n",
    "print(f\"ì •í™•ë„: {both_accuracy_ag:.4f} ({both_accuracy_ag*100:.2f}%)\")\n",
    "print(f\"ì •ë‹µ ê°œìˆ˜: {both_count_ag}/{len(test_y_actual)}\")\n",
    "\n",
    "# 7. ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ê³¼ ì„±ëŠ¥ ë¹„êµ\n",
    "print(f\"\\nğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'ëª¨ë¸':<20} {'Cat1 ì •í™•ë„':<12} {'Cat2 ì •í™•ë„':<12} {'ì¢…í•© ì •í™•ë„':<12} {'ê°œì„ ':<10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# ê¸°ì¡´ ê²°ê³¼ ê³„ì‚°\n",
    "original_both_correct = (test_y_pred == test_y_actual) & (test_y_pred_cat2 == test_y_actual_cat2)\n",
    "original_both_accuracy = original_both_correct.mean()\n",
    "\n",
    "print(f\"{'ê¸°ì¡´ XGBoost':<20} {accuracy:<12.4f} {accuracy_cat2:<12.4f} {original_both_accuracy:<12.4f} {'ê¸°ì¤€':<10}\")\n",
    "\n",
    "# AutoGluon/XGBoost ê³ ì„±ëŠ¥ ê²°ê³¼\n",
    "improvement = both_accuracy_ag - original_both_accuracy\n",
    "improvement_text = f\"+{improvement:.4f}\" if improvement > 0 else f\"{improvement:.4f}\"\n",
    "\n",
    "print(f\"{model_type:<20} {cat1_accuracy_ag:<12.4f} {cat2_accuracy_ag:<12.4f} {both_accuracy_ag:<12.4f} {improvement_text:<10}\")\n",
    "\n",
    "# 8. ìƒì„¸ ë¶„ë¥˜ ì„±ëŠ¥ ë¶„ì„\n",
    "print(f\"\\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ì„±ëŠ¥ (Classification Report)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Category1 ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "print(\"ğŸ”¹ Category1 ë¶„ë¥˜ ì„±ëŠ¥:\")\n",
    "cat1_f1 = f1_score(test_y_actual, test_pred_cat1_ag, average='macro')\n",
    "print(f\"F1-Score (macro): {cat1_f1:.4f}\")\n",
    "cat1_report = classification_report(test_y_actual, test_pred_cat1_ag)\n",
    "print(cat1_report)\n",
    "\n",
    "print(\"\\nğŸ”¹ Category2 ë¶„ë¥˜ ì„±ëŠ¥:\")\n",
    "cat2_f1 = f1_score(test_y_actual_cat2, test_pred_cat2_ag, average='macro')\n",
    "print(f\"F1-Score (macro): {cat2_f1:.4f}\")\n",
    "cat2_report = classification_report(test_y_actual_cat2, test_pred_cat2_ag)\n",
    "print(cat2_report)\n",
    "\n",
    "# 9. ì˜ˆì¸¡ ìƒ˜í”Œ ë¶„ì„\n",
    "print(f\"\\nğŸ” ì˜ˆì¸¡ ìƒ˜í”Œ ë¶„ì„ (ì²˜ìŒ 15ê°œ):\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i in range(min(15, len(test_y_actual))):\n",
    "    text = test_texts[i][:50] + \"...\" if len(test_texts[i]) > 50 else test_texts[i]\n",
    "    \n",
    "    actual_cat1 = test_y_actual[i]\n",
    "    actual_cat2 = test_y_actual_cat2[i]\n",
    "    pred_cat1 = test_pred_cat1_ag[i]\n",
    "    pred_cat2 = test_pred_cat2_ag[i]\n",
    "    \n",
    "    cat1_match = actual_cat1 == pred_cat1\n",
    "    cat2_match = actual_cat2 == pred_cat2\n",
    "    both_match = cat1_match and cat2_match\n",
    "    \n",
    "    status = \"âœ…\" if both_match else \"âŒ\"\n",
    "    cat1_status = \"âœ“\" if cat1_match else \"âœ—\"\n",
    "    cat2_status = \"âœ“\" if cat2_match else \"âœ—\"\n",
    "    \n",
    "    print(f\"{i+1:2d}. {status} Cat1: {actual_cat1} â†’ {pred_cat1} {cat1_status} | Cat2: {actual_cat2} â†’ {pred_cat2} {cat2_status}\")\n",
    "    print(f\"      í…ìŠ¤íŠ¸: {text}\")\n",
    "    print()\n",
    "\n",
    "# 10. ê²°ë¡  ë° ëª¨ë¸ ì €ì¥ ì •ë³´\n",
    "print(f\"\\nğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if improvement > 0.01:\n",
    "    print(f\"âœ… {model_type} ëª¨ë¸ì´ ê¸°ì¡´ë³´ë‹¤ {improvement*100:.2f}%p í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(\"ê¶Œì¥ì‚¬í•­: ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "elif improvement > -0.01:\n",
    "    print(f\"ğŸ“Š {model_type} ëª¨ë¸ê³¼ ê¸°ì¡´ ëª¨ë¸ì´ ìœ ì‚¬í•©ë‹ˆë‹¤ (ì°¨ì´: {abs(improvement)*100:.2f}%p).\")\n",
    "    print(\"ê¶Œì¥ì‚¬í•­: ê³„ì‚° íš¨ìœ¨ì„±ì„ ê³ ë ¤í•˜ì—¬ ì„ íƒí•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(f\"âš ï¸ ê¸°ì¡´ ëª¨ë¸ì´ {abs(improvement)*100:.2f}%p ë” ì¢‹ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ê¶Œì¥ì‚¬í•­: ê¸°ì¡´ ëª¨ë¸ì„ ìœ ì§€í•˜ê±°ë‚˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.\")\n",
    "\n",
    "if autogluon_available and cat1_predictor is not None and cat2_predictor is not None:\n",
    "    print(f\"\\nğŸ’¾ AutoGluon ëª¨ë¸ ì €ì¥ ìœ„ì¹˜:\")\n",
    "    print(f\"- Category1: {cat1_save_path}\")\n",
    "    print(f\"- Category2: {cat2_save_path}\")\n",
    "    print(f\"\\nğŸ“¥ ëª¨ë¸ ë¡œë“œ ë°©ë²•:\")\n",
    "    print(f\"from autogluon.tabular import TabularPredictor\")\n",
    "    print(f\"cat1_predictor = TabularPredictor.load('{cat1_save_path}')\")\n",
    "    print(f\"cat2_predictor = TabularPredictor.load('{cat2_save_path}')\")\n",
    "    print(f\"\\nğŸ”® ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡:\")\n",
    "    print(f\"cat1_pred = cat1_predictor.predict(new_data)\")\n",
    "    print(f\"new_data_with_cat1 = new_data.copy()\")\n",
    "    print(f\"new_data_with_cat1['predicted_category1'] = cat1_pred\")\n",
    "    print(f\"cat2_pred = cat2_predictor.predict(new_data_with_cat1)\")\n",
    "else:\n",
    "    print(f\"\\nâš™ï¸ XGBoost ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"í•„ìš”ì‹œ pickleì„ ì‚¬ìš©í•˜ì—¬ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ìµœì¢… ì„±ê³¼:\")\n",
    "print(f\"ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ì •ë‹µë¥ : {both_accuracy_ag*100:.2f}% ({both_count_ag}/{len(test_y_actual)}ê°œ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ave9ukmaybv",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          16\n",
      "Memory Avail:       14.01 GB / 31.91 GB (43.9%)\n",
      "Disk Space Avail:   87.85 GB / 465.12 GB (18.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality_faster_train']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— AutoGluon ë³µí•© ë¼ë²¨ ë°©ì‹ - ì¹´í…Œê³ ë¦¬ ì¡°í•© ì˜ˆì¸¡\n",
      "================================================================================\n",
      "âœ… AutoGluon ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ\n",
      "\n",
      "ğŸ“Š ë³µí•© ë¼ë²¨ ë°ì´í„° ì¤€ë¹„...\n",
      "í›ˆë ¨ ë°ì´í„° í¬ê¸°: (3359, 1027)\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: (664, 1027)\n",
      "ë³µí•© ë¼ë²¨ ì¢…ë¥˜: 77ê°œ\n",
      "\n",
      "ğŸ“ˆ ë³µí•© ë¼ë²¨ ìƒìœ„ 15ê°œ ë¶„í¬:\n",
      "combined_label\n",
      "ìŠ¬í””_ë¬´ê¸°ë ¥      98\n",
      "ìˆ˜ì¹˜ì‹¬_ë¶€ë„ëŸ¬ì›€    96\n",
      "ê¸°ì¨_í¸ì•ˆí•¨      92\n",
      "ìŠ¬í””_ì™¸ë¡œì›€      90\n",
      "ë‘ë ¤ì›€_ë†€ëŒ      84\n",
      "ìŠ¬í””_í—ˆë§       78\n",
      "ê¸°ì¨_ì•ˆì •ê°      78\n",
      "ê¸°ì¨_ê³µê°       77\n",
      "ê¸°ì¨_ê¸°ëŒ€ê°      77\n",
      "ë¶„ë…¸_ë¶ˆì¾Œ       72\n",
      "ì‚¬ë‘_ë‘ê·¼ê±°ë¦¼     71\n",
      "ìŠ¬í””_ê·¸ë¦¬ì›€      67\n",
      "ê¸°ì¨_ê°ë™       67\n",
      "ì‚¬ë‘_ë‹¤ì •í•¨      65\n",
      "ê¸°ì¨_ê³ ë§ˆì›€      64\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âš ï¸ í¬ì†Œí•œ ë¼ë²¨ (8ê°œ): ['ì¤‘ë¦½_ê³µê°', 'ìˆ˜ì¹˜ì‹¬_ìˆ˜ì¹˜ì‹¬', 'ê¸°ì¨_ê·€ì¤‘í•¨', 'ë¯¸ì›€(ìƒëŒ€ë°©)_ë¶ˆì¾Œ', 'ìŠ¬í””_ì£„ì±…ê°', 'ìŠ¬í””_ë‹µë‹µí•¨', 'ì‚¬ë‘_ê³µê°', 'ìŠ¬í””_ê³µê°']\n",
      "ì´ëŸ¬í•œ ë¼ë²¨ë“¤ì€ í›ˆë ¨ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ¤– ë³µí•© ë¼ë²¨ AutoGluon ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
      "============================================================\n",
      "ë³µí•© ë¼ë²¨ ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘... (ì˜ˆìƒ ì†Œìš” ì‹œê°„: 5-10ë¶„)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"c:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\autogluon_combined_label_model\"\n",
      "Train Data Rows:    3359\n",
      "Train Data Columns: 1024\n",
      "Label Column:       combined_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 67 out of 77 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.993450431676094\n",
      "Train Data Class Count: 67\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14305.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 26.07 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1024 | ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1024 | ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', ...]\n",
      "\t5.2s = Fit runtime\n",
      "\t1024 features in original data used to generate 1024 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.07 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 196.37s of the 294.62s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=2.07%)\n",
      "\t0.5921\t = Validation score   (f1_macro)\n",
      "\t14.01s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 178.39s of the 276.64s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=7.34%)\n",
      "\t0.4963\t = Validation score   (f1_macro)\n",
      "\t144.92s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 30.09s of the 128.34s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=6.78%)\n",
      "\t0.3124\t = Validation score   (f1_macro)\n",
      "\t26.62s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 0.47s of the 98.72s of remaining time.\n",
      "\tWarning: Model is expected to require 26.4s to train, which exceeds the maximum time limit of 0.0s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 294.63s of the 97.45s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.75, 'LightGBM_BAG_L1': 0.188, 'LightGBMXT_BAG_L1': 0.062}\n",
      "\t0.5928\t = Validation score   (f1_macro)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 97.13s of the 97.12s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=2.19%)\n",
      "\t0.6119\t = Validation score   (f1_macro)\n",
      "\t13.61s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 80.30s of the 80.29s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=7.68%)\n",
      "\t0.5149\t = Validation score   (f1_macro)\n",
      "\t66.94s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 10.10s of the 10.08s of remaining time.\n",
      "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=5, gpus=0, memory=7.40%)\n",
      "\t0.5391\t = Validation score   (f1_macro)\n",
      "\t10.97s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 294.63s of the -4.02s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.458, 'LightGBMXT_BAG_L2': 0.333, 'NeuralNetFastAI_BAG_L1': 0.125, 'LightGBM_BAG_L2': 0.083}\n",
      "\t0.6228\t = Validation score   (f1_macro)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 304.47s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1147.3 rows/s (1113 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\user\\Desktop\\SKN_AFTER_STUDY\\autogluon_combined_label_model\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“‹ ë³µí•© ë¼ë²¨ AutoGluon ëª¨ë¸ ë¦¬ë”ë³´ë“œ:\n",
      "                    model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0     WeightedEnsemble_L3   0.622779    f1_macro       0.972064  277.438740   \n",
      "1  NeuralNetFastAI_BAG_L2   0.611854    f1_macro       0.776809  199.168023   \n",
      "2     WeightedEnsemble_L2   0.592764    f1_macro       0.559211  185.839937   \n",
      "3  NeuralNetFastAI_BAG_L1   0.592117    f1_macro       0.197045   14.014211   \n",
      "4         LightGBM_BAG_L2   0.539056    f1_macro       0.606224  196.524052   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.003000           0.368226            3       True   \n",
      "1                0.220598          13.610406            2       True   \n",
      "2                0.003000           0.282319            2       True   \n",
      "3                0.197045          14.014211            1       True   \n",
      "4                0.050013          10.966435            2       True   \n",
      "\n",
      "   fit_order  \n",
      "0          8  \n",
      "1          5  \n",
      "2          4  \n",
      "3          1  \n",
      "4          7  \n",
      "\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥ ë³µí•© ëª¨ë¸: WeightedEnsemble_L3\n",
      "ê²€ì¦ F1-Score: 0.6228\n",
      "\n",
      "ğŸ¯ ë³µí•© ë¼ë²¨ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰...\n",
      "============================================================\n",
      "AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ë¡œ ì˜ˆì¸¡...\n",
      "ë³µí•© ë¼ë²¨ì„ ê°œë³„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬...\n",
      "âœ… AutoGluon ë³µí•© ë³µí•© ë¼ë²¨ ì˜ˆì¸¡ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“Š AutoGluon ë³µí•© ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
      "============================================================\n",
      "Category1 ì •í™•ë„: 0.5572 (55.72%)\n",
      "Category2 ì •í™•ë„: 0.3630 (36.30%)\n",
      "\n",
      "ğŸ¯ í•µì‹¬ ì§€í‘œ - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ:\n",
      "ì •í™•ë„: 0.3404 (34.04%)\n",
      "ì •ë‹µ ê°œìˆ˜: 226/664\n",
      "ë³µí•© ë¼ë²¨ ì§ì ‘ ì •í™•ë„: 0.3404 (34.04%)\n",
      "ë³µí•© ë¼ë²¨ F1-Score (macro): 0.2190\n",
      "\n",
      "ğŸ“ˆ ëª¨ë¸ ë°©ì‹ ì¢…í•© ë¹„êµ:\n",
      "--------------------------------------------------------------------------------\n",
      "ë°©ì‹                   Cat1 ì •í™•ë„     Cat2 ì •í™•ë„     ì¢…í•© ì •í™•ë„       ê°œì„         \n",
      "--------------------------------------------------------------------------------\n",
      "ê¸°ì¡´ ê°œë³„ ëª¨ë¸             0.4880       0.2425       0.2244       ê¸°ì¤€        \n",
      "AutoGluon ë³µí•©         0.5572       0.3630       0.3404       +0.1160   \n",
      "\n",
      "ğŸ“‹ ë³µí•© ë¼ë²¨ Classification Report (ìƒìœ„ 20ê°œ í´ë˜ìŠ¤):\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ê¸°ì¨_ë§Œì¡±ê°       0.64      0.48      0.55        48\n",
      "      ìš•ë§_ì•„ì‰¬ì›€       0.64      0.42      0.51        33\n",
      "       ê¸°ì¨_ê°ë™       0.63      0.40      0.49        30\n",
      "      ê¸°ì¨_ì¦ê±°ì›€       0.52      0.59      0.55        27\n",
      "      ë‘ë ¤ì›€_ê±±ì •       0.60      0.26      0.36        23\n",
      "      ë‘ë ¤ì›€_ë†€ëŒ       0.67      0.17      0.28        23\n",
      "       ë¶„ë…¸_ë¶ˆì¾Œ       0.29      0.50      0.36        20\n",
      "   ìŠ¬í””_ë™ì •(ìŠ¬í””)       0.67      0.60      0.63        20\n",
      "      ê¸°ì¨_ê³ ë§ˆì›€       0.62      0.84      0.71        19\n",
      " ì‹«ì–´í•¨(ìƒíƒœ)_ë‹µë‹µí•¨       0.67      0.44      0.53        18\n",
      "       ìš•ë§_ìš•ì‹¬       0.50      0.44      0.47        18\n",
      "  ë¯¸ì›€(ìƒëŒ€ë°©)_ê²½ë©¸       0.52      0.61      0.56        18\n",
      "      ë‘ë ¤ì›€_ê³µí¬       1.00      0.29      0.45        17\n",
      "      ê¸°ì¨_ë°˜ê°€ì›€       0.67      0.12      0.21        16\n",
      "    ìˆ˜ì¹˜ì‹¬_ë¶€ë„ëŸ¬ì›€       0.50      0.06      0.11        16\n",
      " ì‹«ì–´í•¨(ìƒíƒœ)_ë‚œì²˜í•¨       0.20      0.07      0.11        14\n",
      "      ìŠ¬í””_ë¬´ê¸°ë ¥       0.89      0.57      0.70        14\n",
      "       ìŠ¬í””_ì‹¤ë§       0.38      0.23      0.29        13\n",
      "      ìš•ë§_ê¸°ëŒ€ê°       0.00      0.00      0.00        13\n",
      "      ìŠ¬í””_ì–µìš¸í•¨       0.46      0.46      0.46        13\n",
      "\n",
      "   micro avg       0.56      0.40      0.47       413\n",
      "   macro avg       0.55      0.38      0.42       413\n",
      "weighted avg       0.57      0.40      0.44       413\n",
      "\n",
      "\n",
      "ğŸ” ë³µí•© ë¼ë²¨ ì˜ˆì¸¡ ìƒ˜í”Œ ë¶„ì„ (ì²˜ìŒ 15ê°œ):\n",
      "====================================================================================================\n",
      " 1. âŒ ë³µí•©: ê¸°ì¨_ë§Œì¡±ê° â†’ ê¸°ì¨_ì¦ê±°ì›€\n",
      "      ê°œë³„: ê¸°ì¨|ë§Œì¡±ê° â†’ ê¸°ì¨|ì¦ê±°ì›€\n",
      "      í…ìŠ¤íŠ¸: ë³´ëŠ”ë™ì•ˆ ë„ˆë¬´ í–‰ë³µí–ˆê³  ì´ˆì½œë ›ì´ ë„ˆë¬´ ë¨¹ê³ ì‹¶ì—ˆê³  í‹°ëª¨ì‹œê°€ ì˜ìƒê²¼ê³  ìš¸ì–´!!í•˜ëŠ”ë¶€ë¶„ì´ ìˆì–´ì„œ...\n",
      "\n",
      " 2. âœ… ë³µí•©: ê¸°ì¨_ë§Œì¡±ê° â†’ ê¸°ì¨_ë§Œì¡±ê°\n",
      "      ê°œë³„: ê¸°ì¨|ë§Œì¡±ê° â†’ ê¸°ì¨|ë§Œì¡±ê°\n",
      "      í…ìŠ¤íŠ¸: ì–´ë¦´ ë•Œ ê°€ ë³´ê³  ë¹•ìŠ¤ëŠ” ê±°ì˜ ì²˜ìŒì¸ë°(ê¸°ì–µì— ì—†ìŒ) ì§€ê¸ˆ ë”¸ê¸°ì¶•ì œ ê¸°ê°„ì´ë¼ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì‹...\n",
      "\n",
      " 3. âœ… ë³µí•©: ê¸°ì¨_ë§Œì¡±ê° â†’ ê¸°ì¨_ë§Œì¡±ê°\n",
      "      ê°œë³„: ê¸°ì¨|ë§Œì¡±ê° â†’ ê¸°ì¨|ë§Œì¡±ê°\n",
      "      í…ìŠ¤íŠ¸: ë¯¸ë¦¬ ê³„ì¢Œë¡œ í™˜ì „í•´ë‘” ëˆì„ í•´ì™¸ì—ì„œ í™˜ì „ìˆ˜ìˆ˜ë£Œ ì—†ì´ ì¸ì¶œ ê°€ëŠ¥í•œ íŠ¸ë ˆë¸”ë¡œê·¸ë¼ëŠ” ì¹´ë“œì¸ë°, ...\n",
      "\n",
      " 4. âœ… ë³µí•©: ìŠ¬í””_ë¬´ê¸°ë ¥ â†’ ìŠ¬í””_ë¬´ê¸°ë ¥\n",
      "      ê°œë³„: ìŠ¬í””|ë¬´ê¸°ë ¥ â†’ ìŠ¬í””|ë¬´ê¸°ë ¥\n",
      "      í…ìŠ¤íŠ¸: ìš”ì¦˜ ë²ˆì•„ì›ƒë„ ìê¾¸ ì˜¬ë¼ì˜¤ê³  ë¬´ê¸°ë ¥í•´ì„œ ì¢…ê°•í•˜ê³  êµë¥˜í•˜ê¸°ë„ ë²„ê±°ìš´ ìƒíƒœê°€ ì™€ë¶€ë €ìœ¼ìš”ã… ã…  \n",
      "\n",
      " 5. âŒ ë³µí•©: ê¸°ì¨_ì¦ê±°ì›€ â†’ ë¶„ë…¸_ë¶ˆì¾Œ\n",
      "      ê°œë³„: ê¸°ì¨|ì¦ê±°ì›€ â†’ ë¶„ë…¸|ë¶ˆì¾Œ\n",
      "      í…ìŠ¤íŠ¸: í¬ë¼ì„ì”¬ ì¥ë˜¥ë¯¼ì´ ë²”í–‰ ë„êµ¬ ì°¾ìœ¼ë ¤ê³  í™”ì¥ì‹¤ íƒ±í¬ ë’¤ì§€ëŠ”ë° ê±°ê¸°ì— ì§„ì§œ ë˜¥ ë„£ì–´ë†“ì€ ê±° ì§„...\n",
      "\n",
      " 6. âœ… ë³µí•©: ì‹«ì–´í•¨(ìƒíƒœ)_ë‹µë‹µí•¨ â†’ ì‹«ì–´í•¨(ìƒíƒœ)_ë‹µë‹µí•¨\n",
      "      ê°œë³„: ì‹«ì–´í•¨(ìƒíƒœ)|ë‹µë‹µí•¨ â†’ ì‹«ì–´í•¨(ìƒíƒœ)|ë‹µë‹µí•¨\n",
      "      í…ìŠ¤íŠ¸: ê°€ìŠ´ì´ ë‹µë‹µí•´ì§ ì§„ì§œ ê°œë‹µë‹µí•´ì§\n",
      "ìš°ë¦¬ì§„ì§œíˆ¬í‘œì˜í•˜ì\n",
      "\n",
      " 7. âŒ ë³µí•©: ê¸°ì¨_ë§Œì¡±ê° â†’ ê¸°ì¨_ì¦ê±°ì›€\n",
      "      ê°œë³„: ê¸°ì¨|ë§Œì¡±ê° â†’ ê¸°ì¨|ì¦ê±°ì›€\n",
      "      í…ìŠ¤íŠ¸: ì§€ê·¸ì¬ê·¸ë‘ ì—ì´ë¸”ë¦¬ë‘ í• ì¸ ëŒ€ê²°í•˜ë‚˜\n",
      "ì•„ íë­‡í•´\n",
      "ê³„ì†ë˜ê¸¸...\n",
      "ì˜ì›íˆ....\n",
      "\n",
      " 8. âŒ ë³µí•©: ê¸°ì¨_ìë‘ìŠ¤ëŸ¬ì›€ â†’ ìš•ë§_ìš•ì‹¬\n",
      "      ê°œë³„: ê¸°ì¨|ìë‘ìŠ¤ëŸ¬ì›€ â†’ ìš•ë§|ìš•ì‹¬\n",
      "      í…ìŠ¤íŠ¸: ì²¨ìœ¼ë¡œ ìˆ˜ì œ ì´ˆì½œë¦¿ ë§Œë“¬\n",
      "ì´ˆì½œë¦¿ì„ 5ì‹œê°„ì´ë‚˜ ë§Œë“œëŠ” ì‚¬ëŒì´ ìˆë‹¤??? ê·¸ê²Œ ë°”ë¡œ ë‚˜\n",
      "\n",
      " 9. âŒ ë³µí•©: ìŠ¬í””_ì ˆë§ â†’ ìŠ¬í””_ë¬´ê¸°ë ¥\n",
      "      ê°œë³„: ìŠ¬í””|ì ˆë§ â†’ ìŠ¬í””|ë¬´ê¸°ë ¥\n",
      "      í…ìŠ¤íŠ¸: ë‹¨í†¡ë°©ì— ê³µì§€ë“¤ ìŠ¬ìŠ¬ ì˜¬ë¼ì˜¤ëŠ”ê±° ë³´ë‹ˆê¹Œ ê³§ ê°œê°•ì´ë¼ëŠ”ê²Œ ì‹¤ê°ë‚˜ì„œ ê°‘ìê¸° ì¬ê¸°í•˜ê³ ì‹¶ê³  ì¸ìƒì´...\n",
      "\n",
      "10. âŒ ë³µí•©: ê¸°ì¨_ì¦ê±°ì›€ â†’ ë¯¸ì›€(ìƒëŒ€ë°©)_ê²½ë©¸\n",
      "      ê°œë³„: ê¸°ì¨|ì¦ê±°ì›€ â†’ ë¯¸ì›€(ìƒëŒ€ë°©)|ê²½ë©¸\n",
      "      í…ìŠ¤íŠ¸: ì¥í¥ì‹  ê³µì†í•œ ì†ê°€ë½ì§ˆ ê°œì›ƒê²¨ìš”\n",
      "ì• ë“œë¦½ ë¯¸ì¹œê²ƒ ê°™ìŒ\n",
      "\n",
      "11. âŒ ë³µí•©: ê¸°ì¨_ë§Œì¡±ê° â†’ ë¯¸ì›€(ìƒëŒ€ë°©)_ì¹˜ì‚¬í•¨\n",
      "      ê°œë³„: ê¸°ì¨|ë§Œì¡±ê° â†’ ë¯¸ì›€(ìƒëŒ€ë°©)|ì¹˜ì‚¬í•¨\n",
      "      í…ìŠ¤íŠ¸: íšŒì‚¬ì‚¬ëŒì´ ì´ ëª…í•¨ì¼€ì´ìŠ¤ ì£¼ë©´ì„œ í¬ì¥ ëœ¯ì–´ë³´ë©´ ë„˜ êµ¬ë ¤ì„œ ê¹œì§ ë†€ë„ ê±°ë¼ê³  í–ˆëŠ”ë° ë‚˜......\n",
      "\n",
      "12. âŒ ë³µí•©: ë¯¸ì›€(ìƒëŒ€ë°©)_ì¹˜ì‚¬í•¨ â†’ ë¶„ë…¸_ì›ë§\n",
      "      ê°œë³„: ë¯¸ì›€(ìƒëŒ€ë°©)|ì¹˜ì‚¬í•¨ â†’ ë¶„ë…¸|ì›ë§\n",
      "      í…ìŠ¤íŠ¸: ë¯¸ì¹œìƒˆë¼ 4ê°•ì „ë§Œ ì•ˆì¢‹ì•˜ë˜ê±° ì•„ë‹ˆê³  ê·¸ì „ë¶€í„° ì•ˆì¢‹ì•˜ë˜ê±¸ ì‚¬ëŒë“¤ì´ ë‹¤ë´¤ëŠ”ë° ì§€ ì±…ì„ ì—†ë‹¤ê³  ...\n",
      "\n",
      "13. âŒ ë³µí•©: ë‘ë ¤ì›€_ê±±ì • â†’ ì‹«ì–´í•¨(ìƒíƒœ)_ë‹µë‹µí•¨\n",
      "      ê°œë³„: ë‘ë ¤ì›€|ê±±ì • â†’ ì‹«ì–´í•¨(ìƒíƒœ)|ë‹µë‹µí•¨\n",
      "      í…ìŠ¤íŠ¸: ë‚˜ í˜„ì¥ì—ì„œ ì—„ì²­ í—¤ë§¬ ê±° ê°™ì€ë° ã… \n",
      "\n",
      "14. âŒ ë³µí•©: ìˆ˜ì¹˜ì‹¬_ë¶€ë„ëŸ¬ì›€ â†’ ìš•ë§_ê°ˆë“±\n",
      "      ê°œë³„: ìˆ˜ì¹˜ì‹¬|ë¶€ë„ëŸ¬ì›€ â†’ ìš•ë§|ê°ˆë“±\n",
      "      í…ìŠ¤íŠ¸: ë³´ë¼ìƒ‰ êµë³µ ì¹˜ë‹ˆê¹Œ ì´ëŸ°ê±°ë§Œ ëœ¨ëŠ”ë° ì§€ê¸ˆ ì´ê±¸ ì…ìœ¼ë¼ëŠ”ê±°ì˜ˆìš”?\n",
      "\n",
      "15. âŒ ë³µí•©: ê¸°ì¨_ë§Œì¡±ê° â†’ ê¸°ì¨_ì¦ê±°ì›€\n",
      "      ê°œë³„: ê¸°ì¨|ë§Œì¡±ê° â†’ ê¸°ì¨|ì¦ê±°ì›€\n",
      "      í…ìŠ¤íŠ¸: ìš°ì—°íˆ ë³´ê²Œ ëœ ì˜ìƒì¸ë°, ë…¸ë˜ê°€ ë„ˆë¬´ ì¢‹ì•„ì„œ í”Œë¦¬ì—ë„ ì¶”ê°€í•˜ê³ , ì¹´ì¹´ì˜¤í†¡ í”„ë®¤ë¡œë„ í•´ë†¨ìŒ...\n",
      "\n",
      "\n",
      "ğŸ”¬ ë³µí•© ë¼ë²¨ ë°©ì‹ ë¶„ì„:\n",
      "============================================================\n",
      "ğŸ“ˆ ì¥ì :\n",
      "1. ì¹´í…Œê³ ë¦¬ ê°„ ì˜ì¡´ì„±ì„ ì§ì ‘ì ìœ¼ë¡œ í•™ìŠµ\n",
      "2. í•œ ë²ˆì˜ ì˜ˆì¸¡ìœ¼ë¡œ ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ê²°ì •\n",
      "3. ì¹´í…Œê³ ë¦¬ ì¡°í•©ì˜ ë¹ˆë„ë¥¼ ê³ ë ¤í•œ í•™ìŠµ\n",
      "\n",
      "ğŸ“‰ ë‹¨ì :\n",
      "1. í¬ì†Œí•œ ì¡°í•©ì— ëŒ€í•œ í•™ìŠµ ì–´ë ¤ì›€\n",
      "2. ìƒˆë¡œìš´ ì¡°í•© ì¶œí˜„ ì‹œ ëŒ€ì‘ ê³¤ë€\n",
      "3. í´ë˜ìŠ¤ ìˆ˜ ì¦ê°€ë¡œ ì¸í•œ ë³µì¡ë„ ìƒìŠ¹\n",
      "\n",
      "ğŸ’¡ ë³µí•© ë¼ë²¨ ë°©ì‹ ê²°ë¡ :\n",
      "âœ… ë³µí•© ë¼ë²¨ ë°©ì‹ì´ 11.60%p í–¥ìƒ! ì¹´í…Œê³ ë¦¬ ì˜ì¡´ì„±ì´ ê°•í•œ ê²½ìš° íš¨ê³¼ì \n",
      "\n",
      "ğŸ’¾ ë³µí•© ë¼ë²¨ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: autogluon_combined_label_model\n",
      "ğŸ“¥ ëª¨ë¸ ë¡œë“œ ë°©ë²•:\n",
      "combined_predictor = TabularPredictor.load('autogluon_combined_label_model')\n",
      "combined_pred = combined_predictor.predict(new_data)\n",
      "# ë³µí•© ë¼ë²¨ì„ ê°œë³„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬\n",
      "cat1_pred, cat2_pred = combined_pred.str.split('_', expand=True)[0], combined_pred.str.split('_', expand=True)[1]\n",
      "\n",
      "ğŸ¯ ë³µí•© ë¼ë²¨ ìµœì¢… ì„±ê³¼:\n",
      "ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ì •ë‹µë¥ : 34.04% (226/664ê°œ)\n",
      "ë³µí•© ë¼ë²¨ ì§ì ‘ ì •í™•ë„: 34.04%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mí˜„ì¬ ì…€ ë˜ëŠ” ì´ì „ ì…€ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ Kernelì´ ì¶©ëŒí–ˆìŠµë‹ˆë‹¤. \n",
      "\u001b[1;31mì…€ì˜ ì½”ë“œë¥¼ ê²€í† í•˜ì—¬ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ì›ì¸ì„ ì‹ë³„í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì„ ë³´ë ¤ë©´ <a href='https://aka.ms/vscodeJupyterKernelCrash'>ì—¬ê¸°</a>ë¥¼ í´ë¦­í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì€ Jupyter <a href='command:jupyter.viewOutput'>ë¡œê·¸</a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
     ]
    }
   ],
   "source": [
    "# 15. AutoGluon ë³µí•© ë¼ë²¨ ë°©ì‹ - ë‘ ì¹´í…Œê³ ë¦¬ ì¡°í•©ì„ í•˜ë‚˜ì˜ ë¼ë²¨ë¡œ ì˜ˆì¸¡\n",
    "\n",
    "print(\"ğŸ”— AutoGluon ë³µí•© ë¼ë²¨ ë°©ì‹ - ì¹´í…Œê³ ë¦¬ ì¡°í•© ì˜ˆì¸¡\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    from autogluon.tabular import TabularPredictor\n",
    "    print(\"âœ… AutoGluon ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ\")\n",
    "    autogluon_available = True\n",
    "except ImportError:\n",
    "    print(\"âŒ AutoGluonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:\")\n",
    "    print(\"pip install autogluon\")\n",
    "    autogluon_available = False\n",
    "    \n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. ë³µí•© ë¼ë²¨ ë°ì´í„° ì¤€ë¹„\n",
    "print(\"\\nğŸ“Š ë³µí•© ë¼ë²¨ ë°ì´í„° ì¤€ë¹„...\")\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„°: ë²¡í„° + ë³µí•© ë¼ë²¨\n",
    "train_combined_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "train_combined_df['category1'] = y\n",
    "train_combined_df['category2'] = data['re_category2'].values\n",
    "\n",
    "# ë‘ ì¹´í…Œê³ ë¦¬ë¥¼ ê²°í•©í•œ ë³µí•© ë¼ë²¨ ìƒì„± (ì˜ˆ: \"ê¸°ì¨_ê°ë™\", \"ìŠ¬í””_ì ˆë§\")\n",
    "train_combined_df['combined_label'] = train_combined_df['category1'] + \"_\" + train_combined_df['category2']\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°: ë²¡í„° + ì‹¤ì œ ë¼ë²¨  \n",
    "test_combined_df = pd.DataFrame(test_X, columns=[f'feature_{i}' for i in range(test_X.shape[1])])\n",
    "test_combined_df['category1'] = test_y_actual\n",
    "test_combined_df['category2'] = test_y_actual_cat2\n",
    "test_combined_df['combined_label'] = test_combined_df['category1'] + \"_\" + test_combined_df['category2']\n",
    "\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {train_combined_df.shape}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {test_combined_df.shape}\")\n",
    "print(f\"ë³µí•© ë¼ë²¨ ì¢…ë¥˜: {len(train_combined_df['combined_label'].unique())}ê°œ\")\n",
    "\n",
    "# ë³µí•© ë¼ë²¨ ë¶„í¬ í™•ì¸\n",
    "print(f\"\\nğŸ“ˆ ë³µí•© ë¼ë²¨ ìƒìœ„ 15ê°œ ë¶„í¬:\")\n",
    "label_counts = train_combined_df['combined_label'].value_counts()\n",
    "print(label_counts.head(15))\n",
    "\n",
    "# í¬ì†Œí•œ ë¼ë²¨ í™•ì¸\n",
    "rare_labels = label_counts[label_counts < 3]  # 3ê°œ ë¯¸ë§Œì¸ ë¼ë²¨\n",
    "if len(rare_labels) > 0:\n",
    "    print(f\"\\nâš ï¸ í¬ì†Œí•œ ë¼ë²¨ ({len(rare_labels)}ê°œ): {rare_labels.index.tolist()}\")\n",
    "    print(\"ì´ëŸ¬í•œ ë¼ë²¨ë“¤ì€ í›ˆë ¨ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 2. ë³µí•© ë¼ë²¨ AutoGluon ëª¨ë¸ í›ˆë ¨\n",
    "combined_save_path = \"autogluon_combined_label_model\"\n",
    "if os.path.exists(combined_save_path):\n",
    "    import shutil\n",
    "    shutil.rmtree(combined_save_path)\n",
    "    print(f\"ê¸°ì¡´ ë³µí•© ë¼ë²¨ ëª¨ë¸ í´ë” {combined_save_path} ì‚­ì œë¨\")\n",
    "\n",
    "print(f\"\\nğŸ¤– ë³µí•© ë¼ë²¨ AutoGluon ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "combined_predictor = None\n",
    "if autogluon_available:\n",
    "    try:\n",
    "        # ë³µí•© ë¼ë²¨ ì˜ˆì¸¡ê¸° ìƒì„±\n",
    "        combined_predictor = TabularPredictor(\n",
    "            label='combined_label', \n",
    "            path=combined_save_path,\n",
    "            eval_metric='f1_macro',  # F1 ì ìˆ˜ ìµœì í™”\n",
    "            problem_type='multiclass'\n",
    "        )\n",
    "        \n",
    "        # ëª¨ë¸ í›ˆë ¨ (ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ìë™ ì‹œë„)\n",
    "        print(\"ë³µí•© ë¼ë²¨ ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘... (ì˜ˆìƒ ì†Œìš” ì‹œê°„: 5-10ë¶„)\")\n",
    "        combined_predictor = combined_predictor.fit(\n",
    "            train_data=train_combined_df.drop(['category1', 'category2'], axis=1),\n",
    "            time_limit=300,  # 5ë¶„ ì œí•œ\n",
    "            presets='medium_quality_faster_train',\n",
    "            num_bag_folds=3,  # ì•™ìƒë¸”ì„ ìœ„í•œ í´ë“œ ìˆ˜\n",
    "            num_bag_sets=1,\n",
    "            num_stack_levels=1\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "        \n",
    "        # ë¦¬ë”ë³´ë“œ ì¶œë ¥\n",
    "        print(f\"\\nğŸ“‹ ë³µí•© ë¼ë²¨ AutoGluon ëª¨ë¸ ë¦¬ë”ë³´ë“œ:\")\n",
    "        combined_leaderboard = combined_predictor.leaderboard()\n",
    "        print(combined_leaderboard.head())\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´\n",
    "        best_combined_model = combined_leaderboard.iloc[0]['model']\n",
    "        best_combined_score = combined_leaderboard.iloc[0]['score_val']\n",
    "        print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ë³µí•© ëª¨ë¸: {best_combined_model}\")\n",
    "        print(f\"ê²€ì¦ F1-Score: {best_combined_score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ AutoGluon ë³µí•© ë¼ë²¨ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}\")\n",
    "        combined_predictor = None\n",
    "\n",
    "# 3. XGBoost ëŒ€ì•ˆ ëª¨ë¸ (AutoGluon ì‹¤íŒ¨ì‹œ)\n",
    "if not autogluon_available or combined_predictor is None:\n",
    "    print(f\"\\nğŸ”„ XGBoost ë³µí•© ë¼ë²¨ ëª¨ë¸ë¡œ ì§„í–‰...\")\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    # ë³µí•© ë¼ë²¨ ì¸ì½”ë”©\n",
    "    combined_le = LabelEncoder()\n",
    "    y_combined_encoded = combined_le.fit_transform(train_combined_df['combined_label'])\n",
    "    \n",
    "    print(f\"ë³µí•© ë¼ë²¨ ì¸ì½”ë”©: {len(combined_le.classes_)}ê°œ í´ë˜ìŠ¤\")\n",
    "    \n",
    "    # XGBoost ëª¨ë¸ í›ˆë ¨ (ë³µí•© ë¼ë²¨ìš© ê³ ì„±ëŠ¥ ì„¤ì •)\n",
    "    combined_xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=10,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"XGBoost ë³µí•© ë¼ë²¨ ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "    combined_xgb_model.fit(X, y_combined_encoded)\n",
    "    \n",
    "    print(\"âœ… XGBoost ë³µí•© ë¼ë²¨ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "\n",
    "# 4. ë³µí•© ë¼ë²¨ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "print(f\"\\nğŸ¯ ë³µí•© ë¼ë²¨ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if autogluon_available and combined_predictor is not None:\n",
    "    print(\"AutoGluon ë³µí•© ë¼ë²¨ ëª¨ë¸ë¡œ ì˜ˆì¸¡...\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "    test_pred_combined_labels = combined_predictor.predict(test_combined_df.drop(['category1', 'category2', 'combined_label'], axis=1))\n",
    "    test_pred_combined_proba = combined_predictor.predict_proba(test_combined_df.drop(['category1', 'category2', 'combined_label'], axis=1))\n",
    "    \n",
    "    model_type = \"AutoGluon ë³µí•©\"\n",
    "    \n",
    "else:\n",
    "    print(\"XGBoost ë³µí•© ë¼ë²¨ ëª¨ë¸ë¡œ ì˜ˆì¸¡...\")\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    test_pred_combined_encoded = combined_xgb_model.predict(test_X)\n",
    "    test_pred_combined_labels = combined_le.inverse_transform(test_pred_combined_encoded)\n",
    "    \n",
    "    model_type = \"XGBoost ë³µí•©\"\n",
    "\n",
    "# 5. ë³µí•© ë¼ë²¨ì„ ê°œë³„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬\n",
    "print(f\"ë³µí•© ë¼ë²¨ì„ ê°œë³„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬...\")\n",
    "\n",
    "# ì˜ˆì¸¡ëœ ë³µí•© ë¼ë²¨ì„ ê°œë³„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬\n",
    "if isinstance(test_pred_combined_labels, pd.Series):\n",
    "    test_pred_split = test_pred_combined_labels.str.split('_', expand=True)\n",
    "elif isinstance(test_pred_combined_labels, np.ndarray):\n",
    "    test_pred_split = pd.Series(test_pred_combined_labels).str.split('_', expand=True)\n",
    "\n",
    "test_pred_cat1_combined = test_pred_split[0].values\n",
    "test_pred_cat2_combined = test_pred_split[1].values\n",
    "\n",
    "print(f\"âœ… {model_type} ë³µí•© ë¼ë²¨ ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "\n",
    "# 6. ì„±ëŠ¥ í‰ê°€\n",
    "print(f\"\\nğŸ“Š {model_type} ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ê°œë³„ ì¹´í…Œê³ ë¦¬ ì •í™•ë„\n",
    "cat1_accuracy_combined = (test_pred_cat1_combined == test_y_actual).mean()\n",
    "cat2_accuracy_combined = (test_pred_cat2_combined == test_y_actual_cat2).mean()\n",
    "\n",
    "print(f\"Category1 ì •í™•ë„: {cat1_accuracy_combined:.4f} ({cat1_accuracy_combined*100:.2f}%)\")\n",
    "print(f\"Category2 ì •í™•ë„: {cat2_accuracy_combined:.4f} ({cat2_accuracy_combined*100:.2f}%)\")\n",
    "\n",
    "# ğŸ¯ í•µì‹¬ ì§€í‘œ: ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ (ë³µí•© ë¼ë²¨ì˜ í•µì‹¬ ì¥ì )\n",
    "both_correct_combined = (test_pred_cat1_combined == test_y_actual) & (test_pred_cat2_combined == test_y_actual_cat2)\n",
    "both_accuracy_combined = both_correct_combined.mean()\n",
    "both_count_combined = both_correct_combined.sum()\n",
    "\n",
    "print(f\"\\nğŸ¯ í•µì‹¬ ì§€í‘œ - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ:\")\n",
    "print(f\"ì •í™•ë„: {both_accuracy_combined:.4f} ({both_accuracy_combined*100:.2f}%)\")\n",
    "print(f\"ì •ë‹µ ê°œìˆ˜: {both_count_combined}/{len(test_y_actual)}\")\n",
    "\n",
    "# ë³µí•© ë¼ë²¨ ì§ì ‘ ë¹„êµ (ê°€ì¥ ì •í™•í•œ ì§€í‘œ)\n",
    "test_actual_combined_labels = test_combined_df['combined_label'].values\n",
    "combined_direct_accuracy = (test_pred_combined_labels == test_actual_combined_labels).mean()\n",
    "print(f\"ë³µí•© ë¼ë²¨ ì§ì ‘ ì •í™•ë„: {combined_direct_accuracy:.4f} ({combined_direct_accuracy*100:.2f}%)\")\n",
    "\n",
    "# F1-Score ê³„ì‚° (ë³µí•© ë¼ë²¨ ê¸°ì¤€)\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "f1_combined_direct = f1_score(test_actual_combined_labels, test_pred_combined_labels, average='macro')\n",
    "print(f\"ë³µí•© ë¼ë²¨ F1-Score (macro): {f1_combined_direct:.4f}\")\n",
    "\n",
    "# 7. ëª¨ë“  ë°©ì‹ ì„±ëŠ¥ ë¹„êµ\n",
    "print(f\"\\nğŸ“ˆ ëª¨ë¸ ë°©ì‹ ì¢…í•© ë¹„êµ:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'ë°©ì‹':<20} {'Cat1 ì •í™•ë„':<12} {'Cat2 ì •í™•ë„':<12} {'ì¢…í•© ì •í™•ë„':<12} {'ê°œì„ ':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ê¸°ì¡´ ê°œë³„ ëª¨ë¸ ë°©ì‹\n",
    "original_both_accuracy = (test_y_pred == test_y_actual) & (test_y_pred_cat2 == test_y_actual_cat2)\n",
    "original_both_accuracy = original_both_accuracy.mean()\n",
    "print(f\"{'ê¸°ì¡´ ê°œë³„ ëª¨ë¸':<20} {accuracy:<12.4f} {accuracy_cat2:<12.4f} {original_both_accuracy:<12.4f} {'ê¸°ì¤€':<10}\")\n",
    "\n",
    "# ë³µí•© ë¼ë²¨ ë°©ì‹\n",
    "improvement_combined = both_accuracy_combined - original_both_accuracy\n",
    "improvement_text_combined = f\"+{improvement_combined:.4f}\" if improvement_combined > 0 else f\"{improvement_combined:.4f}\"\n",
    "print(f\"{model_type:<20} {cat1_accuracy_combined:<12.4f} {cat2_accuracy_combined:<12.4f} {both_accuracy_combined:<12.4f} {improvement_text_combined:<10}\")\n",
    "\n",
    "# 8. ë³µí•© ë¼ë²¨ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ (ìƒìœ„ í´ë˜ìŠ¤)\n",
    "print(f\"\\nğŸ“‹ ë³µí•© ë¼ë²¨ Classification Report (ìƒìœ„ 20ê°œ í´ë˜ìŠ¤):\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ê°€ì¥ ë¹ˆë²ˆí•œ ë³µí•© ë¼ë²¨ë“¤ë§Œ ë³´ê³ ì„œ ìƒì„±\n",
    "top_combined_labels = pd.Series(test_actual_combined_labels).value_counts().head(20).index.tolist()\n",
    "mask_combined = pd.Series(test_actual_combined_labels).isin(top_combined_labels)\n",
    "\n",
    "if mask_combined.sum() > 0:\n",
    "    filtered_actual_combined = pd.Series(test_actual_combined_labels)[mask_combined]\n",
    "    filtered_pred_combined = pd.Series(test_pred_combined_labels)[mask_combined]\n",
    "    \n",
    "    combined_report = classification_report(\n",
    "        filtered_actual_combined, \n",
    "        filtered_pred_combined, \n",
    "        target_names=top_combined_labels, \n",
    "        labels=top_combined_labels,\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(combined_report)\n",
    "\n",
    "# 9. ë³µí•© ë¼ë²¨ ì˜ˆì¸¡ ìƒ˜í”Œ ë¶„ì„\n",
    "print(f\"\\nğŸ” ë³µí•© ë¼ë²¨ ì˜ˆì¸¡ ìƒ˜í”Œ ë¶„ì„ (ì²˜ìŒ 15ê°œ):\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i in range(min(15, len(test_y_actual))):\n",
    "    text = test_texts[i][:50] + \"...\" if len(test_texts[i]) > 50 else test_texts[i]\n",
    "    \n",
    "    actual_combined = test_actual_combined_labels[i]\n",
    "    pred_combined = test_pred_combined_labels[i]\n",
    "    \n",
    "    # ê°œë³„ ì¹´í…Œê³ ë¦¬ ë¶„ë¦¬\n",
    "    actual_cat1 = test_y_actual[i]\n",
    "    actual_cat2 = test_y_actual_cat2[i]\n",
    "    pred_cat1 = test_pred_cat1_combined[i]\n",
    "    pred_cat2 = test_pred_cat2_combined[i]\n",
    "    \n",
    "    # ë§¤ì¹­ ìƒíƒœ\n",
    "    combined_match = actual_combined == pred_combined\n",
    "    both_match = (actual_cat1 == pred_cat1) and (actual_cat2 == pred_cat2)\n",
    "    \n",
    "    status = \"âœ…\" if combined_match else \"âŒ\"\n",
    "    \n",
    "    print(f\"{i+1:2d}. {status} ë³µí•©: {actual_combined} â†’ {pred_combined}\")\n",
    "    print(f\"      ê°œë³„: {actual_cat1}|{actual_cat2} â†’ {pred_cat1}|{pred_cat2}\")\n",
    "    print(f\"      í…ìŠ¤íŠ¸: {text}\")\n",
    "    print()\n",
    "\n",
    "# 10. ë³µí•© ë¼ë²¨ ë°©ì‹ì˜ ì¥ë‹¨ì  ë¶„ì„\n",
    "print(f\"\\nğŸ”¬ ë³µí•© ë¼ë²¨ ë°©ì‹ ë¶„ì„:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"ğŸ“ˆ ì¥ì :\")\n",
    "print(f\"1. ì¹´í…Œê³ ë¦¬ ê°„ ì˜ì¡´ì„±ì„ ì§ì ‘ì ìœ¼ë¡œ í•™ìŠµ\")\n",
    "print(f\"2. í•œ ë²ˆì˜ ì˜ˆì¸¡ìœ¼ë¡œ ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ê²°ì •\")\n",
    "print(f\"3. ì¹´í…Œê³ ë¦¬ ì¡°í•©ì˜ ë¹ˆë„ë¥¼ ê³ ë ¤í•œ í•™ìŠµ\")\n",
    "\n",
    "print(f\"\\nğŸ“‰ ë‹¨ì :\")\n",
    "print(f\"1. í¬ì†Œí•œ ì¡°í•©ì— ëŒ€í•œ í•™ìŠµ ì–´ë ¤ì›€\")\n",
    "print(f\"2. ìƒˆë¡œìš´ ì¡°í•© ì¶œí˜„ ì‹œ ëŒ€ì‘ ê³¤ë€\")\n",
    "print(f\"3. í´ë˜ìŠ¤ ìˆ˜ ì¦ê°€ë¡œ ì¸í•œ ë³µì¡ë„ ìƒìŠ¹\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ë³µí•© ë¼ë²¨ ë°©ì‹ ê²°ë¡ :\")\n",
    "if improvement_combined > 0.01:\n",
    "    print(f\"âœ… ë³µí•© ë¼ë²¨ ë°©ì‹ì´ {improvement_combined*100:.2f}%p í–¥ìƒ! ì¹´í…Œê³ ë¦¬ ì˜ì¡´ì„±ì´ ê°•í•œ ê²½ìš° íš¨ê³¼ì \")\n",
    "elif improvement_combined > -0.01:\n",
    "    print(f\"ğŸ“Š ê°œë³„ ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥. ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ì„ íƒ\")\n",
    "else:\n",
    "    print(f\"âš ï¸ ê°œë³„ ëª¨ë¸ì´ {abs(improvement_combined)*100:.2f}%p ë” ì¢‹ìŒ. í¬ì†Œí•œ ì¡°í•© ë•Œë¬¸ì¼ ê°€ëŠ¥ì„±\")\n",
    "\n",
    "if autogluon_available and combined_predictor is not None:\n",
    "    print(f\"\\nğŸ’¾ ë³µí•© ë¼ë²¨ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {combined_save_path}\")\n",
    "    print(f\"ğŸ“¥ ëª¨ë¸ ë¡œë“œ ë°©ë²•:\")\n",
    "    print(f\"combined_predictor = TabularPredictor.load('{combined_save_path}')\")\n",
    "    print(f\"combined_pred = combined_predictor.predict(new_data)\")\n",
    "    print(f\"# ë³µí•© ë¼ë²¨ì„ ê°œë³„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬\")\n",
    "    print(f\"cat1_pred, cat2_pred = combined_pred.str.split('_', expand=True)[0], combined_pred.str.split('_', expand=True)[1]\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ë³µí•© ë¼ë²¨ ìµœì¢… ì„±ê³¼:\")\n",
    "print(f\"ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ì •ë‹µë¥ : {both_accuracy_combined*100:.2f}% ({both_count_combined}/{len(test_y_actual)}ê°œ)\")\n",
    "print(f\"ë³µí•© ë¼ë²¨ ì§ì ‘ ì •í™•ë„: {combined_direct_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb77oepa1tu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ”¬ ë³µí•© ë¼ë²¨ ë°©ì‹: ë¡œì§€ìŠ¤í‹± íšŒê·€ vs SVM Linear ë¹„êµ\n",
      "================================================================================\n",
      "ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì¤‘...\n",
      "ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²¡í„° ìƒì„± ì¤‘...\n",
      "í›ˆë ¨ ë°ì´í„°: 3359ê°œ\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„°: 664ê°œ\n",
      "ë²¡í„° ì°¨ì›: 1024ì°¨ì›\n",
      "ë³µí•© ë¼ë²¨ ì¢…ë¥˜: 77ê°œ\n",
      "í›ˆë ¨ ë°ì´í„° ê³ ìœ  ë¼ë²¨: 77ê°œ\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ê³ ìœ  ë¼ë²¨: 70ê°œ\n",
      "ê³µí†µ ë¼ë²¨: 68ê°œ\n",
      "í…ŒìŠ¤íŠ¸ì—ë§Œ ìˆëŠ” ë¼ë²¨: 2ê°œ\n",
      "ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ë¼ë²¨ë“¤: ['ì¤‘ë¦½_ë†€ëŒ', 'ì¤‘ë¦½_ë§Œì¡±ê°']...\n",
      "í•„í„°ë§ í›„ í›ˆë ¨ ë°ì´í„°: 3335ê°œ\n",
      "í•„í„°ë§ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°: 660ê°œ\n",
      "ì¸ì½”ë”©ëœ ë¼ë²¨ ì¢…ë¥˜: 68\n",
      "\n",
      "ğŸ¤– ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í›ˆë ¨ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\skn_after_study\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– SVM Linear ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¸ ë¡œì§€ìŠ¤í‹± íšŒê·€ ê²°ê³¼:\n",
      "  - ë³µí•© ë¼ë²¨ ì •í™•ë„: 0.2939 (29.39%)\n",
      "  - Category1 ì •í™•ë„: 0.5030 (50.30%)\n",
      "  - Category2 ì •í™•ë„: 0.3106 (31.06%)\n",
      "  - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: 194/660 (29.39%)\n",
      "  - F1-Score (Macro): 0.1735\n",
      "  - F1-Score (Weighted): 0.2628\n",
      "\n",
      "ğŸ”¸ SVM Linear ê²°ê³¼:\n",
      "  - ë³µí•© ë¼ë²¨ ì •í™•ë„: 0.3197 (31.97%)\n",
      "  - Category1 ì •í™•ë„: 0.5303 (53.03%)\n",
      "  - Category2 ì •í™•ë„: 0.3394 (33.94%)\n",
      "  - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: 211/660 (31.97%)\n",
      "  - F1-Score (Macro): 0.1995\n",
      "  - F1-Score (Weighted): 0.2982\n",
      "\n",
      "ğŸ† ì„±ëŠ¥ ë¹„êµ:\n",
      "  - ë³µí•© ë¼ë²¨ ì •í™•ë„ê°€ ë” ë†’ì€ ëª¨ë¸: SVM Linear\n",
      "  - ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ì •ë‹µë¥ ì´ ë” ë†’ì€ ëª¨ë¸: SVM Linear\n",
      "  - F1-Score (Macro)ê°€ ë” ë†’ì€ ëª¨ë¸: SVM Linear\n",
      "\n",
      "ğŸ“ˆ ì„±ëŠ¥ ì°¨ì´:\n",
      "  - ë³µí•© ë¼ë²¨ ì •í™•ë„ ì°¨ì´: 0.0258 (2.58%p)\n",
      "  - ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ì •ë‹µë¥  ì°¨ì´: 0.0258 (2.58%p)\n",
      "  - F1-Score ì°¨ì´: 0.0259\n",
      "\n",
      "ğŸ“ ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ (ì²˜ìŒ 10ê°œ):\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "No. ì‹¤ì œ Cat1      ì‹¤ì œ Cat2      LR Cat1      LR Cat2      SVM Cat1     SVM Cat2     LR ì •ë‹µ    SVM ì •ë‹µ  \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "1   ê¸°ì¨           ë§Œì¡±ê°          ê¸°ì¨           ì¦ê±°ì›€          ê¸°ì¨           ë§Œì¡±ê°          âœ—        âœ“       \n",
      "2   ê¸°ì¨           ë§Œì¡±ê°          ê¸°ì¨           ë§Œì¡±ê°          ê¸°ì¨           ë§Œì¡±ê°          âœ“        âœ“       \n",
      "3   ê¸°ì¨           ë§Œì¡±ê°          ê¸°ì¨           ë§Œì¡±ê°          ê¸°ì¨           ë§Œì¡±ê°          âœ“        âœ“       \n",
      "4   ìŠ¬í””           ë¬´ê¸°ë ¥          ì‹«ì–´í•¨(ìƒíƒœ)      ë‹µë‹µí•¨          ì‹«ì–´í•¨(ìƒíƒœ)      ë‚œì²˜í•¨          âœ—        âœ—       \n",
      "5   ê¸°ì¨           ì¦ê±°ì›€          ë¯¸ì›€(ìƒëŒ€ë°©)      ë¹„ìœ„ìƒí•¨         ë¯¸ì›€(ìƒëŒ€ë°©)      ë¹„ìœ„ìƒí•¨         âœ—        âœ—       \n",
      "6   ì‹«ì–´í•¨(ìƒíƒœ)      ë‹µë‹µí•¨          ì‹«ì–´í•¨(ìƒíƒœ)      ë‹µë‹µí•¨          ì‹«ì–´í•¨(ìƒíƒœ)      ë‹µë‹µí•¨          âœ“        âœ“       \n",
      "7   ê¸°ì¨           ë§Œì¡±ê°          ê¸°ì¨           ì¦ê±°ì›€          ìš•ë§           ì•„ì‰¬ì›€          âœ—        âœ—       \n",
      "8   ê¸°ì¨           ìë‘ìŠ¤ëŸ¬ì›€        ìš•ë§           ê¶ê¸ˆí•¨          ìš•ë§           ìš•ì‹¬           âœ—        âœ—       \n",
      "9   ìŠ¬í””           ì ˆë§           ê¸°ì¨           ê¸°ëŒ€ê°          ìŠ¬í””           ë¬´ê¸°ë ¥          âœ—        âœ—       \n",
      "10  ê¸°ì¨           ì¦ê±°ì›€          ë¯¸ì›€(ìƒëŒ€ë°©)      ê²½ë©¸           ë¯¸ì›€(ìƒëŒ€ë°©)      ê²½ë©¸           âœ—        âœ—       \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ë³µí•© ë¼ë²¨ ë°©ì‹ìœ¼ë¡œ ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ SVM Linear ëª¨ë¸ ë¹„êµ\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ”¬ ë³µí•© ë¼ë²¨ ë°©ì‹: ë¡œì§€ìŠ¤í‹± íšŒê·€ vs SVM Linear ë¹„êµ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. ë°ì´í„° ì¤€ë¹„ (ê¸°ì¡´ ì„ë² ë”© ë²¡í„° ì‚¬ìš©)\n",
    "print(\"ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ (data ë³€ìˆ˜ì˜ ì„ë² ë”© ë²¡í„° ì‚¬ìš©)\n",
    "X_train = np.vstack(data['vector'].values)  # 1024ì°¨ì› ì„ë² ë”© ë²¡í„°\n",
    "train_cat1 = data['re_category1'].values\n",
    "train_cat2 = data['re_category2'].values\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (test_dataì˜ ì„ë² ë”© ë²¡í„° ìƒì„±)\n",
    "print(\"ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²¡í„° ìƒì„± ì¤‘...\")\n",
    "test_texts = test_data['context'].fillna('').astype(str).tolist()\n",
    "test_vectors = [embeddings_model.encode(text).tolist() for text in test_texts]\n",
    "X_test = np.vstack(test_vectors)  # 1024ì°¨ì› ì„ë² ë”© ë²¡í„°\n",
    "\n",
    "test_cat1_actual = test_data['category1'].values\n",
    "test_cat2_actual = test_data['category2'].values\n",
    "\n",
    "# ë³µí•© ë¼ë²¨ ìƒì„± (category1_category2 í˜•íƒœ)\n",
    "train_combined_labels = [f\"{cat1}_{cat2}\" for cat1, cat2 in zip(train_cat1, train_cat2)]\n",
    "test_combined_labels = [f\"{cat1}_{cat2}\" for cat1, cat2 in zip(test_cat1_actual, test_cat2_actual)]\n",
    "\n",
    "print(f\"í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ\")\n",
    "print(f\"ë²¡í„° ì°¨ì›: {X_train.shape[1]}ì°¨ì›\")\n",
    "print(f\"ë³µí•© ë¼ë²¨ ì¢…ë¥˜: {len(set(train_combined_labels))}ê°œ\")\n",
    "\n",
    "# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¼ë²¨ ë¶„ì„\n",
    "train_label_set = set(train_combined_labels)\n",
    "test_label_set = set(test_combined_labels)\n",
    "unseen_labels = test_label_set - train_label_set\n",
    "common_labels = train_label_set & test_label_set\n",
    "\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° ê³ ìœ  ë¼ë²¨: {len(train_label_set)}ê°œ\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê³ ìœ  ë¼ë²¨: {len(test_label_set)}ê°œ\")\n",
    "print(f\"ê³µí†µ ë¼ë²¨: {len(common_labels)}ê°œ\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ì—ë§Œ ìˆëŠ” ë¼ë²¨: {len(unseen_labels)}ê°œ\")\n",
    "\n",
    "if unseen_labels:\n",
    "    print(f\"ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ë¼ë²¨ë“¤: {sorted(list(unseen_labels))[:5]}...\")\n",
    "    \n",
    "    # ê³µí†µ ë¼ë²¨ë§Œ ì‚¬ìš©í•˜ì—¬ í•„í„°ë§\n",
    "    train_mask = np.array([label in common_labels for label in train_combined_labels])\n",
    "    test_mask = np.array([label in common_labels for label in test_combined_labels])\n",
    "    \n",
    "    X_train_filtered = X_train[train_mask]\n",
    "    train_combined_filtered = [label for i, label in enumerate(train_combined_labels) if train_mask[i]]\n",
    "    \n",
    "    X_test_filtered = X_test[test_mask]\n",
    "    test_combined_filtered = [label for i, label in enumerate(test_combined_labels) if test_mask[i]]\n",
    "    test_cat1_filtered = test_cat1_actual[test_mask]\n",
    "    test_cat2_filtered = test_cat2_actual[test_mask]\n",
    "    \n",
    "    print(f\"í•„í„°ë§ í›„ í›ˆë ¨ ë°ì´í„°: {len(X_train_filtered)}ê°œ\")\n",
    "    print(f\"í•„í„°ë§ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test_filtered)}ê°œ\")\n",
    "else:\n",
    "    X_train_filtered = X_train\n",
    "    train_combined_filtered = train_combined_labels\n",
    "    X_test_filtered = X_test\n",
    "    test_combined_filtered = test_combined_labels\n",
    "    test_cat1_filtered = test_cat1_actual\n",
    "    test_cat2_filtered = test_cat2_actual\n",
    "\n",
    "# 2. ë¼ë²¨ ì¸ì½”ë”©\n",
    "le_combined = LabelEncoder()\n",
    "y_train_encoded = le_combined.fit_transform(train_combined_filtered)\n",
    "y_test_encoded = le_combined.transform(test_combined_filtered)\n",
    "\n",
    "print(f\"ì¸ì½”ë”©ëœ ë¼ë²¨ ì¢…ë¥˜: {len(le_combined.classes_)}\")\n",
    "\n",
    "# 3. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í›ˆë ¨\n",
    "print(\"\\nğŸ¤– ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, multi_class='ovr')\n",
    "lr_model.fit(X_train_filtered, y_train_encoded)\n",
    "\n",
    "# ë¡œì§€ìŠ¤í‹± íšŒê·€ ì˜ˆì¸¡\n",
    "lr_pred_encoded = lr_model.predict(X_test_filtered)\n",
    "lr_pred_labels = le_combined.inverse_transform(lr_pred_encoded)\n",
    "\n",
    "# ë¡œì§€ìŠ¤í‹± íšŒê·€ ê²°ê³¼ ë¶„ì„\n",
    "lr_accuracy = accuracy_score(test_combined_filtered, lr_pred_labels)\n",
    "lr_f1_macro = f1_score(y_test_encoded, lr_pred_encoded, average='macro')\n",
    "lr_f1_weighted = f1_score(y_test_encoded, lr_pred_encoded, average='weighted')\n",
    "\n",
    "# ê°œë³„ ì¹´í…Œê³ ë¦¬ ì •í™•ë„ ê³„ì‚° (ë¡œì§€ìŠ¤í‹± íšŒê·€)\n",
    "lr_cat1_pred = [label.split('_')[0] for label in lr_pred_labels]\n",
    "lr_cat2_pred = [label.split('_')[1] for label in lr_pred_labels]\n",
    "lr_cat1_accuracy = accuracy_score(test_cat1_filtered, lr_cat1_pred)\n",
    "lr_cat2_accuracy = accuracy_score(test_cat2_filtered, lr_cat2_pred)\n",
    "lr_both_correct = sum(1 for i in range(len(test_cat1_filtered)) \n",
    "                     if lr_cat1_pred[i] == test_cat1_filtered[i] and lr_cat2_pred[i] == test_cat2_filtered[i])\n",
    "lr_both_accuracy = lr_both_correct / len(test_cat1_filtered)\n",
    "\n",
    "# 4. SVM Linear ëª¨ë¸ í›ˆë ¨\n",
    "print(\"ğŸ¤– SVM Linear ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "svm_model = SVC(kernel='linear', random_state=42, probability=True)\n",
    "svm_model.fit(X_train_filtered, y_train_encoded)\n",
    "\n",
    "# SVM ì˜ˆì¸¡\n",
    "svm_pred_encoded = svm_model.predict(X_test_filtered)\n",
    "svm_pred_labels = le_combined.inverse_transform(svm_pred_encoded)\n",
    "\n",
    "# SVM ê²°ê³¼ ë¶„ì„\n",
    "svm_accuracy = accuracy_score(test_combined_filtered, svm_pred_labels)\n",
    "svm_f1_macro = f1_score(y_test_encoded, svm_pred_encoded, average='macro')\n",
    "svm_f1_weighted = f1_score(y_test_encoded, svm_pred_encoded, average='weighted')\n",
    "\n",
    "# ê°œë³„ ì¹´í…Œê³ ë¦¬ ì •í™•ë„ ê³„ì‚° (SVM)\n",
    "svm_cat1_pred = [label.split('_')[0] for label in svm_pred_labels]\n",
    "svm_cat2_pred = [label.split('_')[1] for label in svm_pred_labels]\n",
    "svm_cat1_accuracy = accuracy_score(test_cat1_filtered, svm_cat1_pred)\n",
    "svm_cat2_accuracy = accuracy_score(test_cat2_filtered, svm_cat2_pred)\n",
    "svm_both_correct = sum(1 for i in range(len(test_cat1_filtered)) \n",
    "                      if svm_cat1_pred[i] == test_cat1_filtered[i] and svm_cat2_pred[i] == test_cat2_filtered[i])\n",
    "svm_both_accuracy = svm_both_correct / len(test_cat1_filtered)\n",
    "\n",
    "# 5. ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ”¸ ë¡œì§€ìŠ¤í‹± íšŒê·€ ê²°ê³¼:\")\n",
    "print(f\"  - ë³µí•© ë¼ë²¨ ì •í™•ë„: {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
    "print(f\"  - Category1 ì •í™•ë„: {lr_cat1_accuracy:.4f} ({lr_cat1_accuracy*100:.2f}%)\")\n",
    "print(f\"  - Category2 ì •í™•ë„: {lr_cat2_accuracy:.4f} ({lr_cat2_accuracy*100:.2f}%)\")\n",
    "print(f\"  - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: {lr_both_correct}/{len(test_cat1_filtered)} ({lr_both_accuracy*100:.2f}%)\")\n",
    "print(f\"  - F1-Score (Macro): {lr_f1_macro:.4f}\")\n",
    "print(f\"  - F1-Score (Weighted): {lr_f1_weighted:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ”¸ SVM Linear ê²°ê³¼:\")\n",
    "print(f\"  - ë³µí•© ë¼ë²¨ ì •í™•ë„: {svm_accuracy:.4f} ({svm_accuracy*100:.2f}%)\")\n",
    "print(f\"  - Category1 ì •í™•ë„: {svm_cat1_accuracy:.4f} ({svm_cat1_accuracy*100:.2f}%)\")\n",
    "print(f\"  - Category2 ì •í™•ë„: {svm_cat2_accuracy:.4f} ({svm_cat2_accuracy*100:.2f}%)\")\n",
    "print(f\"  - ë‘ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì •ë‹µ: {svm_both_correct}/{len(test_cat1_filtered)} ({svm_both_accuracy*100:.2f}%)\")\n",
    "print(f\"  - F1-Score (Macro): {svm_f1_macro:.4f}\")\n",
    "print(f\"  - F1-Score (Weighted): {svm_f1_weighted:.4f}\")\n",
    "\n",
    "# ì„±ëŠ¥ ë¹„êµ\n",
    "print(f\"\\nğŸ† ì„±ëŠ¥ ë¹„êµ:\")\n",
    "better_complex = \"ë¡œì§€ìŠ¤í‹± íšŒê·€\" if lr_accuracy > svm_accuracy else \"SVM Linear\"\n",
    "better_both = \"ë¡œì§€ìŠ¤í‹± íšŒê·€\" if lr_both_accuracy > svm_both_accuracy else \"SVM Linear\"\n",
    "better_f1 = \"ë¡œì§€ìŠ¤í‹± íšŒê·€\" if lr_f1_macro > svm_f1_macro else \"SVM Linear\"\n",
    "\n",
    "print(f\"  - ë³µí•© ë¼ë²¨ ì •í™•ë„ê°€ ë” ë†’ì€ ëª¨ë¸: {better_complex}\")\n",
    "print(f\"  - ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ì •ë‹µë¥ ì´ ë” ë†’ì€ ëª¨ë¸: {better_both}\")\n",
    "print(f\"  - F1-Score (Macro)ê°€ ë” ë†’ì€ ëª¨ë¸: {better_f1}\")\n",
    "\n",
    "# ì°¨ì´ ë¶„ì„\n",
    "acc_diff = abs(lr_accuracy - svm_accuracy)\n",
    "both_diff = abs(lr_both_accuracy - svm_both_accuracy)\n",
    "f1_diff = abs(lr_f1_macro - svm_f1_macro)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ì„±ëŠ¥ ì°¨ì´:\")\n",
    "print(f\"  - ë³µí•© ë¼ë²¨ ì •í™•ë„ ì°¨ì´: {acc_diff:.4f} ({acc_diff*100:.2f}%p)\")\n",
    "print(f\"  - ë‘ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ì •ë‹µë¥  ì°¨ì´: {both_diff:.4f} ({both_diff*100:.2f}%p)\")\n",
    "print(f\"  - F1-Score ì°¨ì´: {f1_diff:.4f}\")\n",
    "\n",
    "# ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ\n",
    "print(f\"\\nğŸ“ ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ (ì²˜ìŒ 10ê°œ):\")\n",
    "print(\"-\" * 120)\n",
    "print(f\"{'No.':<3} {'ì‹¤ì œ Cat1':<12} {'ì‹¤ì œ Cat2':<12} {'LR Cat1':<12} {'LR Cat2':<12} {'SVM Cat1':<12} {'SVM Cat2':<12} {'LR ì •ë‹µ':<8} {'SVM ì •ë‹µ':<8}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for i in range(min(10, len(test_cat1_filtered))):\n",
    "    lr_correct = \"âœ“\" if lr_cat1_pred[i] == test_cat1_filtered[i] and lr_cat2_pred[i] == test_cat2_filtered[i] else \"âœ—\"\n",
    "    svm_correct = \"âœ“\" if svm_cat1_pred[i] == test_cat1_filtered[i] and svm_cat2_pred[i] == test_cat2_filtered[i] else \"âœ—\"\n",
    "    \n",
    "    print(f\"{i+1:<3} {test_cat1_filtered[i]:<12} {test_cat2_filtered[i]:<12} \"\n",
    "          f\"{lr_cat1_pred[i]:<12} {lr_cat2_pred[i]:<12} {svm_cat1_pred[i]:<12} {svm_cat2_pred[i]:<12} \"\n",
    "          f\"{lr_correct:<8} {svm_correct:<8}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e6c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skn_after_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
